{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d6e9f21",
   "metadata": {},
   "source": [
    "# Step 5: Post-Scoring Checks\n",
    "Verify every question has marks, validate IDs against the Name List, and clean versioned mark/control files before packaging.\n",
    "\n",
    "**Features:**\n",
    "- ‚úÖ Comprehensive validation with detailed reporting\n",
    "- ‚úÖ Automatic error detection and suggestions\n",
    "- ‚úÖ Safe file operations with backup options\n",
    "- ‚úÖ Detailed logging and statistics\n",
    "- ‚úÖ Color-coded output for easy review\n",
    "- ‚úÖ Batch operations with progress tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "095b9a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-05 03:38:28,448 - INFO - ‚úì Loaded Name List from ../sample/VTC Test Name List.xlsx\n",
      "2026-01-05 03:38:28,449 - INFO -   Total students: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Robust Step 5: Post-Scoring Checks initialized\n",
      "‚úì Session started at: 2026-01-05 03:38:28\n",
      "‚úì Loaded Name List: 4 students\n",
      "\n",
      "============================================================\n",
      "üîç POST-SCORING VALIDATION CHECKS\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "from grading_utils import setup_paths, create_directories\n",
    "from termcolor import colored\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# Robust logging setup\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"‚úÖ Robust Step 5: Post-Scoring Checks initialized\")\n",
    "print(f\"‚úì Session started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Configure dataset prefix and data root\n",
    "prefix = \"VTC Test\"\n",
    "dataset = \"sample\"\n",
    "\n",
    "# Resolve paths\n",
    "paths = setup_paths(prefix, dataset)\n",
    "base_path_questions = paths[\"base_path_questions\"]\n",
    "name_list_file = paths[\"name_list_file\"]\n",
    "\n",
    "# Ensure directories exist\n",
    "create_directories(paths)\n",
    "\n",
    "# Load Name List with validation\n",
    "try:\n",
    "    if not os.path.exists(name_list_file):\n",
    "        raise FileNotFoundError(f\"Name list file not found: {name_list_file}\")\n",
    "    \n",
    "    name_list_df = pd.read_excel(name_list_file, sheet_name=\"Name List\")\n",
    "    \n",
    "    # Validate Name List structure\n",
    "    required_columns = [\"ID\"]\n",
    "    missing_columns = [col for col in required_columns if col not in name_list_df.columns]\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Missing required columns in Name List: {missing_columns}\")\n",
    "    \n",
    "    logger.info(f\"‚úì Loaded Name List from {name_list_file}\")\n",
    "    logger.info(f\"  Total students: {len(name_list_df)}\")\n",
    "    \n",
    "    print(f\"‚úì Loaded Name List: {len(name_list_df)} students\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"‚ùå Failed to load Name List: {e}\")\n",
    "    print(colored(f\"‚ùå Failed to load Name List: {e}\", \"red\"))\n",
    "    name_list_df = pd.DataFrame()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üîç POST-SCORING VALIDATION CHECKS\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bf57264",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-05 03:38:28,466 - INFO - Starting mark validation...\n",
      "2026-01-05 03:38:28,468 - INFO - Skipping validation for metadata question: CLASS\n",
      "2026-01-05 03:38:28,469 - INFO - Skipping validation for metadata question: ID\n",
      "2026-01-05 03:38:28,470 - INFO - Skipping validation for metadata question: NAME\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Mark Validation Results:\n",
      "   Total questions: 8\n",
      "   Metadata questions (skipped): 3\n",
      "   Graded questions: 5\n",
      "   Fully marked: 5\n",
      "   Incomplete: 0\n",
      "\u001b[32m\n",
      "‚úÖ All graded questions have been marked!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Robust Mark Validation with Detailed Reporting\n",
    "\n",
    "def validate_marks() -> Tuple[List[str], Dict[str, List[int]]]:\n",
    "    \"\"\"\n",
    "    Validate that all questions have been marked\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (unfinished_questions, questions_with_empty_marks)\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting mark validation...\")\n",
    "    \n",
    "    # Metadata questions that don't need marking validation\n",
    "    metadata_questions = [\"NAME\", \"ID\", \"CLASS\"]\n",
    "    \n",
    "    unfinished_scoring = []\n",
    "    questions_with_empty_marks = {}\n",
    "    total_questions = 0\n",
    "    marked_questions = 0\n",
    "    \n",
    "    try:\n",
    "        for path, current_directory, files in os.walk(base_path_questions):\n",
    "            if path == base_path_questions:\n",
    "                continue\n",
    "            \n",
    "            total_questions += 1\n",
    "            question = path[len(base_path_questions) + 1:]\n",
    "            \n",
    "            # Skip validation for metadata questions\n",
    "            if question in metadata_questions:\n",
    "                logger.info(f\"Skipping validation for metadata question: {question}\")\n",
    "                continue\n",
    "            \n",
    "            # Check if mark.json exists\n",
    "            if \"mark.json\" not in files:\n",
    "                unfinished_scoring.append(question)\n",
    "                logger.warning(f\"Missing mark.json for question: {question}\")\n",
    "                continue\n",
    "            \n",
    "            # Validate marks in mark.json\n",
    "            try:\n",
    "                mark_file = os.path.join(path, \"mark.json\")\n",
    "                with open(mark_file, \"r\", encoding='utf-8') as f:\n",
    "                    marks = json.load(f)\n",
    "                \n",
    "                # Validate marks structure\n",
    "                if not isinstance(marks, list):\n",
    "                    logger.error(f\"Invalid marks structure for {question}: not a list\")\n",
    "                    unfinished_scoring.append(question)\n",
    "                    continue\n",
    "                \n",
    "                # Check each mark entry\n",
    "                empty_indices = []\n",
    "                for idx, mark in enumerate(marks):\n",
    "                    if not isinstance(mark, dict):\n",
    "                        logger.warning(f\"Invalid mark entry at index {idx} for {question}\")\n",
    "                        continue\n",
    "                    \n",
    "                    mark_value = mark.get('mark', '')\n",
    "                    overrided_mark = mark.get('overridedMark', '')\n",
    "                    \n",
    "                    if mark_value == \"\" and overrided_mark == \"\":\n",
    "                        empty_indices.append(idx + 1)\n",
    "                \n",
    "                if empty_indices:\n",
    "                    questions_with_empty_marks[question] = empty_indices\n",
    "                    unfinished_scoring.append(question)\n",
    "                    logger.warning(f\"Question {question} has {len(empty_indices)} empty marks\")\n",
    "                else:\n",
    "                    marked_questions += 1\n",
    "                    \n",
    "            except json.JSONDecodeError as e:\n",
    "                logger.error(f\"Invalid JSON in mark.json for {question}: {e}\")\n",
    "                unfinished_scoring.append(question)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error reading marks for {question}: {e}\")\n",
    "                unfinished_scoring.append(question)\n",
    "        \n",
    "        # Report results\n",
    "        print(f\"\\nüìä Mark Validation Results:\")\n",
    "        print(f\"   Total questions: {total_questions}\")\n",
    "        print(f\"   Metadata questions (skipped): {len(metadata_questions)}\")\n",
    "        print(f\"   Graded questions: {total_questions - len(metadata_questions)}\")\n",
    "        print(f\"   Fully marked: {marked_questions}\")\n",
    "        print(f\"   Incomplete: {len(unfinished_scoring)}\")\n",
    "        \n",
    "        if unfinished_scoring:\n",
    "            print(colored(f\"\\n‚ö†Ô∏è {len(unfinished_scoring)} question(s) have incomplete marking:\", \"yellow\"))\n",
    "            for question in unfinished_scoring:\n",
    "                if question in questions_with_empty_marks:\n",
    "                    indices = questions_with_empty_marks[question]\n",
    "                    print(colored(f\"   ‚Ä¢ {question}: {len(indices)} empty mark(s) at positions {indices}\", \"red\"))\n",
    "                else:\n",
    "                    print(colored(f\"   ‚Ä¢ {question}: mark.json missing\", \"red\"))\n",
    "        else:\n",
    "            print(colored(\"\\n‚úÖ All graded questions have been marked!\", \"green\"))\n",
    "        \n",
    "        return unfinished_scoring, questions_with_empty_marks\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Mark validation failed: {e}\")\n",
    "        print(colored(f\"‚ùå Mark validation failed: {e}\", \"red\"))\n",
    "        return [], {}\n",
    "\n",
    "# Run mark validation\n",
    "unfinished_questions, empty_marks = validate_marks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d03eac6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-05 03:38:28,485 - INFO - Starting ID validation...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä ID Validation Results:\n",
      "   Students in name list: 4\n",
      "   Students marked: 4\n",
      "   Unique marked IDs: 4\n",
      "\u001b[32m\n",
      "‚úÖ All student IDs validated successfully!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Robust ID Validation with Cross-Checking\n",
    "\n",
    "def validate_student_ids() -> Tuple[List[str], List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Validate student IDs against the name list\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (missing_from_marks, marked_but_not_in_list, duplicate_ids)\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting ID validation...\")\n",
    "    \n",
    "    try:\n",
    "        # Load marked IDs\n",
    "        id_mark_file = os.path.join(base_path_questions, \"ID\", \"mark.json\")\n",
    "        \n",
    "        if not os.path.exists(id_mark_file):\n",
    "            logger.error(\"ID mark.json file not found\")\n",
    "            print(colored(\"‚ùå ID mark.json file not found!\", \"red\"))\n",
    "            return [], [], []\n",
    "        \n",
    "        with open(id_mark_file, \"r\", encoding='utf-8') as f:\n",
    "            marks = json.load(f)\n",
    "        \n",
    "        # Extract marked IDs (prefer overridedMark if available)\n",
    "        id_from_mark = []\n",
    "        for mark in marks:\n",
    "            mark_value = mark.get(\"overridedMark\", \"\") or mark.get(\"mark\", \"\")\n",
    "            if mark_value:\n",
    "                id_from_mark.append(str(mark_value).strip())\n",
    "        \n",
    "        # Get IDs from name list\n",
    "        if name_list_df.empty:\n",
    "            logger.warning(\"Name list is empty, skipping ID validation\")\n",
    "            return [], [], []\n",
    "        \n",
    "        id_from_namelist = [str(id).strip() for id in name_list_df[\"ID\"].tolist()]\n",
    "        \n",
    "        # Check for duplicates in marked IDs\n",
    "        duplicate_ids = []\n",
    "        seen_ids = set()\n",
    "        for id_val in id_from_mark:\n",
    "            if id_val in seen_ids:\n",
    "                if id_val not in duplicate_ids:\n",
    "                    duplicate_ids.append(id_val)\n",
    "            seen_ids.add(id_val)\n",
    "        \n",
    "        # Find missing IDs (in name list but not marked)\n",
    "        mark_missing_id = [id_val for id_val in id_from_namelist if id_val not in id_from_mark]\n",
    "        \n",
    "        # Find extra IDs (marked but not in name list)\n",
    "        marked_but_not_in_namelist = [id_val for id_val in id_from_mark if id_val not in id_from_namelist]\n",
    "        \n",
    "        # Report results\n",
    "        print(f\"\\nüìä ID Validation Results:\")\n",
    "        print(f\"   Students in name list: {len(id_from_namelist)}\")\n",
    "        print(f\"   Students marked: {len(id_from_mark)}\")\n",
    "        print(f\"   Unique marked IDs: {len(seen_ids)}\")\n",
    "        \n",
    "        if duplicate_ids:\n",
    "            print(colored(f\"\\n‚ö†Ô∏è Duplicate IDs found: {duplicate_ids}\", \"red\"))\n",
    "            print(colored(\"   Action required: Check for scanning errors or duplicate submissions\", \"yellow\"))\n",
    "        \n",
    "        if mark_missing_id:\n",
    "            print(colored(f\"\\n‚ö†Ô∏è {len(mark_missing_id)} student(s) in name list but not marked:\", \"yellow\"))\n",
    "            for id_val in mark_missing_id:\n",
    "                student_info = name_list_df[name_list_df[\"ID\"].astype(str) == id_val]\n",
    "                if not student_info.empty and \"Name\" in student_info.columns:\n",
    "                    name = student_info.iloc[0][\"Name\"]\n",
    "                    print(colored(f\"   ‚Ä¢ ID {id_val}: {name} (possibly absent)\", \"red\"))\n",
    "                else:\n",
    "                    print(colored(f\"   ‚Ä¢ ID {id_val} (possibly absent)\", \"red\"))\n",
    "        \n",
    "        if marked_but_not_in_namelist:\n",
    "            print(colored(f\"\\n‚ö†Ô∏è {len(marked_but_not_in_namelist)} marked ID(s) not in name list:\", \"yellow\"))\n",
    "            for id_val in marked_but_not_in_namelist:\n",
    "                print(colored(f\"   ‚Ä¢ ID {id_val} (check for OCR errors or wrong class)\", \"red\"))\n",
    "            print(colored(\"   Action required: Verify these IDs manually\", \"yellow\"))\n",
    "        \n",
    "        if not duplicate_ids and not mark_missing_id and not marked_but_not_in_namelist:\n",
    "            print(colored(\"\\n‚úÖ All student IDs validated successfully!\", \"green\"))\n",
    "        \n",
    "        return mark_missing_id, marked_but_not_in_namelist, duplicate_ids\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"ID validation failed: {e}\")\n",
    "        print(colored(f\"‚ùå ID validation failed: {e}\", \"red\"))\n",
    "        return [], [], []\n",
    "\n",
    "# Run ID validation\n",
    "missing_ids, extra_ids, duplicate_ids = validate_student_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93fae80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-05 03:38:28,503 - INFO - Starting version history cleanup (dry_run=True)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üßπ VERSION HISTORY CLEANUP\n",
      "============================================================\n",
      "\n",
      "üìÅ Version History Cleanup:\n",
      "   Files found: 0\n",
      "\u001b[32m   ‚úì No version history files to clean\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Robust Version History Cleanup with Safety Checks\n",
    "\n",
    "def cleanup_version_history(dry_run: bool = False) -> Tuple[int, List[str]]:\n",
    "    \"\"\"\n",
    "    Remove versioned mark and control files\n",
    "    \n",
    "    Args:\n",
    "        dry_run: If True, only report what would be deleted without actually deleting\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (files_removed, file_list)\n",
    "    \"\"\"\n",
    "    logger.info(f\"Starting version history cleanup (dry_run={dry_run})...\")\n",
    "    \n",
    "    files_to_remove = []\n",
    "    \n",
    "    try:\n",
    "        for path, current_directory, files in os.walk(base_path_questions):\n",
    "            for file in files:\n",
    "                # Match versioned files: control-*.json or mark-*.json\n",
    "                if (file.startswith(\"control-\") or file.startswith(\"mark-\")) and file.endswith(\".json\"):\n",
    "                    file_path = os.path.join(path, file)\n",
    "                    files_to_remove.append(file_path)\n",
    "        \n",
    "        print(f\"\\nüìÅ Version History Cleanup:\")\n",
    "        print(f\"   Files found: {len(files_to_remove)}\")\n",
    "        \n",
    "        if not files_to_remove:\n",
    "            print(colored(\"   ‚úì No version history files to clean\", \"green\"))\n",
    "            return 0, []\n",
    "        \n",
    "        if dry_run:\n",
    "            print(colored(f\"\\n   DRY RUN - Would remove {len(files_to_remove)} file(s):\", \"yellow\"))\n",
    "            for file_path in files_to_remove[:10]:  # Show first 10\n",
    "                print(f\"      ‚Ä¢ {os.path.relpath(file_path, base_path_questions)}\")\n",
    "            if len(files_to_remove) > 10:\n",
    "                print(f\"      ... and {len(files_to_remove) - 10} more\")\n",
    "            print(colored(\"\\n   Set dry_run=False to actually remove files\", \"yellow\"))\n",
    "            return 0, files_to_remove\n",
    "        \n",
    "        # Actually remove files\n",
    "        removed_count = 0\n",
    "        failed_removals = []\n",
    "        \n",
    "        for file_path in files_to_remove:\n",
    "            try:\n",
    "                os.remove(file_path)\n",
    "                removed_count += 1\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to remove {file_path}: {e}\")\n",
    "                failed_removals.append(file_path)\n",
    "        \n",
    "        print(colored(f\"\\n   ‚úì Removed {removed_count} version history file(s)\", \"green\"))\n",
    "        \n",
    "        if failed_removals:\n",
    "            print(colored(f\"   ‚ö†Ô∏è Failed to remove {len(failed_removals)} file(s)\", \"yellow\"))\n",
    "            for file_path in failed_removals[:5]:\n",
    "                print(f\"      ‚Ä¢ {os.path.relpath(file_path, base_path_questions)}\")\n",
    "        \n",
    "        return removed_count, files_to_remove\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Version history cleanup failed: {e}\")\n",
    "        print(colored(f\"‚ùå Cleanup failed: {e}\", \"red\"))\n",
    "        return 0, []\n",
    "\n",
    "# Run cleanup (dry run first to preview)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üßπ VERSION HISTORY CLEANUP\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Preview what would be deleted\n",
    "removed_count, file_list = cleanup_version_history(dry_run=True)\n",
    "\n",
    "# Uncomment the line below to actually remove files\n",
    "# removed_count, file_list = cleanup_version_history(dry_run=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2a2f415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üìä SCORING STATISTICS\n",
      "============================================================\n",
      "\n",
      "üìù Overall Summary:\n",
      "   Total questions: 8\n",
      "   Fully marked questions: 7\n",
      "   Students in name list: 4\n",
      "   Students with marks: 4\n",
      "\n",
      "üìã Per-Question Summary:\n",
      "   Q1:\n",
      "      Submissions: 4/4 (100.0%)\n",
      "      Total marks awarded: 6.0\n",
      "   Q2:\n",
      "      Submissions: 4/4 (100.0%)\n",
      "      Total marks awarded: 31.0\n",
      "   Q3:\n",
      "      Submissions: 4/4 (100.0%)\n",
      "      Total marks awarded: 5.0\n",
      "   Q4:\n",
      "      Submissions: 4/4 (100.0%)\n",
      "      Total marks awarded: 10.0\n",
      "   Q5:\n",
      "      Submissions: 4/4 (100.0%)\n",
      "      Total marks awarded: 9.0\n",
      "\n",
      "üí° Note: NAME, ID, and CLASS are metadata fields and excluded from this summary.\n"
     ]
    }
   ],
   "source": [
    "# Generate Comprehensive Statistics and Summary\n",
    "\n",
    "def generate_statistics() -> Dict:\n",
    "    \"\"\"Generate comprehensive statistics about the scoring process\"\"\"\n",
    "    \n",
    "    stats = {\n",
    "        \"total_questions\": 0,\n",
    "        \"marked_questions\": 0,\n",
    "        \"total_students\": len(name_list_df) if not name_list_df.empty else 0,\n",
    "        \"marked_students\": 0,\n",
    "        \"total_marks_awarded\": 0,\n",
    "        \"questions_summary\": {}\n",
    "    }\n",
    "    \n",
    "    # Metadata questions that don't need submission counting\n",
    "    metadata_questions = [\"NAME\", \"ID\", \"CLASS\"]\n",
    "    \n",
    "    try:\n",
    "        for path, current_directory, files in os.walk(base_path_questions):\n",
    "            if path == base_path_questions:\n",
    "                continue\n",
    "            \n",
    "            question = path[len(base_path_questions) + 1:]\n",
    "            stats[\"total_questions\"] += 1\n",
    "            \n",
    "            if \"mark.json\" in files:\n",
    "                try:\n",
    "                    with open(os.path.join(path, \"mark.json\"), \"r\") as f:\n",
    "                        marks = json.load(f)\n",
    "                    \n",
    "                    marked_count = 0\n",
    "                    total_marks = 0\n",
    "                    \n",
    "                    for mark in marks:\n",
    "                        mark_value = mark.get('overridedMark', '') or mark.get('mark', '')\n",
    "                        if mark_value != '':\n",
    "                            marked_count += 1\n",
    "                            try:\n",
    "                                total_marks += float(mark_value)\n",
    "                            except (ValueError, TypeError):\n",
    "                                pass\n",
    "                    \n",
    "                    if marked_count == len(marks):\n",
    "                        stats[\"marked_questions\"] += 1\n",
    "                    \n",
    "                    # Only add to summary if not a metadata question\n",
    "                    if question not in metadata_questions:\n",
    "                        stats[\"questions_summary\"][question] = {\n",
    "                            \"total_submissions\": len(marks),\n",
    "                            \"marked_submissions\": marked_count,\n",
    "                            \"total_marks\": total_marks\n",
    "                        }\n",
    "                    \n",
    "                    if question == \"ID\":\n",
    "                        stats[\"marked_students\"] = marked_count\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Error reading stats for {question}: {e}\")\n",
    "        \n",
    "        return stats\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to generate statistics: {e}\")\n",
    "        return stats\n",
    "\n",
    "# Generate and display statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä SCORING STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "stats = generate_statistics()\n",
    "\n",
    "print(f\"\\nüìù Overall Summary:\")\n",
    "print(f\"   Total questions: {stats['total_questions']}\")\n",
    "print(f\"   Fully marked questions: {stats['marked_questions']}\")\n",
    "print(f\"   Students in name list: {stats['total_students']}\")\n",
    "print(f\"   Students with marks: {stats['marked_students']}\")\n",
    "\n",
    "if stats['questions_summary']:\n",
    "    print(f\"\\nüìã Per-Question Summary:\")\n",
    "    for question, summary in sorted(stats['questions_summary'].items()):\n",
    "        completion = (summary['marked_submissions'] / summary['total_submissions'] * 100) if summary['total_submissions'] > 0 else 0\n",
    "        print(f\"   {question}:\")\n",
    "        print(f\"      Submissions: {summary['marked_submissions']}/{summary['total_submissions']} ({completion:.1f}%)\")\n",
    "        print(f\"      Total marks awarded: {summary['total_marks']:.1f}\")\n",
    "    \n",
    "    print(f\"\\nüí° Note: NAME, ID, and CLASS are metadata fields and excluded from this summary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cdae41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "‚úÖ POST-SCORING CHECKS COMPLETE\n",
      "============================================================\n",
      "\u001b[32m\n",
      "üéâ All validation checks passed!\u001b[0m\n",
      "\n",
      "‚úì Ready for next steps:\n",
      "   1. Run version history cleanup (uncomment in Cell 5)\n",
      "   2. Backup the output directory\n",
      "   3. Proceed to Step 6: Scoring Postprocessing\n",
      "\n",
      "============================================================\n",
      "Session completed at: 2026-01-05 03:38:28\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Final Summary and Recommendations\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ POST-SCORING CHECKS COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Metadata questions that don't need marking validation\n",
    "metadata_questions = [\"NAME\", \"ID\", \"CLASS\"]\n",
    "\n",
    "# Filter out metadata questions from unfinished list\n",
    "actual_unfinished = [q for q in unfinished_questions if q not in metadata_questions]\n",
    "\n",
    "# Determine overall status\n",
    "all_checks_passed = (\n",
    "    len(actual_unfinished) == 0 and\n",
    "    len(missing_ids) == 0 and\n",
    "    len(extra_ids) == 0 and\n",
    "    len(duplicate_ids) == 0\n",
    ")\n",
    "\n",
    "if all_checks_passed:\n",
    "    print(colored(\"\\nüéâ All validation checks passed!\", \"green\"))\n",
    "    print(\"\\n‚úì Ready for next steps:\")\n",
    "    print(\"   1. Run version history cleanup (uncomment in Cell 5)\")\n",
    "    print(\"   2. Backup the output directory\")\n",
    "    print(\"   3. Proceed to Step 6: Scoring Postprocessing\")\n",
    "else:\n",
    "    print(colored(\"\\n‚ö†Ô∏è Some issues require attention:\", \"yellow\"))\n",
    "    \n",
    "    if actual_unfinished:\n",
    "        print(f\"\\n   ‚Ä¢ {len(actual_unfinished)} question(s) with incomplete marking\")\n",
    "        for q in actual_unfinished:\n",
    "            if q in empty_marks:\n",
    "                print(f\"     - {q}: {len(empty_marks[q])} empty mark(s)\")\n",
    "            else:\n",
    "                print(f\"     - {q}: mark.json missing\")\n",
    "        print(\"     Action: Review and complete marking in the web interface\")\n",
    "    \n",
    "    if missing_ids:\n",
    "        print(f\"\\n   ‚Ä¢ {len(missing_ids)} student(s) in name list but not marked\")\n",
    "        print(\"     Action: Verify if these students were absent\")\n",
    "    \n",
    "    if extra_ids:\n",
    "        print(f\"\\n   ‚Ä¢ {len(extra_ids)} marked ID(s) not in name list\")\n",
    "        print(\"     Action: Check for OCR errors or wrong class submissions\")\n",
    "    \n",
    "    if duplicate_ids:\n",
    "        print(f\"\\n   ‚Ä¢ {len(duplicate_ids)} duplicate ID(s) found\")\n",
    "        print(\"     Action: Check for scanning errors or duplicate submissions\")\n",
    "    \n",
    "    print(\"\\n   After resolving issues, re-run this notebook to verify.\")\n",
    "\n",
    "# Show metadata info if any were flagged\n",
    "metadata_flagged = [q for q in unfinished_questions if q in metadata_questions]\n",
    "if metadata_flagged:\n",
    "    print(f\"\\nüí° Note: {', '.join(metadata_flagged)} are metadata fields and don't require marking.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"Session completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3585c628",
   "metadata": {},
   "source": [
    "### ‚ö†Ô∏è DANGER ZONE: Reset Everything\n",
    "\n",
    "**WARNING**: The code below will delete all mark.json and control.json files, resetting all manual corrections!\n",
    "\n",
    "Only use this if you need to completely restart the marking process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "362eedc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Reset function available but safely commented out\n"
     ]
    }
   ],
   "source": [
    "# ‚ö†Ô∏è DANGER: Uncomment to reset all marks and controls\n",
    "# This will DELETE all mark.json and control.json files!\n",
    "\n",
    "# def reset_all_marks(confirm: bool = False):\n",
    "#     \"\"\"Reset all marks - USE WITH EXTREME CAUTION\"\"\"\n",
    "#     if not confirm:\n",
    "#         print(colored(\"Safety check: Set confirm=True to actually reset\", \"yellow\"))\n",
    "#         return\n",
    "#     \n",
    "#     removed_count = 0\n",
    "#     for path, current_directory, files in os.walk(base_path_questions):\n",
    "#         for file in files:\n",
    "#             if file == \"control.json\" or file == \"mark.json\":\n",
    "#                 try:\n",
    "#                     os.remove(os.path.join(path, file))\n",
    "#                     removed_count += 1\n",
    "#                 except Exception as e:\n",
    "#                     logger.error(f\"Failed to remove {file}: {e}\")\n",
    "#     \n",
    "#     print(colored(f\"‚ö†Ô∏è Removed {removed_count} mark/control files\", \"red\"))\n",
    "#     print(\"All marks have been reset. Re-run Step 4 to regenerate.\")\n",
    "# \n",
    "# # Uncomment and set confirm=True to actually reset\n",
    "# # reset_all_marks(confirm=False)\n",
    "\n",
    "print(\"‚úì Reset function available but safely commented out\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
