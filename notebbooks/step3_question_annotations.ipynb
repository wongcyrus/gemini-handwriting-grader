{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Define Answer Bounding Boxes\n",
    "1. Convert the exam PDF into page images.\n",
    "2. Auto-detect bounding boxes with AI.\n",
    "3. Manually review and adjust each answer region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grading_utils import setup_paths, create_directories\n",
    "\n",
    "prefix = \"VTC Test\"\n",
    "paths = setup_paths(prefix, \"sample\")\n",
    "\n",
    "pdf_file = paths[\"pdf_file\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Processing Settings\n",
    "\n",
    "Set the number of pages to process from the PDF. \n",
    "- Use `len(pages)` to process all pages\n",
    "- Use a specific number (e.g., `1`, `2`, `3`) to process only that many pages\n",
    "- Useful for testing on a subset before processing the entire document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of pages to process\n",
    "# Set to 1 for testing, or use len(pages) after converting PDF to process all pages\n",
    "number_of_pages = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Extract paths from setup\n",
    "file_name = paths[\"file_name\"]\n",
    "base_path = paths[\"base_path\"]\n",
    "base_path_images = paths[\"base_path_images\"]\n",
    "base_path_annotations = paths[\"base_path_annotations\"]\n",
    "\n",
    "# Create directories\n",
    "create_directories(paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert PDF to JPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read pdf and convert to images\n",
    "# https://stackoverflow.com/questions/46184239/how-to-convert-pdf-to-image-using-python\n",
    "from pdf2image import convert_from_path\n",
    "import os\n",
    "\n",
    "pages = convert_from_path(pdf_file, fmt='jpeg')\n",
    "# extrat file name from pdf_file\n",
    "file_name = os.path.basename(pdf_file)\n",
    "file_name = os.path.splitext(file_name)[0]\n",
    "\n",
    "for count, page in enumerate(pages):\n",
    "    page.save(f'{base_path_images}{count}.jpg', 'JPEG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import json\n",
    "\n",
    "def update_json_file(annotations, path):\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(annotations, f, indent=4)   \n",
    "\n",
    "def image_to_data_url(filename):\n",
    "    ext = filename.split(\".\")[-1]\n",
    "    prefix = f\"data:image/{ext};base64,\"\n",
    "    with open(filename, \"rb\") as f:\n",
    "        img = f.read()\n",
    "    return prefix + base64.b64encode(img).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Vertex AI Express Mode with API Key\n",
    "\n",
    "This notebook now uses **Vertex AI Express Mode** with API key authentication instead of OAuth/ADC.\n",
    "\n",
    "**Steps to get your API key:**\n",
    "1. Visit https://aistudio.google.com/apikey\n",
    "2. Create or select your API key\n",
    "3. Copy the API key and add it to the `.env` file in the parent directory:\n",
    "   ```\n",
    "   GOOGLE_GENAI_API_KEY=your-actual-api-key-here\n",
    "   ```\n",
    "\n",
    "**Benefits of Express Mode:**\n",
    "- ✓ Simpler authentication (just an API key)\n",
    "- ✓ No need for gcloud CLI authentication\n",
    "- ✓ No service account JSON files\n",
    "- ✓ Easy to use in notebooks and scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Ready to initialize Gemini client\n"
     ]
    }
   ],
   "source": [
    "from grading_utils import init_gemini_client\n",
    "\n",
    "# Gemini client will be initialized in the next cell\n",
    "print(\"✓ Ready to initialize Gemini client\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Vertex AI Client\n",
    "\n",
    "Initialize the Gemini API client using the API key from the `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Vertex AI Express Mode initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "# Get API key from environment variable\n",
    "API_KEY = os.getenv(\"GOOGLE_GENAI_API_KEY\")\n",
    "\n",
    "if not API_KEY or API_KEY == \"your-api-key-here\":\n",
    "    raise ValueError(\n",
    "        \"Please set your GOOGLE_GENAI_API_KEY in the .env file. \"\n",
    "        \"Get your API key from: https://aistudio.google.com/apikey\"\n",
    "    )\n",
    "\n",
    "# Initialize client with Vertex AI Express Mode\n",
    "client = genai.Client(vertexai=True, api_key=API_KEY)\n",
    "\n",
    "print(\"✓ Vertex AI Express Mode initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Pydantic models defined for structured output\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "class BoundingBox(BaseModel):\n",
    "    \"\"\"Represents a single bounding box annotation\"\"\"\n",
    "    x: int = Field(description=\"X coordinate of the top-left corner\")\n",
    "    y: int = Field(description=\"Y coordinate of the top-left corner\")\n",
    "    width: int = Field(description=\"Width of the bounding box\")\n",
    "    height: int = Field(description=\"Height of the bounding box\")\n",
    "    label: str = Field(description=\"Question number (e.g., '1', '2', '3')\")\n",
    "\n",
    "class BoundingBoxResponse(BaseModel):\n",
    "    \"\"\"Wrapper class for list of bounding boxes\"\"\"\n",
    "    boxes: List[BoundingBox] = Field(description=\"List of bounding boxes for question cells\")\n",
    "\n",
    "print(\"✓ Pydantic models defined for structured output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Structured OCR function defined\n"
     ]
    }
   ],
   "source": [
    "def ocr_structured(prompt: str, filePath: str, response_schema: BaseModel):\n",
    "    \"\"\"\n",
    "    OCR function using Vertex AI Express Mode with structured output\n",
    "    \n",
    "    Args:\n",
    "        prompt: The prompt describing what to extract\n",
    "        filePath: Path to the image file\n",
    "        response_schema: Pydantic BaseModel class defining the expected response structure\n",
    "    \n",
    "    Returns:\n",
    "        Parsed response as the specified Pydantic model\n",
    "    \"\"\"\n",
    "    # Read the image file\n",
    "    with open(filePath, \"rb\") as f:\n",
    "        data = f.read()\n",
    "    \n",
    "    # Create configuration with structured output\n",
    "    config = types.GenerateContentConfig(\n",
    "        temperature=0,\n",
    "        top_p=0.5,\n",
    "        max_output_tokens=65535,\n",
    "        response_mime_type=\"application/json\",\n",
    "        response_schema=response_schema,\n",
    "        safety_settings=[\n",
    "            types.SafetySetting(\n",
    "                category=\"HARM_CATEGORY_HATE_SPEECH\",\n",
    "                threshold=\"BLOCK_ONLY_HIGH\",\n",
    "            ),\n",
    "            types.SafetySetting(\n",
    "                category=\"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "                threshold=\"BLOCK_ONLY_HIGH\",\n",
    "            ),\n",
    "            types.SafetySetting(\n",
    "                category=\"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
    "                threshold=\"BLOCK_ONLY_HIGH\",\n",
    "            ),\n",
    "            types.SafetySetting(\n",
    "                category=\"HARM_CATEGORY_HARASSMENT\",\n",
    "                threshold=\"BLOCK_ONLY_HIGH\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Generate content with structured output\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-3-flash-preview\",\n",
    "        contents=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"parts\": [\n",
    "                    {\"inline_data\": {\"mime_type\": \"image/jpeg\", \"data\": data}},\n",
    "                    {\"text\": prompt}\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        config=config,\n",
    "    )\n",
    "    \n",
    "    # Try to use parsed property first, fall back to text parsing\n",
    "    import json\n",
    "    \n",
    "    # Check if parsed property exists and has data\n",
    "    if hasattr(response, 'parsed') and response.parsed is not None:\n",
    "        print(f\"✓ Response parsed successfully - found {len(response.parsed.boxes)} boxes\")\n",
    "        return response.parsed\n",
    "    \n",
    "    # Fall back to text-based parsing\n",
    "    if response.text is None or response.text == \"\":\n",
    "        print(f\"⚠️ Warning: Empty response received for {filePath}\")\n",
    "        print(f\"Response attributes: {dir(response)}\")\n",
    "        # Return empty response matching schema\n",
    "        return response_schema(boxes=[])\n",
    "    \n",
    "    print(f\"✓ Response received ({len(response.text)} chars)\")\n",
    "    \n",
    "    try:\n",
    "        result = json.loads(response.text)\n",
    "        parsed = response_schema(**result)\n",
    "        print(f\"✓ Successfully parsed {len(parsed.boxes)} boxes from text\")\n",
    "        return parsed\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"❌ JSON parsing error: {e}\")\n",
    "        print(f\"Response text (first 1000 chars):\\n{response.text[:1000]}\")\n",
    "        # Return empty response on parse error\n",
    "        return response_schema(boxes=[])\n",
    "\n",
    "print(\"✓ Structured OCR function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Bounding Boxes with Structured Output\n",
    "\n",
    "Using **Pydantic models** with Gemini's structured output feature ensures:\n",
    "- ✓ Consistent JSON format (no parsing errors)\n",
    "- ✓ Type safety and validation\n",
    "- ✓ No need for manual JSON parsing or cleanup\n",
    "- ✓ Automatic schema enforcement by the model\n",
    "\n",
    "The model will return responses that match the `BoundingBoxResponse` schema exactly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Processing page 0 (../marking_form/VTC Test/images/0.jpg)\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Response parsed successfully - found 3 boxes\n",
      "✓ Page 0: Found 3 bounding boxes\n",
      "[\n",
      "  {\n",
      "    \"x\": 134,\n",
      "    \"y\": 260,\n",
      "    \"width\": 719,\n",
      "    \"height\": 110,\n",
      "    \"label\": \"Q1\"\n",
      "  },\n",
      "  {\n",
      "    \"x\": 134,\n",
      "    \"y\": 370,\n",
      "    \"width\": 719,\n",
      "    \"height\": 125,\n",
      "    \"label\": \"Q2\"\n",
      "  },\n",
      "  {\n",
      "    \"x\": 134,\n",
      "    \"y\": 495,\n",
      "    \"width\": 719,\n",
      "    \"height\": 108,\n",
      "    \"label\": \"Q3\"\n",
      "  }\n",
      "]\n",
      "\n",
      "============================================================\n",
      "Processing page 1 (../marking_form/VTC Test/images/1.jpg)\n",
      "============================================================\n",
      "✓ Response parsed successfully - found 2 boxes\n",
      "✓ Page 1: Found 2 bounding boxes\n",
      "[\n",
      "  {\n",
      "    \"x\": 135,\n",
      "    \"y\": 234,\n",
      "    \"width\": 719,\n",
      "    \"height\": 111,\n",
      "    \"label\": \"4\"\n",
      "  },\n",
      "  {\n",
      "    \"x\": 135,\n",
      "    \"y\": 345,\n",
      "    \"width\": 719,\n",
      "    \"height\": 123,\n",
      "    \"label\": \"5\"\n",
      "  }\n",
      "]\n",
      "\n",
      "============================================================\n",
      "✓ All annotations extraction completed!\n",
      "Successfully processed 2 pages\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import copy\n",
    "\n",
    "# Updated prompt for structured output\n",
    "prompt = \"\"\"Extract the coordinates of bounding boxes for each question/answer cell from the table in the image.\n",
    "\n",
    "Instructions:\n",
    "- Identify all table cells that contain question numbers (like \"20\", \"21\", \"22a\", \"22b\", \"23\", \"24\", etc.)\n",
    "- Question numbers are typically located in the top-left corner or top area of each cell\n",
    "- Each bounding box should cover the entire cell area where a student would write their answer\n",
    "- Include cells with sub-questions (like 22a, 22b, 22c, etc.) as separate bounding boxes\n",
    "- Do NOT include cells that only contain \"XXXXXXX\" or are marked as non-answer areas\n",
    "- Bounding boxes may be adjacent but should not overlap\n",
    "- For merged cells spanning multiple rows/columns, create one bounding box covering the entire merged area\n",
    "\n",
    "For each bounding box, provide:\n",
    "- x: X coordinate of the top-left corner of the cell\n",
    "- y: Y coordinate of the top-left corner of the cell\n",
    "- width: Width of the entire cell (including answer space)\n",
    "- height: Height of the entire cell (including answer space)\n",
    "- label: The question number only (e.g., \"20\", \"21\", \"22a\", \"22b\", \"23\", \"24\", etc.)\n",
    "\n",
    "Important: \n",
    "- Extract the question number text exactly as shown (including letters like \"a\", \"b\", \"c\" for sub-questions)\n",
    "- Do not include the period after the question number in the label\n",
    "- Focus on cells where students write answers, not header cells or instruction text\n",
    "\"\"\"\n",
    "\n",
    "aiAnnoation = {}\n",
    "\n",
    "for i in range(number_of_pages):\n",
    "    image_path = base_path_images + f\"{i}.jpg\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing page {i} ({image_path})\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    try:\n",
    "        # Use structured output with Pydantic schema\n",
    "        result = ocr_structured(prompt, image_path, BoundingBoxResponse)\n",
    "        \n",
    "        # Convert Pydantic model to dict and extract boxes\n",
    "        boxes_dict = [box.model_dump() for box in result.boxes]\n",
    "        aiAnnoation[str(i)] = boxes_dict\n",
    "        \n",
    "        print(f\"✓ Page {i}: Found {len(boxes_dict)} bounding boxes\")\n",
    "        if boxes_dict:\n",
    "            print(json.dumps(boxes_dict, indent=2))\n",
    "        else:\n",
    "            print(\"  (No bounding boxes detected)\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing page {i}: {type(e).__name__}: {e}\")\n",
    "        aiAnnoation[str(i)] = []\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"✓ All annotations extraction completed!\")\n",
    "print(f\"Successfully processed {len(aiAnnoation)} pages\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "backup = copy.deepcopy(aiAnnoation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Width: 1654, Height: 2338\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Open an image file\n",
    "with Image.open(image_path) as img:\n",
    "    # Get width and height\n",
    "    width, height = img.size\n",
    "\n",
    "print(f\"Width: {width}, Height: {height}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import copy\n",
    "aiAnnoation = copy.deepcopy(backup)\n",
    "\n",
    "x_scale = width / 1000.0\n",
    "y_scale = height / 1000.0\n",
    "# x_scale = 1\n",
    "# y_scale = 1\n",
    "for i in range(number_of_pages):\n",
    "    for item in aiAnnoation[str(i)]:\n",
    "        item['x'] = int(round(item['x'] * x_scale))\n",
    "        item['y'] = int(round(item['y'] * y_scale)) \n",
    "        item['width'] = int(round(item['width'] * x_scale))\n",
    "        item['height'] = int(round(item['height'] * y_scale))\n",
    "\n",
    "\n",
    "ai_annotations_path = base_path_annotations + \"ai_annotations.json\"\n",
    "\n",
    "# Save the aiAnnoation variable to a JSON file\n",
    "with open(ai_annotations_path, \"w\") as f:\n",
    "    json.dump(aiAnnoation, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please ensure the following are clearly marked on each page before grading:\n",
    "- ID\n",
    "- NAME\n",
    "- CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded AI annotations for 2 pages\n",
      "Total pages with annotations: ['0', '1']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1f729db81a04b66bfcde489afb2d432",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Text(value='', description='Question:', disabled=True, placeholder=''),)), IntPr…"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from jupyter_bbox_widget import BBoxWidget\n",
    "import ipywidgets as widgets\n",
    "import json\n",
    "import glob\n",
    "\n",
    "page = 1\n",
    "pageAndBoxingBoxes={}\n",
    "\n",
    "files = sorted(glob.glob(base_path_images + \"*.jpg\"))\n",
    "\n",
    "w_progress = widgets.IntProgress(value=0, max=len(files), description=\"Progress\")\n",
    "annotations_path = base_path_annotations + \"annotations.json\"\n",
    "ai_annotations_path = base_path_annotations + \"ai_annotations.json\"\n",
    "\n",
    "annotations = {}\n",
    "# Load AI annotations first (as base)\n",
    "if os.path.exists(ai_annotations_path):\n",
    "    with open(ai_annotations_path, \"r\") as f: \n",
    "        annotations = json.load(f) \n",
    "    print(f\"✓ Loaded AI annotations for {len(annotations)} pages\")\n",
    "\n",
    "# Then merge/override with manual annotations if they exist\n",
    "if os.path.exists(annotations_path):\n",
    "    with open(annotations_path, \"r\") as f: \n",
    "        manual_annotations = json.load(f)\n",
    "        annotations.update(manual_annotations)  # Merge instead of replace\n",
    "    print(f\"✓ Merged manual annotations for {len(manual_annotations)} pages\")\n",
    "\n",
    "print(f\"Total pages with annotations: {list(annotations.keys())}\")\n",
    "\n",
    "question_widget = widgets.Text(value=\"\", placeholder=\"\", description=\"Question:\")\n",
    "\n",
    "w_bbox = BBoxWidget(\n",
    "    image=image_to_data_url(files[0])   \n",
    ")\n",
    "w_bbox.attach(question_widget, name=\"label\")\n",
    "w_bbox.bboxes = annotations[str(w_progress.value)] if str(w_progress.value) in annotations else []\n",
    "\n",
    "# when Skip button is pressed we move on to the next file\n",
    "def on_skip():\n",
    "    if w_progress.value + 1 >= len(files):\n",
    "        print(f\"Already at the last page ({len(files)-1})\")\n",
    "        return\n",
    "    \n",
    "    w_progress.value += 1\n",
    "    current_page = str(w_progress.value)\n",
    "    \n",
    "    # open new image in the widget\n",
    "    image_file = files[w_progress.value]\n",
    "    w_bbox.image = image_to_data_url(image_file)\n",
    "    \n",
    "    # Load bounding boxes for current page\n",
    "    if current_page in annotations:\n",
    "        w_bbox.bboxes = annotations[current_page]\n",
    "        print(f\"✓ Loaded {len(annotations[current_page])} bounding boxes for page {w_progress.value}\")\n",
    "    else:\n",
    "        w_bbox.bboxes = []\n",
    "        print(f\"⚠️ No annotations found for page {w_progress.value}\")\n",
    "\n",
    "\n",
    "w_bbox.on_skip(on_skip)\n",
    "\n",
    "# when Submit button is pressed we save current annotations\n",
    "# and then move on to the next file\n",
    "def on_submit():\n",
    "    image_file = files[w_progress.value]\n",
    "    # save annotations for current image\n",
    "    annotations[str(w_progress.value)] = w_bbox.bboxes\n",
    "    update_json_file(annotations, annotations_path)\n",
    "    print(f\"✓ Saved {len(w_bbox.bboxes)} annotations for page {w_progress.value}\")\n",
    "    # move on to the next file\n",
    "    on_skip()\n",
    "\n",
    "\n",
    "w_bbox.on_submit(on_submit)\n",
    "w_out = widgets.Output()\n",
    "\n",
    "def on_bbox_change(change):\n",
    "    w_out.clear_output(wait=True)\n",
    "    with w_out:\n",
    "        print(json.dumps(change[\"new\"], indent=4))\n",
    "        pageAndBoxingBoxes[w_progress.value] = change[\"new\"]\n",
    "\n",
    "\n",
    "w_bbox.observe(on_bbox_change, names=[\"bboxes\"])\n",
    "\n",
    "w_container = widgets.VBox(\n",
    "    [\n",
    "        widgets.HBox(\n",
    "            [\n",
    "                question_widget            \n",
    "            ]\n",
    "        ),\n",
    "        w_progress,\n",
    "        w_bbox,\n",
    "        w_out,\n",
    "    ]\n",
    ")\n",
    "w_container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
