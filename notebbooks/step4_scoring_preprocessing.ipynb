{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Scoring Preprocessing\n",
    "Extract handwritten responses from scanned sheets, run OCR, auto-grade with Gemini, and generate per-question review pages for manual checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grading_utils import setup_paths, create_directories\n",
    "\n",
    "prefix = \"VTC Test\"\n",
    "paths = setup_paths(prefix, \"sample\")\n",
    "\n",
    "# Extract commonly used paths\n",
    "pdf_file = paths[\"pdf_file\"]\n",
    "name_list_file = paths[\"name_list_file\"]\n",
    "marking_scheme_file = paths[\"marking_scheme_file\"]\n",
    "standard_answer = marking_scheme_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract cache for sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cd .. && tar -xzvf cache.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Vertex AI Express Mode initialized\n"
     ]
    }
   ],
   "source": [
    "from grading_utils import init_gemini_client\n",
    "\n",
    "# Initialize Gemini client\n",
    "client = init_gemini_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Vertex AI Express Mode with API Key\n",
    "\n",
    "This notebook now uses **Vertex AI Express Mode** with API key authentication instead of OAuth/ADC.\n",
    "\n",
    "**Steps to get your API key:**\n",
    "1. Visit https://aistudio.google.com/apikey\n",
    "2. Create or select your API key\n",
    "3. Copy the API key and add it to the `.env` file in the parent directory:\n",
    "   ```\n",
    "   GOOGLE_GENAI_API_KEY=your-actual-api-key-here\n",
    "   ```\n",
    "\n",
    "**Benefits of Express Mode:**\n",
    "- ✓ Simpler authentication (just an API key)\n",
    "- ✓ No need for gcloud CLI authentication\n",
    "- ✓ No service account JSON files\n",
    "- ✓ Easy to use in notebooks and scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Extract paths from setup\n",
    "file_name = paths[\"file_name\"]\n",
    "base_path = paths[\"base_path\"]\n",
    "base_path_images = paths[\"base_path_images\"]\n",
    "base_path_annotations = paths[\"base_path_annotations\"]\n",
    "base_path_questions = paths[\"base_path_questions\"]\n",
    "base_path_javascript = paths[\"base_path_javascript\"]\n",
    "\n",
    "# Create all necessary directories\n",
    "create_directories(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grading_utils import load_annotations\n",
    "\n",
    "annotations_path = base_path_annotations + \"annotations.json\"\n",
    "annotations_list, annotations_dict, questions_from_annotations = load_annotations(annotations_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NAME', 'ID', 'CLASS', 'Q1', 'Q2', 'Q3', 'Q4', 'Q5']"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use questions from loaded annotations\n",
    "questions = questions_from_annotations\n",
    "\n",
    "# Extract question_with_answer (excludes NAME, ID, CLASS)\n",
    "question_with_answer = [q for q in questions if q not in [\"NAME\", \"ID\", \"CLASS\"]]\n",
    "questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Provided Standard Answer for each question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded Name List from: ../sample/VTC Test Name List.xlsx\n",
      "✓ Loaded Marking Scheme directly\n",
      "  Columns: ['question_number', 'question_text', 'marking_scheme', 'marks']\n",
      "✓ Prepared data for scoring\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>QuestionText</th>\n",
       "      <th>Answer</th>\n",
       "      <th>Mark</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q1</td>\n",
       "      <td>The VTC is the largest provider of VPET in Hon...</td>\n",
       "      <td>- **Definition (2 marks)**: Correctly stating ...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q2</td>\n",
       "      <td>Compare IVE (Hong Kong Institute of Vocational...</td>\n",
       "      <td>- **IVE Qualification (5 marks)**: Correctly i...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q3</td>\n",
       "      <td>VTC emphasizes the \"Think and Do\" approach. Ex...</td>\n",
       "      <td>- **\"Think\" Component (3 marks)**: Explaining ...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q4</td>\n",
       "      <td>If a Secondary 6 student does not achieve the ...</td>\n",
       "      <td>- **Programme Identification (5 marks)**: Corr...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q5</td>\n",
       "      <td>Why does the VTC collaborate closely with indu...</td>\n",
       "      <td>- **General Rationale (4 marks)**: Explaining ...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Question                                       QuestionText  \\\n",
       "0       Q1  The VTC is the largest provider of VPET in Hon...   \n",
       "1       Q2  Compare IVE (Hong Kong Institute of Vocational...   \n",
       "2       Q3  VTC emphasizes the \"Think and Do\" approach. Ex...   \n",
       "3       Q4  If a Secondary 6 student does not achieve the ...   \n",
       "4       Q5  Why does the VTC collaborate closely with indu...   \n",
       "\n",
       "                                              Answer  Mark  \n",
       "0  - **Definition (2 marks)**: Correctly stating ...    10  \n",
       "1  - **IVE Qualification (5 marks)**: Correctly i...    10  \n",
       "2  - **\"Think\" Component (3 marks)**: Explaining ...    10  \n",
       "3  - **Programme Identification (5 marks)**: Corr...    10  \n",
       "4  - **General Rationale (4 marks)**: Explaining ...    10  "
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load data directly from Marking Scheme Excel\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Try to load Name List sheet from name list file\n",
    "try:\n",
    "    name_list_df = pd.read_excel(name_list_file, sheet_name=\"Name List\")\n",
    "    print(f\"✓ Loaded Name List from: {name_list_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Failed to load Name List: {e}\")\n",
    "    name_list_df = None\n",
    "\n",
    "# Load Marking Scheme sheet - this contains all the data needed\n",
    "marking_scheme_df = pd.read_excel(standard_answer, sheet_name=\"Marking Scheme\")\n",
    "print(f\"✓ Loaded Marking Scheme directly\")\n",
    "print(f\"  Columns: {list(marking_scheme_df.columns)}\")\n",
    "\n",
    "# Create Answer sheet dictionary for backward compatibility\n",
    "# Map question_number to marking_scheme for standard_answer lookup\n",
    "standard_answer_df = marking_scheme_df[['question_number', 'question_text', 'marking_scheme', 'marks']].copy()\n",
    "standard_answer_df.columns = ['Question', 'QuestionText', 'Answer', 'Mark']\n",
    "\n",
    "print(f\"✓ Prepared data for scoring\")\n",
    "standard_answer_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Covert Question to str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_answer_df[\"Question\"] = standard_answer_df[\"Question\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "from termcolor import colored\n",
    "\n",
    "# check question_with_answer in standard_answer_df Question column\n",
    "for question in question_with_answer:\n",
    "    if question not in standard_answer_df[\"Question\"].values:\n",
    "        print(colored(\"Question {} is not in standard_answer!\".format(question), 'red'))\n",
    "\n",
    "for question in standard_answer_df[\"Question\"].values:\n",
    "    if question not in question_with_answer:\n",
    "        print(colored(\"Question {} is not in annotations!\".format(question), 'red'))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Q1': '- **Definition (2 marks)**: Correctly stating \"**Vocational and Professional Education and Training**\".\\n- **Focus (4 marks)**: Explaining that it focuses on **practical skills** or **specialized trades**.\\n- **Workforce Impact (4 marks)**: Explaining the benefit to the workforce (e.g., **reducing skills gap**, **employment readiness**, supporting the economy).\\n\\n**Grading Note**: Apply the 0-10 scale rubric based on the clarity of explanation and use of terminology.\\n\\n---\\n\\n**General Grading Guide:**\\n### General Rubric for Partial Marks (0-10 Scale)\\n\\n- **9-10 marks**: The answer is complete, accurate, uses correct terminology, and is well-explained.\\n- **6-8 marks**: The answer is mostly correct but misses a specific detail (e.g., forgets the full name of a diploma) or the explanation is slightly vague.\\n- **3-5 marks**: The student shows basic understanding but misses the core point or only answers half the question.\\n- **0-2 marks**: The answer is largely incorrect, irrelevant, or blank.',\n",
       " 'Q2': '- **IVE Qualification (5 marks)**: Correctly identifying that **IVE** primarily focuses on **Higher Diploma (HD)** programmes or technical training.\\n- **THEi Qualification (5 marks)**: Correctly identifying that **THEi** focuses on vocationally-oriented **Bachelor’s Degree** programmes.\\n\\n**Grading Note**: Full marks require identifying both specific qualification types. Partial marks (6-8) if one is vague.\\n\\n---\\n\\n**General Grading Guide:**\\n### General Rubric for Partial Marks (0-10 Scale)\\n\\n- **9-10 marks**: The answer is complete, accurate, uses correct terminology, and is well-explained.\\n- **6-8 marks**: The answer is mostly correct but misses a specific detail (e.g., forgets the full name of a diploma) or the explanation is slightly vague.\\n- **3-5 marks**: The student shows basic understanding but misses the core point or only answers half the question.\\n- **0-2 marks**: The answer is largely incorrect, irrelevant, or blank.',\n",
       " 'Q3': '- **\"Think\" Component (3 marks)**: Explaining the focus on **theory** or **academic knowledge**.\\n- **\"Do\" Component (3 marks)**: Explaining the focus on **practical skills** or **hands-on** execution.\\n- **Synthesis (4 marks)**: Explaining the integration of both (e.g., **solving problems** by combining head and hands).\\n\\n**Grading Note**: A score of 9-10 requires a clear explanation of how the two components work together.\\n\\n---\\n\\n**General Grading Guide:**\\n### General Rubric for Partial Marks (0-10 Scale)\\n\\n- **9-10 marks**: The answer is complete, accurate, uses correct terminology, and is well-explained.\\n- **6-8 marks**: The answer is mostly correct but misses a specific detail (e.g., forgets the full name of a diploma) or the explanation is slightly vague.\\n- **3-5 marks**: The student shows basic understanding but misses the core point or only answers half the question.\\n- **0-2 marks**: The answer is largely incorrect, irrelevant, or blank.',\n",
       " 'Q4': '- **Programme Identification (5 marks)**: Correctly naming the \"**Diploma of Foundation Studies**\" (DFS) or \"**Diploma of Vocational Education**\" (DVE).\\n- **Progression Pathway (5 marks)**: Explaining that successful completion allows/guarantees entry to **Higher Diploma (HD)** programmes.\\n\\n**Grading Note**: If the student misses the specific name but describes the pathway, award 3-5 marks.\\n\\n---\\n\\n**General Grading Guide:**\\n### General Rubric for Partial Marks (0-10 Scale)\\n\\n- **9-10 marks**: The answer is complete, accurate, uses correct terminology, and is well-explained.\\n- **6-8 marks**: The answer is mostly correct but misses a specific detail (e.g., forgets the full name of a diploma) or the explanation is slightly vague.\\n- **3-5 marks**: The student shows basic understanding but misses the core point or only answers half the question.\\n- **0-2 marks**: The answer is largely incorrect, irrelevant, or blank.',\n",
       " 'Q5': '- **General Rationale (4 marks)**: Explaining that collaboration ensures **curriculum relevance** or meets **market trends/industry needs**.\\n- **Benefit Example 1 (3 marks)**: e.g., **Internships** or Work-integrated learning.\\n- **Benefit Example 2 (3 marks)**: e.g., **Job placement support** or access to **industry-standard equipment**.\\n\\n**Grading Note**: To achieve 9-10 marks, the student must provide two distinct and valid benefits.\\n\\n---\\n\\n**General Grading Guide:**\\n### General Rubric for Partial Marks (0-10 Scale)\\n\\n- **9-10 marks**: The answer is complete, accurate, uses correct terminology, and is well-explained.\\n- **6-8 marks**: The answer is mostly correct but misses a specific detail (e.g., forgets the full name of a diploma) or the explanation is slightly vague.\\n- **3-5 marks**: The student shows basic understanding but misses the core point or only answers half the question.\\n- **0-2 marks**: The answer is largely incorrect, irrelevant, or blank.'}"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standard_question_text = standard_answer_df.set_index(\"Question\").to_dict()[\"QuestionText\"]\n",
    "standard_answer = standard_answer_df.set_index(\"Question\").to_dict()[\"Answer\"]\n",
    "standard_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Q1': 10, 'Q2': 10, 'Q3': 10, 'Q4': 10, 'Q5': 10}"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standard_mark = standard_answer_df.set_index(\"Question\").to_dict()[\"Mark\"]\n",
    "standard_mark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for the regeneration of question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "questionAndControl = {}\n",
    "for path, currentDirectory, files in os.walk(base_path_questions):\n",
    "    for file in files:\n",
    "        if file == \"control.json\":\n",
    "            question = path[len(base_path_questions) + 1 :]\n",
    "            f = open(os.path.join(path, file))\n",
    "            data = json.load(f)\n",
    "            if \"regenerate\" in data:\n",
    "                questionAndControl[question] = data\n",
    "            f.close()\n",
    "\n",
    "questionAndControl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../marking_form/VTC Test/favicon.ico'"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "from_directory = os.path.join(os.getcwd(), \"..\",\"templates\", \"javascript\")\n",
    "shutil.copytree(from_directory, base_path_javascript, dirs_exist_ok=True)\n",
    "ico = os.path.join(os.getcwd(), \"..\",\"templates\", \"favicon.ico\")\n",
    "# copy ico file  to base_path\n",
    "shutil.copyfile(ico, base_path+\"/favicon.ico\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from jinja2 import Environment, FileSystemLoader\n",
    "import markdown\n",
    "\n",
    "file_loader = FileSystemLoader(\"../templates\")\n",
    "env = Environment(loader=file_loader)\n",
    "\n",
    "# Add markdown filter\n",
    "def markdown_filter(text):\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    return markdown.markdown(text)\n",
    "\n",
    "env.filters['markdown'] = markdown_filter\n",
    "\n",
    "template = env.get_template(\"index.html\")\n",
    "\n",
    "output = template.render(\n",
    "    studentsScriptFileName=file_name,\n",
    "    textAnswer=questions,\n",
    "    optionAnswer=[],\n",
    ")\n",
    "# open text file\n",
    "path = Path(os.path.join(base_path, \"index.html\"))\n",
    "text_file = open(path, \"w\")\n",
    "text_file.write(output)\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grading_utils import create_gemini_config\n",
    "\n",
    "def ocr(prompt: str, filePath: str):\n",
    "    \"\"\"\n",
    "    OCR function using Vertex AI Express Mode\n",
    "    \n",
    "    Args:\n",
    "        prompt: The prompt describing what to extract\n",
    "        filePath: Path to the image file\n",
    "    \n",
    "    Returns:\n",
    "        Extracted text as string\n",
    "    \"\"\"\n",
    "    # Read the image file\n",
    "    with open(filePath, \"rb\") as f:\n",
    "        data = f.read()\n",
    "    \n",
    "    # Create configuration\n",
    "    config = create_gemini_config(\n",
    "        temperature=0,\n",
    "        top_p=0.5,\n",
    "        max_output_tokens=4096,\n",
    "    )\n",
    "    \n",
    "    # Generate content\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-3-flash-preview\",\n",
    "        contents=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"parts\": [\n",
    "                    {\"inline_data\": {\"mime_type\": \"image/png\", \"data\": data}},\n",
    "                    {\"text\": prompt}\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        config=config,\n",
    "    )\n",
    "    \n",
    "    # Return extracted text (Gemini returns text in response.text)\n",
    "    if response.text:\n",
    "        return response.text.strip()\n",
    "    return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "from PIL import Image, ImageEnhance\n",
    "import hashlib\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Initialize cache directory\n",
    "cache_dir = \"../cache\"\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "def get_cache_key(cache_type, **params):\n",
    "    \"\"\"Generate cache key including model parameters\n",
    "    \n",
    "    Args:\n",
    "        cache_type: Type of cache (e.g., 'ocr', 'grade_answer', 'grade_moderator')\n",
    "        **params: Parameters to include in cache key\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (cache_type, hash_key) for folder-based caching\n",
    "    \"\"\"\n",
    "    # Create a dictionary of all parameters for hashing\n",
    "    key_data = {\n",
    "        \"type\": cache_type,\n",
    "        **params\n",
    "    }\n",
    "    # Convert to JSON string for hashing\n",
    "    key_str = json.dumps(key_data, sort_keys=True)\n",
    "    # Return tuple of cache_type and hash for folder structure\n",
    "    hash_key = hashlib.sha256(key_str.encode()).hexdigest()\n",
    "    return (cache_type, hash_key)\n",
    "\n",
    "def get_from_cache(cache_key):\n",
    "    \"\"\"Retrieve result from cache using folder structure\n",
    "    \n",
    "    Args:\n",
    "        cache_key: Tuple of (cache_type, hash_key)\n",
    "    \n",
    "    Returns:\n",
    "        Cached data or None if not found\n",
    "    \"\"\"\n",
    "    cache_type, hash_key = cache_key\n",
    "    cache_subdir = os.path.join(cache_dir, cache_type)\n",
    "    cache_file = os.path.join(cache_subdir, f\"{hash_key}.json\")\n",
    "    \n",
    "    if os.path.exists(cache_file):\n",
    "        try:\n",
    "            with open(cache_file, 'r') as f:\n",
    "                return json.load(f)\n",
    "        except:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def save_to_cache(cache_key, data):\n",
    "    \"\"\"Save result to cache using folder structure\n",
    "    \n",
    "    Args:\n",
    "        cache_key: Tuple of (cache_type, hash_key)\n",
    "        data: Data to cache\n",
    "    \"\"\"\n",
    "    cache_type, hash_key = cache_key\n",
    "    cache_subdir = os.path.join(cache_dir, cache_type)\n",
    "    \n",
    "    # Create subdirectory if it doesn't exist\n",
    "    os.makedirs(cache_subdir, exist_ok=True)\n",
    "    \n",
    "    cache_file = os.path.join(cache_subdir, f\"{hash_key}.json\")\n",
    "    try:\n",
    "        with open(cache_file, 'w') as f:\n",
    "            json.dump(data, f)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Failed to save cache - {e}\")\n",
    "\n",
    "def ocr_image_from_file(question, image_path, left, top, width, height):\n",
    "    if question == \"NAME\" :\n",
    "        return \"\"\n",
    "    \n",
    "    imageFile = tempfile.NamedTemporaryFile(suffix=\".png\").name\n",
    "    with Image.open(image_path) as im:\n",
    "        # The crop method from the Image module takes four coordinates as input.\n",
    "        # The right can also be represented as (left+width)\n",
    "        # and lower can be represented as (upper+height).\n",
    "        (left, top, right, lower) = (\n",
    "            left,\n",
    "            top,\n",
    "            left + width,\n",
    "            top + height,\n",
    "        )\n",
    "        # Here the image \"im\" is cropped and assigned to new variable im_crop\n",
    "        im_crop = im.crop((left, top, right, lower))\n",
    "        imageEnhance = ImageEnhance.Sharpness(im_crop)\n",
    "        # showing resultant image\n",
    "        im_crop = imageEnhance.enhance(3)\n",
    "        im_crop.save(imageFile, format=\"png\")\n",
    "        \n",
    "    if question == \"ID\" :\n",
    "        text_message = \"\"\"\n",
    "            Extract text in this image.\n",
    "            It is a Student ID in 9 digit number.\n",
    "            Return only the 9-digit Student ID with no other words, no bullets, and no numbering.\n",
    "            Strip whitespace. If you cannot extract Student ID, please return 'No text found!!!'.\n",
    "            \"\"\"\n",
    "    elif question == \"CLASS\":\n",
    "        text_message = \"\"\"\n",
    "            Extract the class code from this image. It is printed/computer text, typically letters and numbers (e.g., \"3A\", \"S1\", \"Class B\").\n",
    "            Return only the class value with no other words, no bullets, and no numbering. Strip whitespace.\n",
    "            If you cannot extract the class value, please return 'No text found!!!'.\n",
    "            \"\"\"\n",
    "    else:    \n",
    "        text_message =\"\"\"\n",
    "            Extract only the handwritten text from this image (English, numbers, math symbols, and common punctuation).\n",
    "            Ignore printed or pre-printed computer text, headers, labels, or barcodes; capture handwriting only.\n",
    "            Preserve the student's original formatting: keep existing line breaks and any numbering or bullets that appear in the handwriting (e.g., \"1.\", \"2.\", \"- \"). Do not introduce new numbering or bullets.\n",
    "            Return exactly the extracted handwritten text with no added labels, summaries, paraphrasing, or translation.\n",
    "            Strip leading/trailing whitespace on each line. If you cannot extract text, please return 'No text found!!!'.\"\"\"       \n",
    "\n",
    "    try:\n",
    "        # Compute file hash for cache key\n",
    "        with open(imageFile, 'rb') as f:\n",
    "            file_hash = hashlib.sha256(f.read()).hexdigest()\n",
    "        \n",
    "        # Check cache\n",
    "        cache_key = get_cache_key(\"ocr\", model=\"gemini-3-flash-preview\", prompt=text_message, image_hash=file_hash, temperature=0, top_p=0.5)\n",
    "        cached_result = get_from_cache(cache_key)\n",
    "        \n",
    "        if cached_result is not None:\n",
    "            print(f\"[CACHE] {question} {image_path} {cached_result}\")\n",
    "            ocr_text = cached_result.get(\"result\", \"\")\n",
    "        else:\n",
    "            ocr_text = ocr(text_message, imageFile)\n",
    "            # Save to cache\n",
    "            save_to_cache(cache_key, {\"result\": ocr_text})\n",
    "            print(f\"[NEW] {question} {image_path} {ocr_text}\")\n",
    "        \n",
    "        if ocr_text == \"No text found!!!\":\n",
    "            return \"\"\n",
    "        return ocr_text\n",
    "    except Exception as e:\n",
    "        print(question, image_path, e)    \n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from grading_utils import create_gemini_config\n",
    "\n",
    "class GradingResult(BaseModel):\n",
    "    \"\"\"Pydantic model for grading results - used both as response schema and data container\"\"\"\n",
    "    similarity_score: float = Field(description=\"Similarity score from 0 to 1\")\n",
    "    mark: float = Field(description=\"Actual mark awarded based on marking scheme\")\n",
    "    reasoning: str = Field(description=\"Brief explanation of the score\")\n",
    "\n",
    "\n",
    "def grade_answer(question_text, submitted_answer, marking_scheme_text, total_marks):\n",
    "    \"\"\"\n",
    "    Grade a student's answer using Vertex AI Express Mode with structured output\n",
    "    \n",
    "    Args:\n",
    "        question_text: The original question text\n",
    "        submitted_answer: The student's answer\n",
    "        marking_scheme_text: Detailed marking scheme/rubric with correct answer\n",
    "        total_marks: Total marks available\n",
    "    \n",
    "    Returns:\n",
    "        GradingResult object with similarity_score, mark and reasoning\n",
    "    \"\"\"\n",
    "    # Check cache first\n",
    "    cache_key = get_cache_key(\n",
    "        \"grade_answer\",\n",
    "        model=\"gemini-3-flash-preview\",\n",
    "        temperature=0,\n",
    "        top_p=0.3,\n",
    "        max_output_tokens=8192,\n",
    "        question=question_text,\n",
    "        answer=submitted_answer,\n",
    "        scheme=marking_scheme_text,\n",
    "        marks=total_marks\n",
    "    )\n",
    "    \n",
    "    cached_result = get_from_cache(cache_key)\n",
    "    if cached_result is not None:\n",
    "        return GradingResult(**cached_result)\n",
    "    \n",
    "    prompt = f'''You are an expert grader. Evaluate the student's answer based on the question and marking scheme provided.\n",
    "\n",
    "<QUESTION>\n",
    "{question_text}\n",
    "</QUESTION>\n",
    "\n",
    "<MARKING_SCHEME>\n",
    "{marking_scheme_text}\n",
    "</MARKING_SCHEME>\n",
    "\n",
    "<TOTAL_MARKS>\n",
    "{total_marks}\n",
    "</TOTAL_MARKS>\n",
    "\n",
    "<STUDENT_ANSWER>\n",
    "{submitted_answer}\n",
    "</STUDENT_ANSWER>\n",
    "\n",
    "Evaluate how well the student's answer matches the marking scheme for this specific question.\n",
    "Consider partial credit possibilities outlined in the marking scheme.\n",
    "Provide:\n",
    "1. reasoning: Brief explanation of the scoring (think through this first)\n",
    "2. similarity_score: A score from 0 to 1 indicating how well the answer matches\n",
    "3. mark: The actual mark to award (from 0 to {total_marks})'''\n",
    "    \n",
    "    config = create_gemini_config(\n",
    "        temperature=0,\n",
    "        top_p=0.3,\n",
    "        max_output_tokens=1024 * 8,\n",
    "        response_mime_type=\"application/json\",\n",
    "        response_schema=GradingResult,\n",
    "    )\n",
    "    \n",
    "    retry = 0\n",
    "    while retry < 3:\n",
    "        try:\n",
    "            response = client.models.generate_content(\n",
    "                model=\"gemini-3-flash-preview\",\n",
    "                contents=[{\"role\": \"user\", \"parts\": [{\"text\": prompt}]}],\n",
    "                config=config,\n",
    "            )\n",
    "            \n",
    "            # Extract score and reasoning from structured response\n",
    "            if hasattr(response, 'parsed') and response.parsed is not None:\n",
    "                # Return the parsed Pydantic model directly\n",
    "                similarity_score_val = max(0.0, min(1.0, response.parsed.similarity_score))\n",
    "                mark = max(0.0, min(float(total_marks), response.parsed.mark))\n",
    "                result = GradingResult(similarity_score=similarity_score_val, mark=mark, reasoning=response.parsed.reasoning)\n",
    "                # Save to cache using model_dump() for Pydantic v2 compatibility\n",
    "                save_to_cache(cache_key, result.model_dump())\n",
    "                return result\n",
    "            else:\n",
    "                # Fallback to text parsing if structured output not available\n",
    "                text = response.text if response.text else \"\"\n",
    "                try:\n",
    "                    import json\n",
    "                    parsed = json.loads(text)\n",
    "                    similarity_score_val = float(parsed.get('similarity_score', 0))\n",
    "                    mark = float(parsed.get('mark', 0))\n",
    "                    reasoning = parsed.get('reasoning', 'N/A')\n",
    "                    similarity_score_val = max(0.0, min(1.0, similarity_score_val))\n",
    "                    mark = max(0.0, min(float(total_marks), mark))\n",
    "                    result = GradingResult(similarity_score=similarity_score_val, mark=mark, reasoning=reasoning)\n",
    "                    # Save to cache using model_dump() for Pydantic v2 compatibility\n",
    "                    save_to_cache(cache_key, result.model_dump())\n",
    "                    return result\n",
    "                except:\n",
    "                    print(\"Retry\")\n",
    "                    retry += 1\n",
    "                    continue\n",
    "        except Exception as e:\n",
    "            print(f\"Error in grade_answer: {e}\")\n",
    "            retry += 1\n",
    "            continue\n",
    "    \n",
    "    return GradingResult(similarity_score=0, mark=0, reasoning=\"Error: Could not retrieve scoring\")\n",
    "\n",
    "\n",
    "def grade_answers(answers, question):\n",
    "    \"\"\"\n",
    "    Grade multiple answers for a question\n",
    "    \n",
    "    Args:\n",
    "        answers: List of student answers\n",
    "        question: Question label/name\n",
    "    \n",
    "    Returns:\n",
    "        List of GradingResult objects\n",
    "    \"\"\"\n",
    "    # Get question text, marking scheme, and marks - must exist for all questions\n",
    "    # Handle case mismatch (e.g., 'NAME' vs 'Name')\n",
    "    question_text = standard_question_text.get(question, \"\")\n",
    "    marking_scheme_text = standard_answer.get(question, \"\")\n",
    "    total_marks = standard_mark.get(question, 0)\n",
    "    \n",
    "    # If not found, try case-insensitive lookup\n",
    "    if not marking_scheme_text or not question_text:\n",
    "        question_lower = question.lower()\n",
    "        for key in standard_answer.keys():\n",
    "            if key.lower() == question_lower:\n",
    "                marking_scheme_text = standard_answer[key]\n",
    "                question_text = standard_question_text.get(key, \"\")\n",
    "                total_marks = standard_mark.get(key, 0)\n",
    "                break\n",
    "    \n",
    "    results = []\n",
    "    for submitted_answer in answers:\n",
    "        submitted_answer = str(submitted_answer)\n",
    "        if submitted_answer.strip() == \"\":\n",
    "            results.append(GradingResult(similarity_score=0, mark=0, reasoning=\"Empty answer\"))\n",
    "            continue\n",
    "        result = grade_answer(question_text, submitted_answer, marking_scheme_text, total_marks)\n",
    "        results.append(result)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class ModerationItem(BaseModel):\n",
    "    \"\"\"Individual moderation result for one answer\"\"\"\n",
    "    moderated_mark: float = Field(description=\"Final moderated mark\")\n",
    "    flag: bool = Field(description=\"True if adjusted or needs review\")\n",
    "    note: str = Field(description=\"Short reason for moderation\")\n",
    "\n",
    "\n",
    "class ModerationResponse(BaseModel):\n",
    "    \"\"\"Response containing all moderation items\"\"\"\n",
    "    items: List[ModerationItem] = Field(description=\"List of moderation items\")\n",
    "\n",
    "\n",
    "def grade_moderator(question, answers, grading_results, row_numbers):\n",
    "    \"\"\"\n",
    "    Use Gemini to harmonize marks across similar answers for a single question.\n",
    "    \n",
    "    This function ensures consistency by identifying similar answers that received\n",
    "    different marks and adjusting them to be fair and consistent.\n",
    "    \n",
    "    Args:\n",
    "        question: Question label/name\n",
    "        answers: List of student answers (strings)\n",
    "        grading_results: List of GradingResult objects from grade_answers\n",
    "        row_numbers: List of row numbers for student identification (required)\n",
    "    \n",
    "    Returns:\n",
    "        List of dicts with keys: moderated_mark, flag, note\n",
    "    \"\"\"\n",
    "    # Validate row_numbers is provided\n",
    "    if not row_numbers or len(row_numbers) != len(answers):\n",
    "        raise ValueError(f\"row_numbers is required and must match the number of answers ({len(answers)})\")\n",
    "    \n",
    "    # Get question metadata\n",
    "    question_text = standard_question_text.get(question, \"\")\n",
    "    marking_scheme_text = standard_answer.get(question, \"\")\n",
    "    total_marks = standard_mark.get(question, 0)\n",
    "\n",
    "    # Prepare entries for moderation with row numbers\n",
    "    entries = []\n",
    "    for row_num, ans, res in zip(row_numbers, answers, grading_results):\n",
    "        entries.append({\n",
    "            \"row\": int(row_num),\n",
    "            \"answer\": str(ans or \"\"),\n",
    "            \"mark\": float(res.mark),\n",
    "            \"reasoning\": str(res.reasoning or \"\"),\n",
    "        })\n",
    "\n",
    "    # Check cache\n",
    "    cache_key = get_cache_key(\n",
    "        \"grade_moderator\",\n",
    "        model=\"gemini-3-pro-preview\",\n",
    "        temperature=0,\n",
    "        top_p=0.3,\n",
    "        question=question_text,\n",
    "        scheme=marking_scheme_text,\n",
    "        total_marks=total_marks,\n",
    "        entries=entries,\n",
    "    )\n",
    "    \n",
    "    cached = get_from_cache(cache_key)\n",
    "    if cached is not None:\n",
    "        return cached\n",
    "\n",
    "    # Build moderation prompt\n",
    "    prompt = f\"\"\"You are a grading moderator ensuring fairness and consistency.\n",
    "\n",
    "Question: {question_text}\n",
    "Marking scheme: {marking_scheme_text}\n",
    "Total marks: {total_marks}\n",
    "\n",
    "You are given {len(entries)} student responses with their current marks and grading reasons. \n",
    "Your task is to review all responses and ensure that similar answers receive similar marks.\n",
    "\n",
    "For each response, decide:\n",
    "1. Should the mark be adjusted for consistency with peer responses?\n",
    "2. Does it need human review?\n",
    "\n",
    "Return a JSON object with an \"items\" array of exactly {len(entries)} objects in the same order.\n",
    "Each object must have:\n",
    "- \"moderated_mark\": number between 0 and {total_marks}\n",
    "- \"flag\": boolean (true if you adjusted the mark or want human review, false otherwise)\n",
    "- \"note\": string (max 120 chars) explaining your decision; when referencing peers, use their row number (e.g., \"row 2\", \"row 4\")\n",
    "\n",
    "Be concise and fair. Maintain ordering.\"\"\"\n",
    "\n",
    "    content = json.dumps(entries, ensure_ascii=False)\n",
    "    \n",
    "    # Create Gemini config with structured output\n",
    "    config = create_gemini_config(\n",
    "        temperature=0,\n",
    "        top_p=0.3,\n",
    "        max_output_tokens=65535,\n",
    "        response_mime_type=\"application/json\",\n",
    "        response_schema=ModerationResponse,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        response = client.models.generate_content(\n",
    "            model=\"gemini-3-pro-preview\",\n",
    "            contents=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"parts\": [\n",
    "                        {\"text\": prompt},\n",
    "                        {\"text\": \"\\nResponses:\\n\" + content},\n",
    "                    ],\n",
    "                }\n",
    "            ],\n",
    "            config=config,\n",
    "        )\n",
    "        \n",
    "        moderation = []\n",
    "        \n",
    "        # Try to use structured output first\n",
    "        if hasattr(response, 'parsed') and response.parsed is not None:\n",
    "            for item in response.parsed.items:\n",
    "                moderated_mark = max(0.0, min(float(total_marks), float(item.moderated_mark)))\n",
    "                flag = bool(item.flag)\n",
    "                note = str(item.note)\n",
    "                moderation.append({\n",
    "                    \"moderated_mark\": moderated_mark,\n",
    "                    \"flag\": flag,\n",
    "                    \"note\": note,\n",
    "                })\n",
    "        else:\n",
    "            # Fallback to text parsing\n",
    "            text = response.text if hasattr(response, \"text\") else None\n",
    "            if not text:\n",
    "                raise ValueError(\"Empty Gemini response\")\n",
    "            \n",
    "            parsed = json.loads(text)\n",
    "            parsed_items = parsed.get(\"items\", parsed) if isinstance(parsed, dict) else parsed\n",
    "            \n",
    "            for item, original in zip(parsed_items, entries):\n",
    "                try:\n",
    "                    moderated_mark = max(0.0, min(float(total_marks), float(item.get(\"moderated_mark\", original[\"mark\"]))))\n",
    "                    flag = bool(item.get(\"flag\", False))\n",
    "                    note = str(item.get(\"note\", \"\"))\n",
    "                except Exception:\n",
    "                    # If parsing fails, keep original mark\n",
    "                    moderated_mark = float(original[\"mark\"])\n",
    "                    flag = False\n",
    "                    note = \"parse_error\"\n",
    "                \n",
    "                moderation.append({\n",
    "                    \"moderated_mark\": moderated_mark,\n",
    "                    \"flag\": flag,\n",
    "                    \"note\": note,\n",
    "                })\n",
    "        \n",
    "        # Save to cache\n",
    "        save_to_cache(cache_key, moderation)\n",
    "        return moderation\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"grade_moderator error for question '{question}': {e}\")\n",
    "        # Fallback: return original marks without moderation\n",
    "        return [\n",
    "            {\n",
    "                \"moderated_mark\": float(res.mark),\n",
    "                \"flag\": False,\n",
    "                \"note\": \"moderation_error\",\n",
    "            }\n",
    "            for res in grading_results\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_the_list_of_files(path):\n",
    "    \"\"\"\n",
    "    Get the list of files in the directory\n",
    "    \"\"\"\n",
    "    files = []\n",
    "    for dirpath, dirnames, filenames in os.walk(path):\n",
    "        files.extend(filenames)\n",
    "        break\n",
    "    return sorted(files)\n",
    "\n",
    "\n",
    "images = get_the_list_of_files(base_path_images)\n",
    "\n",
    "# get max page from annotations_list\n",
    "max_page = 0\n",
    "for annotation in annotations_list:\n",
    "    if annotation[\"page\"] > max_page:\n",
    "        max_page = annotation[\"page\"]\n",
    "max_page = max_page + (1 if max_page % 2 == 1 else max_page + 2) # Scanner will have a blank page!\n",
    "\n",
    "# filter images by file name divided by page\n",
    "images_by_page = []\n",
    "for page in range(max_page):\n",
    "    images_by_page.append([])\n",
    "    for image in images:\n",
    "        p = int(image.split(\".\")[0])\n",
    "        if p % max_page == page:\n",
    "            images_by_page[page].append(image)\n",
    "\n",
    "\n",
    "def get_df(question):\n",
    "    row = annotations_dict[question].copy()\n",
    "    row[\"Similarity\"] = 0\n",
    "    row[\"Reasoning\"] = \"\"\n",
    "    row[\"Image\"] = images_by_page[row[\"page\"]]\n",
    "    # append base_path_images to each image\n",
    "    row[\"Image\"] = [\"images/\" + image for image in row[\"Image\"]]\n",
    "\n",
    "    # expend row to dataframe for each image in row[\"Image\"]\n",
    "    data = pd.DataFrame(row)\n",
    "    data = data.explode(\"Image\")\n",
    "    data = data.reset_index(drop=True)\n",
    "\n",
    "    data[\"Answer\"] = data.apply(\n",
    "        lambda row: ocr_image_from_file(question,\n",
    "            base_path + \"/\" + row[\"Image\"],\n",
    "            row[\"left\"],\n",
    "            row[\"top\"],\n",
    "            row[\"width\"],\n",
    "            row[\"height\"],\n",
    "        ),\n",
    "        axis=1,\n",
    "    )\n",
    "    # add column RowNumber\n",
    "    data[\"RowNumber\"] = data.index + 1\n",
    "    data[\"maskPage\"] = data[\"page\"]\n",
    "\n",
    "    # Check if this is a metadata question (ID, NAME, CLASS) - no grading needed\n",
    "    if question in [\"ID\", \"NAME\", \"CLASS\"]:\n",
    "        # For metadata questions, just populate with zeros/empty values\n",
    "        data[\"Similarity\"] = 0.0\n",
    "        data[\"Reasoning\"] = \"\"\n",
    "        data[\"MarkRaw\"] = 0.0\n",
    "        data[\"Mark\"] = 0.0\n",
    "        data[\"ModeratorFlag\"] = False\n",
    "        data[\"ModeratorNote\"] = \"\"\n",
    "    else:\n",
    "        # For regular questions, perform grading and moderation\n",
    "        scoring_results = grade_answers(data[\"Answer\"].tolist(), question)\n",
    "        \n",
    "        # Use row numbers for moderation (reliable, unlike OCR'd student IDs)\n",
    "        row_numbers = data[\"RowNumber\"].tolist()\n",
    "        \n",
    "        # Apply moderation to harmonize marks across similar answers\n",
    "        moderation = grade_moderator(question, data[\"Answer\"].tolist(), scoring_results, row_numbers)\n",
    "        \n",
    "        # Extract all fields from GradingResult objects and moderation\n",
    "        data[\"Similarity\"] = [result.similarity_score for result in scoring_results]\n",
    "        data[\"Reasoning\"] = [result.reasoning for result in scoring_results]\n",
    "        data[\"MarkRaw\"] = [result.mark for result in scoring_results]\n",
    "        data[\"Mark\"] = [m[\"moderated_mark\"] for m in moderation]\n",
    "        data[\"ModeratorFlag\"] = [m[\"flag\"] for m in moderation]\n",
    "        data[\"ModeratorNote\"] = [m[\"note\"] for m in moderation]\n",
    "\n",
    "    data[\"page\"] = data[\"Image\"].apply(\n",
    "        lambda x: x.replace(\"images/\", \"\").replace(\".jpg\", \"\")\n",
    "    )\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def save_template_output(output, question, filename):\n",
    "    path = Path(base_path_questions, question)\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "    path = Path(os.path.join(path, filename))\n",
    "    text_file = open(path, \"w\")\n",
    "    text_file.write(output)\n",
    "    text_file.close()\n",
    "\n",
    "\n",
    "# question = \"NAME\"\n",
    "# get_df(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate individual question page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "299e69932f7d43d884df4570e25b389a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=8)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CACHE] ID ../marking_form/VTC Test/images/0.jpg {'result': '123456789'}\n",
      "[CACHE] ID ../marking_form/VTC Test/images/2.jpg {'result': '987654321'}\n",
      "[CACHE] ID ../marking_form/VTC Test/images/4.jpg {'result': '234567890'}\n",
      "[CACHE] ID ../marking_form/VTC Test/images/6.jpg {'result': '345678912'}\n",
      "[CACHE] CLASS ../marking_form/VTC Test/images/0.jpg {'result': 'A'}\n",
      "[CACHE] CLASS ../marking_form/VTC Test/images/2.jpg {'result': 'B'}\n",
      "[CACHE] CLASS ../marking_form/VTC Test/images/4.jpg {'result': 'C'}\n",
      "[CACHE] CLASS ../marking_form/VTC Test/images/6.jpg {'result': 'D'}\n",
      "[CACHE] Q1 ../marking_form/VTC Test/images/0.jpg {'result': 'Vocational and Professional\\nEducation and Traing'}\n",
      "[CACHE] Q1 ../marking_form/VTC Test/images/2.jpg {'result': 'Vacational and professional\\nEduate Training'}\n",
      "[CACHE] Q1 ../marking_form/VTC Test/images/4.jpg {'result': 'Hong Kong skilled labor force'}\n",
      "[CACHE] Q1 ../marking_form/VTC Test/images/6.jpg {'result': 'Vocational and Professional\\nEducation and Training'}\n",
      "[CACHE] Q2 ../marking_form/VTC Test/images/0.jpg {'result': 'IVE is Highed Diploma\\nTHEi is Degree'}\n",
      "[CACHE] Q2 ../marking_form/VTC Test/images/2.jpg {'result': 'HD is IVE\\nDegree is THEi'}\n",
      "[CACHE] Q2 ../marking_form/VTC Test/images/4.jpg {'result': 'IVE is VTC\\nthei is also VTC'}\n",
      "[CACHE] Q2 ../marking_form/VTC Test/images/6.jpg {'result': 'higher Diploma for IVE\\nDegree for THEi'}\n",
      "[CACHE] Q3 ../marking_form/VTC Test/images/0.jpg {'result': 'thinking and doing'}\n",
      "[CACHE] Q3 ../marking_form/VTC Test/images/2.jpg {'result': \"Sorry I don't know\"}\n",
      "[CACHE] Q3 ../marking_form/VTC Test/images/4.jpg {'result': 'brainpower to doing\\nhand-on'}\n",
      "[CACHE] Q3 ../marking_form/VTC Test/images/6.jpg {'result': 'Yeah'}\n",
      "[CACHE] Q4 ../marking_form/VTC Test/images/1.jpg {'result': 'DFS -> Higher Diploma'}\n",
      "[CACHE] Q4 ../marking_form/VTC Test/images/3.jpg {'result': 'No text found!!'}\n",
      "[CACHE] Q4 ../marking_form/VTC Test/images/5.jpg {'result': 'Ha ha good'}\n",
      "[CACHE] Q4 ../marking_form/VTC Test/images/7.jpg {'result': 'No text found!!'}\n",
      "[CACHE] Q5 ../marking_form/VTC Test/images/1.jpg {'result': 'Intenship'}\n",
      "[CACHE] Q5 ../marking_form/VTC Test/images/3.jpg {'result': 'No text found!!'}\n",
      "[CACHE] Q5 ../marking_form/VTC Test/images/5.jpg {'result': 'Intern, placement, industry'}\n",
      "[CACHE] Q5 ../marking_form/VTC Test/images/7.jpg {'result': 'No text found!!'}\n"
     ]
    }
   ],
   "source": [
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "\n",
    "max_count = len(questions)\n",
    "f = IntProgress(min=0, max=max_count) # instantiate the bar\n",
    "display(f) # display the bar\n",
    "\n",
    "for question in questions:\n",
    "    dataTable = get_df(question)\n",
    "    os.makedirs(base_path_questions + \"/\" + question, exist_ok=True)\n",
    "    dataTable.to_csv(base_path_questions + \"/\" + question + \"/data.csv\", index=False)\n",
    "\n",
    "    if question == \"ID\" or question == \"NAME\" or question == \"CLASS\":\n",
    "        template = env.get_template(\"questions/index-answer.html\")\n",
    "    else:\n",
    "        template = env.get_template(\"questions/index.html\")\n",
    "    output = template.render(\n",
    "        studentsScriptFileName=file_name,\n",
    "        question=question,\n",
    "        standardAnswer=standard_answer[question] if question in standard_answer else \"\",\n",
    "        standardMark=standard_mark[question] if question in standard_mark else \"\",\n",
    "        estimatedBoundingBox=annotations_dict[question],\n",
    "        dataTable=dataTable,\n",
    "    )\n",
    "    save_template_output(output, question, \"index.html\")\n",
    "\n",
    "    template = env.get_template(\"questions/question.js\")\n",
    "    output = template.render(\n",
    "        dataTable=dataTable,\n",
    "        estimatedBoundingBox=annotations_dict[question],\n",
    "    )\n",
    "    save_template_output(output, question, \"question.js\")\n",
    "\n",
    "    template = env.get_template(\"questions/style.css\")\n",
    "    output = template.render(\n",
    "        dataTable=dataTable,\n",
    "    )\n",
    "    save_template_output(output, question, \"style.css\")\n",
    "    f.value += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ce133e559d6479a867c122f180ec55c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=8)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "\n",
    "max_count = len(questions)\n",
    "f = IntProgress(min=0, max=max_count) # instantiate the bar\n",
    "display(f) # display the bar\n",
    "\n",
    "for question in questions:\n",
    "    data_path = base_path_questions + \"/\" + question + \"/data.csv\"\n",
    "    dataTable = pd.read_csv(data_path)\n",
    "    dataTable = dataTable.replace(\".*No text found!!!.*\", \"\", regex=True)\n",
    "    \n",
    "    scoring_results = grade_answers(dataTable[\"Answer\"].tolist(), question)\n",
    "    dataTable[\"Similarity\"] = [result.similarity_score for result in scoring_results]\n",
    "    dataTable[\"Reasoning\"] = [result.reasoning for result in scoring_results]\n",
    "    \n",
    "    dataTable.to_csv(base_path_questions + \"/\" + question + \"/data.csv\", index=False)\n",
    "\n",
    "    if question == \"ID\" or question == \"NAME\" or question == \"CLASS\":\n",
    "        template = env.get_template(\"questions/index-answer.html\")\n",
    "    else:\n",
    "        template = env.get_template(\"questions/index.html\")\n",
    "    output = template.render(\n",
    "        studentsScriptFileName=file_name,\n",
    "        question=question,\n",
    "        standardAnswer=standard_answer[question] if question in standard_answer else \"\",\n",
    "        standardMark=standard_mark[question] if question in standard_mark else \"\",\n",
    "        estimatedBoundingBox=annotations_dict[question],\n",
    "        dataTable=dataTable,\n",
    "    )\n",
    "    save_template_output(output, question, \"index.html\")\n",
    "\n",
    "    template = env.get_template(\"questions/question.js\")\n",
    "    output = template.render(\n",
    "        dataTable=dataTable,\n",
    "        estimatedBoundingBox=annotations_dict[question],\n",
    "    )\n",
    "    save_template_output(output, question, \"question.js\")\n",
    "\n",
    "    template = env.get_template(\"questions/style.css\")\n",
    "    output = template.render(\n",
    "        dataTable=dataTable,\n",
    "    )\n",
    "    save_template_output(output, question, \"style.css\")\n",
    "    f.value += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Student ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csv file to dataframe\n",
    "import pandas as pd\n",
    "\n",
    "id_from_oscr = pd.read_csv(base_path_questions + \"/\" + \"ID\" + \"/data.csv\")[\"Answer\"].tolist()\n",
    "id_from_oscr = [str(int(float(x))) if pd.notna(x) else x for x in id_from_oscr]\n",
    "\n",
    "id_from_namelist = name_list_df[\"ID\"].to_list()\n",
    "\n",
    "# check duplicate id\n",
    "duplicate_id = []\n",
    "for id in id_from_oscr:\n",
    "    if id_from_oscr.count(id) > 1:\n",
    "        duplicate_id.append(id)\n",
    "duplicate_id = list(set(duplicate_id))\n",
    "if len(duplicate_id) > 0:\n",
    "    print(colored(\"Duplicate ID: {}\".format(duplicate_id), \"red\"))\n",
    "\n",
    "id_from_oscr = [str(id) for id in id_from_oscr]\n",
    "id_from_namelist = [str(id) for id in id_from_namelist]\n",
    "\n",
    "# compare oscr_id and validate_id\n",
    "ocr_missing_id = []\n",
    "name_list_missing_id = []\n",
    "for id in id_from_oscr:    \n",
    "    if id not in id_from_namelist:       \n",
    "        name_list_missing_id.append(id)\n",
    "\n",
    "for id in id_from_namelist:\n",
    "    if id not in id_from_oscr:   \n",
    "        ocr_missing_id.append(id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OCR scan error case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "from termcolor import colored\n",
    "if len(ocr_missing_id) > 0:\n",
    "    print(colored(\"Some IDs OCR is not in NameList and you need to fix it manually!\", \"red\"))\n",
    "    for id in name_list_missing_id:\n",
    "        print(colored(id, \"red\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Potential Absent Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "from termcolor import colored\n",
    "\n",
    "if len(ocr_missing_id) > 0:\n",
    "    print(colored(\"Number of absentee {}.\".format(len(ocr_missing_id)), \"red\"))\n",
    "    print(colored(\"ID in Name List does not find from OCR!\", \"red\"))\n",
    "    for id in ocr_missing_id:\n",
    "        print(colored(id, \"red\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Python HTTPServer\n",
    "\n",
    "The webserver log is in output/server.log.\n",
    "\n",
    "If you are in development and don't want the notebook being blocked by running webserver, you can open a terminal and run the below command.\n",
    "\n",
    "file_name=XXXX python server.py 8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_name=\"VTC Test\" python server.py\n"
     ]
    }
   ],
   "source": [
    "print(\"file_name=\\\"{}\\\" python server.py\".format(file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also uncomment the following line to run the web server but if it crashes, you need to restart the kernel.\n",
    "# !cd .. && file_name=TestScript python server.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
