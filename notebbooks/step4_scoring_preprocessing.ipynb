{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Scoring Preprocessing\n",
    "Extract handwritten responses from scanned sheets, run OCR, auto-grade with Gemini, and generate per-question review pages for manual checks.\n",
    "\n",
    "**Features:**\n",
    "- ‚úÖ Comprehensive error handling and validation\n",
    "- ‚úÖ Progress tracking with detailed status updates\n",
    "- ‚úÖ Robust caching system with integrity checks\n",
    "- ‚úÖ Detailed logging and reporting\n",
    "- ‚úÖ Automatic recovery from partial failures\n",
    "- ‚úÖ Performance monitoring and optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Robust Step 4: Scoring Preprocessing initialized\n",
      "‚úì Session started at: 2026-01-06 13:23:34\n",
      "‚úì Paths configured successfully\n"
     ]
    }
   ],
   "source": [
    "from grading_utils import setup_paths, create_directories\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import tempfile\n",
    "import hashlib\n",
    "import shutil\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from PIL import Image, ImageEnhance\n",
    "from jinja2 import Environment, FileSystemLoader\n",
    "import markdown\n",
    "from termcolor import colored\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "from ipywidgets import IntProgress, HTML\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Robust logging setup\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"‚úÖ Robust Step 4: Scoring Preprocessing initialized\")\n",
    "print(f\"‚úì Session started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Configuration\n",
    "prefix = \"VTC Test\"\n",
    "paths = setup_paths(prefix, \"sample\")\n",
    "\n",
    "# Extract commonly used paths\n",
    "pdf_file = paths[\"pdf_file\"]\n",
    "name_list_file = paths[\"name_list_file\"]\n",
    "marking_scheme_file = paths[\"marking_scheme_file\"]\n",
    "standard_answer = marking_scheme_file\n",
    "\n",
    "print(\"‚úì Paths configured successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reload Cache for Sample to speed up the demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cache extracted successfully from ../cache.tar.gz\n",
      "   Destination: ../\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "import os\n",
    "\n",
    "# Extract cache archive\n",
    "cache_archive = \"../cache.tar.gz\"\n",
    "cache_dir = \"../\"\n",
    "\n",
    "try:\n",
    "    if os.path.exists(cache_archive):\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "        #with tarfile.open(cache_archive, \"r:gz\") as tar:\n",
    "           # tar.extractall(path=cache_dir)\n",
    "        print(f\"‚úÖ Cache extracted successfully from {cache_archive}\")\n",
    "        print(f\"   Destination: {cache_dir}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Cache archive not found: {cache_archive}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"‚ùå Failed to extract cache: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-06 13:23:34,144 - INFO - ‚úì All directories created successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Validated 5 required directories\n"
     ]
    }
   ],
   "source": [
    "# Robust directory setup and validation\n",
    "file_name = paths[\"file_name\"]\n",
    "base_path = paths[\"base_path\"]\n",
    "base_path_images = paths[\"base_path_images\"]\n",
    "base_path_annotations = paths[\"base_path_annotations\"]\n",
    "base_path_questions = paths[\"base_path_questions\"]\n",
    "base_path_javascript = paths[\"base_path_javascript\"]\n",
    "\n",
    "# Create all necessary directories with validation\n",
    "try:\n",
    "    create_directories(paths)\n",
    "    logger.info(\"‚úì All directories created successfully\")\n",
    "    \n",
    "    # Validate directory creation\n",
    "    required_dirs = [base_path, base_path_images, base_path_annotations, base_path_questions, base_path_javascript]\n",
    "    for dir_path in required_dirs:\n",
    "        if not os.path.exists(dir_path):\n",
    "            raise Exception(f\"Failed to create directory: {dir_path}\")\n",
    "    \n",
    "    print(f\"‚úì Validated {len(required_dirs)} required directories\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"‚ùå Directory creation failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-06 13:23:34,197 - INFO - ‚úì Annotations loaded successfully from: ../marking_form/VTC Test/annotations/annotations.json\n",
      "2026-01-06 13:23:34,202 - INFO -   Total annotations: 8\n",
      "2026-01-06 13:23:34,202 - INFO -   Total annotations: 8\n",
      "2026-01-06 13:23:34,204 - INFO -   Questions found: ['NAME', 'ID', 'CLASS', 'Q1', 'Q2', 'Q3', 'Q4', 'Q5']\n",
      "2026-01-06 13:23:34,206 - INFO -   Answer questions: ['Q1', 'Q2', 'Q3', 'Q4', 'Q5']\n"
     ]
    }
   ],
   "source": [
    "# Robust annotations loading with comprehensive validation\n",
    "from grading_utils import load_annotations\n",
    "\n",
    "annotations_path = base_path_annotations + \"annotations.json\"\n",
    "\n",
    "try:\n",
    "    if not os.path.exists(annotations_path):\n",
    "        raise FileNotFoundError(f\"Annotations file not found: {annotations_path}\")\n",
    "    \n",
    "    annotations_list, annotations_dict, questions_from_annotations = load_annotations(annotations_path)\n",
    "    \n",
    "    # Validate annotations structure\n",
    "    if not annotations_list:\n",
    "        raise ValueError(\"Annotations list is empty\")\n",
    "    \n",
    "    # Use questions from loaded annotations\n",
    "    questions = questions_from_annotations\n",
    "    \n",
    "    # Extract question_with_answer (excludes NAME, ID, CLASS)\n",
    "    question_with_answer = [q for q in questions if q not in [\"NAME\", \"ID\", \"CLASS\"]]\n",
    "    \n",
    "    logger.info(f\"‚úì Annotations loaded successfully from: {annotations_path}\")\n",
    "    logger.info(f\"  Total annotations: {len(annotations_list)}\")\n",
    "    logger.info(f\"  Questions found: {questions}\")\n",
    "    logger.info(f\"  Answer questions: {question_with_answer}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"‚ùå Failed to load annotations: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-06 13:23:34,682 - INFO - ‚úì Loaded Name List from: ../sample/VTC Test Name List.xlsx\n",
      "2026-01-06 13:23:34,686 - INFO -   Students found: 4\n",
      "2026-01-06 13:23:34,703 - INFO - ‚úì Loaded Marking Scheme from: ../sample/VTC Test Marking Scheme.xlsx\n",
      "2026-01-06 13:23:34,704 - INFO -   Columns: ['question_number', 'question_text', 'marking_scheme', 'marks']\n",
      "2026-01-06 13:23:34,706 - INFO -   Questions in scheme: 5\n",
      "2026-01-06 13:23:34,716 - INFO - ‚úì Prepared standard answer data\n",
      "2026-01-06 13:23:34,727 - INFO - ‚úì Standard answer validation completed successfully\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>QuestionText</th>\n",
       "      <th>Answer</th>\n",
       "      <th>Mark</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q1</td>\n",
       "      <td>The Role of VTC. The VTC is the largest provid...</td>\n",
       "      <td>- **VPET Definition**: Correctly stating **Voc...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q2</td>\n",
       "      <td>Member Institutions. Compare IVE (Hong Kong In...</td>\n",
       "      <td>- **IVE Identification**: Correctly identifyin...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q3</td>\n",
       "      <td>Educational Philosophy. VTC emphasizes the \"Th...</td>\n",
       "      <td>- **\"Think\" Component**: Explaining the applic...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q4</td>\n",
       "      <td>Study Pathways. If a Secondary 6 student does ...</td>\n",
       "      <td>- **Programme Name**: Correctly naming the **D...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q5</td>\n",
       "      <td>Industry Partnership. Why does the VTC collabo...</td>\n",
       "      <td>- **General Rationale**: Ensuring curriculum i...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Question                                       QuestionText  \\\n",
       "0       Q1  The Role of VTC. The VTC is the largest provid...   \n",
       "1       Q2  Member Institutions. Compare IVE (Hong Kong In...   \n",
       "2       Q3  Educational Philosophy. VTC emphasizes the \"Th...   \n",
       "3       Q4  Study Pathways. If a Secondary 6 student does ...   \n",
       "4       Q5  Industry Partnership. Why does the VTC collabo...   \n",
       "\n",
       "                                              Answer  Mark  \n",
       "0  - **VPET Definition**: Correctly stating **Voc...    10  \n",
       "1  - **IVE Identification**: Correctly identifyin...    10  \n",
       "2  - **\"Think\" Component**: Explaining the applic...    10  \n",
       "3  - **Programme Name**: Correctly naming the **D...    10  \n",
       "4  - **General Rationale**: Ensuring curriculum i...    10  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Standard Answer Summary:\n",
      "   Questions: ['Q1', 'Q2', 'Q3', 'Q4', 'Q5']\n",
      "   Total marks: 50\n"
     ]
    }
   ],
   "source": [
    "# Robust standard answer loading with comprehensive validation\n",
    "try:\n",
    "    # Load Name List\n",
    "    name_list_df = pd.read_excel(name_list_file, sheet_name=\"Name List\")\n",
    "    logger.info(f\"‚úì Loaded Name List from: {name_list_file}\")\n",
    "    logger.info(f\"  Students found: {len(name_list_df)}\")\n",
    "    \n",
    "    # Load Marking Scheme\n",
    "    marking_scheme_df = pd.read_excel(standard_answer, sheet_name=\"Marking Scheme\")\n",
    "    logger.info(f\"‚úì Loaded Marking Scheme from: {standard_answer}\")\n",
    "    logger.info(f\"  Columns: {list(marking_scheme_df.columns)}\")\n",
    "    logger.info(f\"  Questions in scheme: {len(marking_scheme_df)}\")\n",
    "    \n",
    "    # Create Answer sheet dictionary for backward compatibility\n",
    "    standard_answer_df = marking_scheme_df[['question_number', 'question_text', 'marking_scheme', 'marks']].copy()\n",
    "    standard_answer_df.columns = ['Question', 'QuestionText', 'Answer', 'Mark']\n",
    "    standard_answer_df[\"Question\"] = standard_answer_df[\"Question\"].astype(str)\n",
    "    \n",
    "    logger.info(f\"‚úì Prepared standard answer data\")\n",
    "    \n",
    "    # Cross-validate questions\n",
    "    scheme_questions = set(standard_answer_df[\"Question\"].values)\n",
    "    annotation_questions = set(question_with_answer)\n",
    "    \n",
    "    missing_in_scheme = annotation_questions - scheme_questions\n",
    "    missing_in_annotations = scheme_questions - annotation_questions\n",
    "    \n",
    "    if missing_in_scheme:\n",
    "        logger.error(f\"Questions in annotations but not in marking scheme: {missing_in_scheme}\")\n",
    "        raise ValueError(f\"Missing questions in marking scheme: {missing_in_scheme}\")\n",
    "    \n",
    "    if missing_in_annotations:\n",
    "        logger.warning(f\"Questions in marking scheme but not in annotations: {missing_in_annotations}\")\n",
    "    \n",
    "    # Create lookup dictionaries\n",
    "    standard_question_text = standard_answer_df.set_index(\"Question\").to_dict()[\"QuestionText\"]\n",
    "    standard_answer_dict = standard_answer_df.set_index(\"Question\").to_dict()[\"Answer\"]\n",
    "    standard_mark = standard_answer_df.set_index(\"Question\").to_dict()[\"Mark\"]\n",
    "    \n",
    "    logger.info(\"‚úì Standard answer validation completed successfully\")\n",
    "    display(standard_answer_df.head())\n",
    "    \n",
    "    print(f\"\\nüìä Standard Answer Summary:\")\n",
    "    print(f\"   Questions: {list(standard_mark.keys())}\")\n",
    "    print(f\"   Total marks: {sum(standard_mark.values())}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"‚ùå Failed to load standard answers: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-06 13:23:34,815 - INFO - ‚úì JavaScript files copied to: ../marking_form/VTC Test/javascript\n",
      "2026-01-06 13:23:34,818 - INFO - ‚úì Favicon copied to: ../marking_form/VTC Test/favicon.ico\n",
      "2026-01-06 13:23:34,835 - INFO - ‚úì Generated index.html: ../marking_form/VTC Test/index.html\n",
      "2026-01-06 13:23:34,838 - INFO -   File size: 1038 bytes\n",
      "2026-01-06 13:23:34,841 - INFO -   Questions included: 8\n"
     ]
    }
   ],
   "source": [
    "# Robust template setup with comprehensive error handling\n",
    "try:\n",
    "    # Copy JavaScript files\n",
    "    from_directory = os.path.join(os.getcwd(), \"..\", \"templates\", \"javascript\")\n",
    "    if not os.path.exists(from_directory):\n",
    "        logger.warning(f\"JavaScript template directory not found: {from_directory}\")\n",
    "    else:\n",
    "        shutil.copytree(from_directory, base_path_javascript, dirs_exist_ok=True)\n",
    "        logger.info(f\"‚úì JavaScript files copied to: {base_path_javascript}\")\n",
    "    \n",
    "    # Copy favicon\n",
    "    ico_source = os.path.join(os.getcwd(), \"..\", \"templates\", \"favicon.ico\")\n",
    "    ico_dest = os.path.join(base_path, \"favicon.ico\")\n",
    "    \n",
    "    if os.path.exists(ico_source):\n",
    "        shutil.copyfile(ico_source, ico_dest)\n",
    "        logger.info(f\"‚úì Favicon copied to: {ico_dest}\")\n",
    "    else:\n",
    "        logger.warning(f\"Favicon not found: {ico_source}\")\n",
    "    \n",
    "    # Generate index.html with error handling\n",
    "    template_dir = \"../templates\"\n",
    "    if not os.path.exists(template_dir):\n",
    "        raise FileNotFoundError(f\"Template directory not found: {template_dir}\")\n",
    "    \n",
    "    file_loader = FileSystemLoader(template_dir)\n",
    "    env = Environment(loader=file_loader)\n",
    "    \n",
    "    # Add markdown filter\n",
    "    def markdown_filter(text):\n",
    "        if text is None:\n",
    "            return \"\"\n",
    "        return markdown.markdown(text)\n",
    "    \n",
    "    env.filters['markdown'] = markdown_filter\n",
    "    template = env.get_template(\"index.html\")\n",
    "    \n",
    "    output = template.render(\n",
    "        studentsScriptFileName=file_name,\n",
    "        textAnswer=questions\n",
    "    )\n",
    "    \n",
    "    output_path = Path(os.path.join(base_path, \"index.html\"))\n",
    "    with open(output_path, \"w\", encoding='utf-8') as text_file:\n",
    "        text_file.write(output)\n",
    "    \n",
    "    if not output_path.exists():\n",
    "        raise Exception(\"Failed to create index.html file\")\n",
    "    \n",
    "    file_size = output_path.stat().st_size\n",
    "    logger.info(f\"‚úì Generated index.html: {output_path}\")\n",
    "    logger.info(f\"  File size: {file_size} bytes\")\n",
    "    logger.info(f\"  Questions included: {len(questions)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"‚ùå Template setup failed: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3ce9453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Robust caching system initialized\n"
     ]
    }
   ],
   "source": [
    "# Robust Caching System with Comprehensive Error Handling\n",
    "from typing import Dict, Any, Optional, Tuple\n",
    "\n",
    "# Initialize cache directory\n",
    "cache_dir = \"../cache\"\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "# Performance tracking\n",
    "performance_stats = {\n",
    "    \"ocr_calls\": 0, \"cache_hits\": 0, \"cache_misses\": 0,\n",
    "    \"grading_calls\": 0, \"moderation_calls\": 0,\n",
    "    \"total_processing_time\": 0, \"errors\": []\n",
    "}\n",
    "\n",
    "def get_cache_key(cache_type: str, **params) -> Tuple[str, str]:\n",
    "    \"\"\"Generate cache key with parameter handling\"\"\"\n",
    "    try:\n",
    "        key_data = {\"type\": cache_type, \"version\": \"2.0\", **params}\n",
    "        key_str = json.dumps(key_data, sort_keys=True, ensure_ascii=False)\n",
    "        hash_key = hashlib.sha256(key_str.encode('utf-8')).hexdigest()\n",
    "        return (cache_type, hash_key)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error generating cache key: {e}\")\n",
    "        fallback_str = f\"{cache_type}_{str(params)}\"\n",
    "        hash_key = hashlib.sha256(fallback_str.encode()).hexdigest()\n",
    "        return (cache_type, hash_key)\n",
    "\n",
    "def get_from_cache(cache_key: Tuple[str, str]) -> Optional[Any]:\n",
    "    \"\"\"Robust cache retrieval with integrity checks\"\"\"\n",
    "    try:\n",
    "        cache_type, hash_key = cache_key\n",
    "        cache_subdir = os.path.join(cache_dir, cache_type)\n",
    "        cache_file = os.path.join(cache_subdir, f\"{hash_key}.json\")\n",
    "        \n",
    "        if not os.path.exists(cache_file):\n",
    "            performance_stats[\"cache_misses\"] += 1\n",
    "            return None\n",
    "        \n",
    "        with open(cache_file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Accept dict or list payloads; others are treated as miss\n",
    "        if not isinstance(data, (dict, list)):\n",
    "            performance_stats[\"cache_misses\"] += 1\n",
    "            return None\n",
    "        \n",
    "        performance_stats[\"cache_hits\"] += 1\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error reading cache: {e}\")\n",
    "        performance_stats[\"cache_misses\"] += 1\n",
    "        return None\n",
    "\n",
    "def save_to_cache(cache_key: Tuple[str, str], data: Any) -> bool:\n",
    "    \"\"\"Robust cache saving with validation\"\"\"\n",
    "    try:\n",
    "        cache_type, hash_key = cache_key\n",
    "        cache_subdir = os.path.join(cache_dir, cache_type)\n",
    "        os.makedirs(cache_subdir, exist_ok=True)\n",
    "        \n",
    "        cache_file = os.path.join(cache_subdir, f\"{hash_key}.json\")\n",
    "        with open(cache_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Failed to save cache: {e}\")\n",
    "        return False\n",
    "\n",
    "print(\"‚úÖ Robust caching system initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3554fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Robust OCR functions initialized (Agent-based)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Robust OCR Functions with Retry Logic (Agent-based)\n",
    "\n",
    "async def ocr_image_from_file(question, image_path, left, top, width, height):\n",
    "    \"\"\"Robust OCR processing with caching via AI Agent\"\"\"\n",
    "    if question == \"NAME\":\n",
    "        return \"\"\n",
    "    \n",
    "    try:\n",
    "        with tempfile.NamedTemporaryFile(suffix=\".png\", delete=False) as temp_file:\n",
    "            temp_path = temp_file.name\n",
    "        \n",
    "        with Image.open(image_path) as im:\n",
    "            crop_box = (left, top, left + width, top + height)\n",
    "            im_crop = im.crop(crop_box)\n",
    "            \n",
    "            enhancer = ImageEnhance.Sharpness(im_crop)\n",
    "            im_crop = enhancer.enhance(3)\n",
    "            im_crop.save(temp_path, format=\"png\")\n",
    "        \n",
    "        with open(temp_path, 'rb') as f:\n",
    "            file_hash = hashlib.sha256(f.read()).hexdigest()\n",
    "        \n",
    "        # Create prompt based on question type\n",
    "        if question == \"ID\":\n",
    "            text_message = \"\"\"Extract text in this image. It is a Student ID in 9 digit number.\n",
    "Return only the 9-digit Student ID with no other words. Strip whitespace.\n",
    "If you cannot extract Student ID, return 'No text found!!!'.\"\"\"\n",
    "        elif question == \"CLASS\":\n",
    "            text_message = \"\"\"Extract the class code from this image.\n",
    "Return only the class value with no other words. Strip whitespace.\n",
    "If you cannot extract the class value, return 'No text found!!!'.\"\"\"\n",
    "        else:\n",
    "            text_message = \"\"\"Extract only the handwritten text from this image.\n",
    "Ignore printed text. Preserve original formatting and line breaks.\n",
    "Return exactly the extracted handwritten text. Strip whitespace.\n",
    "If you cannot extract text, return 'No text found!!!'.\"\"\"\n",
    "        \n",
    "        cache_key = get_cache_key(\"ocr\", model=\"gemini-3-flash-preview\",\n",
    "                                   prompt=text_message, image_hash=file_hash,\n",
    "                                   temperature=0, top_p=0.5)\n",
    "        \n",
    "        cached_result = get_from_cache(cache_key)\n",
    "        if cached_result is not None:\n",
    "            ocr_text = cached_result.get(\"result\", \"\")\n",
    "            print(f\"[CACHE] {question} {os.path.basename(image_path)}\")\n",
    "            return \"\" if ocr_text == \"No text found!!!\" else ocr_text\n",
    "        \n",
    "        # Use Agent\n",
    "        ocr_text = await perform_ocr_with_ai(text_message, image_path=temp_path)\n",
    "        \n",
    "        save_to_cache(cache_key, {\"result\": ocr_text})\n",
    "        print(f\"[NEW] {question} {os.path.basename(image_path)}: {ocr_text[:50]}\")\n",
    "        \n",
    "        return \"\" if ocr_text == \"No text found!!!\" else ocr_text\n",
    "    except Exception as e:\n",
    "        logger.error(f\"OCR failed for {question} {image_path}: {e}\")\n",
    "        return \"\"\n",
    "    finally:\n",
    "        if 'temp_path' in locals() and os.path.exists(temp_path):\n",
    "            os.unlink(temp_path)\n",
    "\n",
    "print(\"‚úÖ Robust OCR functions initialized (Agent-based)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d544645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Robust grading system initialized\n"
     ]
    }
   ],
   "source": [
    "# Robust Grading System\n",
    "from agents.grading_agent.agent import GradingResult, grade_answer_with_ai\n",
    "\n",
    "async def grade_answer(question_text, submitted_answer, marking_scheme_text, total_marks):\n",
    "    \"\"\"Grade a student's answer using Gemini (via agent)\"\"\"\n",
    "    performance_stats[\"grading_calls\"] += 1\n",
    "    \n",
    "    cache_key = get_cache_key(\"grade_answer\", model=\"gemini-3-flash-preview\",\n",
    "                               temperature=0, top_p=0.3, max_output_tokens=8192,\n",
    "                               question=question_text, answer=submitted_answer,\n",
    "                               scheme=marking_scheme_text, marks=total_marks)\n",
    "    \n",
    "    cached_result = get_from_cache(cache_key)\n",
    "    if cached_result is not None:\n",
    "        return GradingResult(**cached_result)\n",
    "    \n",
    "    # Use Agent\n",
    "    result = await grade_answer_with_ai(question_text, submitted_answer, marking_scheme_text, total_marks)\n",
    "    \n",
    "    # Cache the result\n",
    "    save_to_cache(cache_key, result.model_dump())\n",
    "    return result\n",
    "\n",
    "async def grade_answers(answers, question):\n",
    "    \"\"\"Grade multiple answers for a question\"\"\"\n",
    "    question_text = standard_question_text.get(question, \"\")\n",
    "    marking_scheme_text = standard_answer_dict.get(question, \"\")\n",
    "    total_marks = standard_mark.get(question, 0)\n",
    "    \n",
    "    results = []\n",
    "    for submitted_answer in answers:\n",
    "        submitted_answer = str(submitted_answer)\n",
    "        if not submitted_answer.strip():\n",
    "            results.append(GradingResult(similarity_score=0, mark=0, reasoning=\"Empty answer\"))\n",
    "            continue\n",
    "        result = await grade_answer(question_text, submitted_answer, marking_scheme_text, total_marks)\n",
    "        results.append(result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"‚úÖ Robust grading system initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45740bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Robust moderation system initialized\n"
     ]
    }
   ],
   "source": [
    "# Robust Moderation System\n",
    "from typing import List\n",
    "from agents.moderation_agent.agent import ModerationItem, ModerationResponse, moderate_grades_with_ai\n",
    "\n",
    "async def grade_moderator(question, answers, grading_results, row_numbers):\n",
    "    \"\"\"Use Gemini to harmonize marks across similar answers (via agent)\"\"\"\n",
    "    performance_stats[\"moderation_calls\"] += 1\n",
    "    \n",
    "    question_text = standard_question_text.get(question, \"\")\n",
    "    marking_scheme_text = standard_answer_dict.get(question, \"\")\n",
    "    total_marks = standard_mark.get(question, 0)\n",
    "    \n",
    "    entries = []\n",
    "    for row_num, ans, res in zip(row_numbers, answers, grading_results):\n",
    "        entries.append({\n",
    "            \"row\": int(row_num),\n",
    "            \"answer\": str(ans or \"\"),\n",
    "            \"mark\": float(res.mark),\n",
    "            \"reasoning\": str(res.reasoning or \"\"),\n",
    "        })\n",
    "    \n",
    "    cache_key = get_cache_key(\"grade_moderator\", model=\"gemini-3-pro-preview\",\n",
    "                               temperature=0, top_p=0.3, question=question_text,\n",
    "                               scheme=marking_scheme_text, total_marks=total_marks,\n",
    "                               entries=entries)\n",
    "    \n",
    "    cached = get_from_cache(cache_key)\n",
    "    if cached is not None:\n",
    "        return cached\n",
    "    \n",
    "    # Use Agent\n",
    "    moderation = await moderate_grades_with_ai(question_text, marking_scheme_text, total_marks, entries)\n",
    "    \n",
    "    save_to_cache(cache_key, moderation)\n",
    "    return moderation\n",
    "\n",
    "print(\"‚úÖ Robust moderation system initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e5f7df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Image organization complete\n",
      "   Total images: 8\n",
      "   Max page: 2\n"
     ]
    }
   ],
   "source": [
    "# Image Processing and Data Organization Functions\n",
    "\n",
    "def get_the_list_of_files(path):\n",
    "    \"\"\"Get the list of files in the directory\"\"\"\n",
    "    files = []\n",
    "    for dirpath, dirnames, filenames in os.walk(path):\n",
    "        files.extend(filenames)\n",
    "        break\n",
    "    return sorted(files)\n",
    "\n",
    "def calculate_max_page(annotations_list):\n",
    "    \"\"\"Calculate maximum page number from annotations\"\"\"\n",
    "    max_page = max((ann[\"page\"] for ann in annotations_list), default=0)\n",
    "    return max_page + (1 if max_page % 2 == 1 else max_page + 2)\n",
    "\n",
    "def organize_images_by_page(images, max_page):\n",
    "    \"\"\"Organize images into page buckets\"\"\"\n",
    "    images_by_page = [[] for _ in range(max_page)]\n",
    "    for image in images:\n",
    "        page_num = int(image.split(\".\")[0])\n",
    "        page_index = page_num % max_page\n",
    "        images_by_page[page_index].append(image)\n",
    "    return images_by_page\n",
    "\n",
    "# Organize images\n",
    "images = get_the_list_of_files(base_path_images)\n",
    "max_page = calculate_max_page(annotations_list)\n",
    "images_by_page = organize_images_by_page(images, max_page)\n",
    "\n",
    "print(f\"‚úÖ Image organization complete\")\n",
    "print(f\"   Total images: {len(images)}\")\n",
    "print(f\"   Max page: {max_page}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c444dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Template rendering functions initialized\n"
     ]
    }
   ],
   "source": [
    "# Template Rendering Functions\n",
    "\n",
    "def get_template_name(question):\n",
    "    \"\"\"Determine which HTML template to use\"\"\"\n",
    "    if question in [\"ID\", \"NAME\", \"CLASS\"]:\n",
    "        return \"questions/index-answer.html\"\n",
    "    return \"questions/index.html\"\n",
    "\n",
    "def render_question_html(question, dataTable):\n",
    "    \"\"\"Render the main HTML page for a question\"\"\"\n",
    "    current_index = questions.index(question) if question in questions else -1\n",
    "    prev_question = questions[current_index - 1] if current_index > 0 else None\n",
    "    next_question = questions[current_index + 1] if current_index < len(questions) - 1 else None\n",
    "    \n",
    "    template = env.get_template(get_template_name(question))\n",
    "    return template.render(\n",
    "        studentsScriptFileName=file_name,\n",
    "        question=question,\n",
    "        standardAnswer=standard_answer_dict.get(question, \"\"),\n",
    "        standardMark=standard_mark.get(question, \"\"),\n",
    "        estimatedBoundingBox=annotations_dict[question],\n",
    "        dataTable=dataTable,\n",
    "        prev_question=prev_question,\n",
    "        next_question=next_question,\n",
    "    )\n",
    "\n",
    "def render_question_js(question, dataTable):\n",
    "    \"\"\"Render the JavaScript file for a question\"\"\"\n",
    "    template = env.get_template(\"questions/question.js\")\n",
    "    return template.render(\n",
    "        dataTable=dataTable,\n",
    "        estimatedBoundingBox=annotations_dict[question],\n",
    "    )\n",
    "\n",
    "def render_question_css(dataTable):\n",
    "    \"\"\"Render the CSS file for a question\"\"\"\n",
    "    template = env.get_template(\"questions/style.css\")\n",
    "    return template.render(dataTable=dataTable)\n",
    "\n",
    "def save_question_data(question, dataTable):\n",
    "    \"\"\"Save CSV data for a question\"\"\"\n",
    "    question_dir = Path(base_path_questions) / question\n",
    "    question_dir.mkdir(parents=True, exist_ok=True)\n",
    "    dataTable.to_csv(question_dir / \"data.csv\", index=False)\n",
    "\n",
    "def save_template_output(output, question, filename):\n",
    "    \"\"\"Save rendered template to question folder\"\"\"\n",
    "    question_dir = Path(base_path_questions, question)\n",
    "    question_dir.mkdir(parents=True, exist_ok=True)\n",
    "    output_file = question_dir / filename\n",
    "    output_file.write_text(output)\n",
    "\n",
    "print(\"‚úÖ Template rendering functions initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c679ae34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-06 13:23:36,334 - INFO - GOOGLE_API_KEY found in environment\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29c80164c5254baf9f7be88984230925",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, description='Processing:', max=8)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1/8: NAME\n",
      "Processing 2/8: ID\n",
      "[CACHE] ID 0.jpg\n",
      "[CACHE] ID 2.jpg\n",
      "[CACHE] ID 4.jpg\n",
      "[CACHE] ID 6.jpg\n",
      "Processing 3/8: CLASS\n",
      "[CACHE] CLASS 0.jpg\n",
      "[CACHE] CLASS 2.jpg\n",
      "[CACHE] CLASS 4.jpg\n",
      "[CACHE] CLASS 6.jpg\n",
      "Processing 4/8: Q1\n",
      "[CACHE] Q1 0.jpg\n",
      "[CACHE] Q1 2.jpg\n",
      "[CACHE] Q1 4.jpg\n",
      "[CACHE] Q1 6.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-06 13:23:37,136 - INFO - Sending out request, model: gemini-3-flash-preview, backend: GoogleLLMVariant.VERTEX_AI, stream: False\n",
      "2026-01-06 13:23:37,139 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2026-01-06 13:23:42,600 - INFO - HTTP Request: POST https://aiplatform.googleapis.com/v1beta1/publishers/google/models/gemini-3-flash-preview:generateContent \"HTTP/1.1 200 OK\"\n",
      "2026-01-06 13:23:42,609 - INFO - Response received from the model.\n",
      "2026-01-06 13:23:42,957 - INFO - Sending out request, model: gemini-3-pro-preview, backend: GoogleLLMVariant.VERTEX_AI, stream: False\n",
      "2026-01-06 13:23:42,961 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2026-01-06 13:24:40,306 - INFO - HTTP Request: POST https://aiplatform.googleapis.com/v1beta1/publishers/google/models/gemini-3-pro-preview:generateContent \"HTTP/1.1 200 OK\"\n",
      "2026-01-06 13:24:40,314 - INFO - Response received from the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 5/8: Q2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-06 13:24:40,618 - INFO - Sending out request, model: gemini-3-flash-preview, backend: GoogleLLMVariant.VERTEX_AI, stream: False\n",
      "2026-01-06 13:24:40,622 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2026-01-06 13:24:43,994 - INFO - HTTP Request: POST https://aiplatform.googleapis.com/v1beta1/publishers/google/models/gemini-3-flash-preview:generateContent \"HTTP/1.1 200 OK\"\n",
      "2026-01-06 13:24:44,000 - INFO - Response received from the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NEW] Q2 0.jpg: IVE is Highed Diploma\n",
      "THEi is Degree\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-06 13:24:44,250 - INFO - Sending out request, model: gemini-3-flash-preview, backend: GoogleLLMVariant.VERTEX_AI, stream: False\n",
      "2026-01-06 13:24:44,253 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2026-01-06 13:24:59,027 - INFO - HTTP Request: POST https://aiplatform.googleapis.com/v1beta1/publishers/google/models/gemini-3-flash-preview:generateContent \"HTTP/1.1 200 OK\"\n",
      "2026-01-06 13:24:59,037 - INFO - Response received from the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NEW] Q2 2.jpg: HD is IVE\n",
      "Degree is THei\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-06 13:24:59,631 - INFO - Sending out request, model: gemini-3-flash-preview, backend: GoogleLLMVariant.VERTEX_AI, stream: False\n",
      "2026-01-06 13:24:59,646 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2026-01-06 13:25:08,826 - INFO - HTTP Request: POST https://aiplatform.googleapis.com/v1beta1/publishers/google/models/gemini-3-flash-preview:generateContent \"HTTP/1.1 200 OK\"\n",
      "2026-01-06 13:25:08,831 - INFO - Response received from the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NEW] Q2 4.jpg: IVE is VTC\n",
      "thei is also VTC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-06 13:25:09,348 - INFO - Sending out request, model: gemini-3-flash-preview, backend: GoogleLLMVariant.VERTEX_AI, stream: False\n",
      "2026-01-06 13:25:09,352 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2026-01-06 13:25:16,666 - INFO - HTTP Request: POST https://aiplatform.googleapis.com/v1beta1/publishers/google/models/gemini-3-flash-preview:generateContent \"HTTP/1.1 200 OK\"\n",
      "2026-01-06 13:25:16,678 - INFO - Response received from the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NEW] Q2 6.jpg: higher Diploma for IVE\n",
      "Degree for THEi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-06 13:25:17,080 - INFO - Sending out request, model: gemini-3-flash-preview, backend: GoogleLLMVariant.VERTEX_AI, stream: False\n",
      "2026-01-06 13:25:17,083 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2026-01-06 13:25:23,799 - INFO - HTTP Request: POST https://aiplatform.googleapis.com/v1beta1/publishers/google/models/gemini-3-flash-preview:generateContent \"HTTP/1.1 200 OK\"\n",
      "2026-01-06 13:25:23,807 - INFO - Response received from the model.\n",
      "2026-01-06 13:25:24,184 - INFO - Sending out request, model: gemini-3-flash-preview, backend: GoogleLLMVariant.VERTEX_AI, stream: False\n",
      "2026-01-06 13:25:24,189 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2026-01-06 13:25:30,474 - INFO - HTTP Request: POST https://aiplatform.googleapis.com/v1beta1/publishers/google/models/gemini-3-flash-preview:generateContent \"HTTP/1.1 200 OK\"\n",
      "2026-01-06 13:25:30,481 - INFO - Response received from the model.\n",
      "2026-01-06 13:25:30,852 - INFO - Sending out request, model: gemini-3-flash-preview, backend: GoogleLLMVariant.VERTEX_AI, stream: False\n",
      "2026-01-06 13:25:30,857 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2026-01-06 13:25:35,624 - INFO - HTTP Request: POST https://aiplatform.googleapis.com/v1beta1/publishers/google/models/gemini-3-flash-preview:generateContent \"HTTP/1.1 200 OK\"\n",
      "2026-01-06 13:25:35,633 - INFO - Response received from the model.\n",
      "2026-01-06 13:25:35,985 - INFO - Sending out request, model: gemini-3-flash-preview, backend: GoogleLLMVariant.VERTEX_AI, stream: False\n",
      "2026-01-06 13:25:35,990 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2026-01-06 13:25:42,154 - INFO - HTTP Request: POST https://aiplatform.googleapis.com/v1beta1/publishers/google/models/gemini-3-flash-preview:generateContent \"HTTP/1.1 200 OK\"\n",
      "2026-01-06 13:25:42,160 - INFO - Response received from the model.\n",
      "2026-01-06 13:25:42,550 - INFO - Sending out request, model: gemini-3-pro-preview, backend: GoogleLLMVariant.VERTEX_AI, stream: False\n",
      "2026-01-06 13:25:42,556 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2026-01-06 13:26:10,950 - INFO - HTTP Request: POST https://aiplatform.googleapis.com/v1beta1/publishers/google/models/gemini-3-pro-preview:generateContent \"HTTP/1.1 200 OK\"\n",
      "2026-01-06 13:26:10,958 - INFO - Response received from the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 6/8: Q3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-06 13:26:11,235 - INFO - Sending out request, model: gemini-3-flash-preview, backend: GoogleLLMVariant.VERTEX_AI, stream: False\n",
      "2026-01-06 13:26:11,238 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2026-01-06 13:26:24,890 - INFO - HTTP Request: POST https://aiplatform.googleapis.com/v1beta1/publishers/google/models/gemini-3-flash-preview:generateContent \"HTTP/1.1 200 OK\"\n",
      "2026-01-06 13:26:24,903 - INFO - Response received from the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NEW] Q3 0.jpg: thin king and doing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-06 13:26:25,218 - INFO - Sending out request, model: gemini-3-flash-preview, backend: GoogleLLMVariant.VERTEX_AI, stream: False\n",
      "2026-01-06 13:26:25,221 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2026-01-06 13:26:29,764 - INFO - HTTP Request: POST https://aiplatform.googleapis.com/v1beta1/publishers/google/models/gemini-3-flash-preview:generateContent \"HTTP/1.1 200 OK\"\n",
      "2026-01-06 13:26:29,769 - INFO - Response received from the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NEW] Q3 2.jpg: Sorry I don't know\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-06 13:26:30,023 - INFO - Sending out request, model: gemini-3-flash-preview, backend: GoogleLLMVariant.VERTEX_AI, stream: False\n",
      "2026-01-06 13:26:30,026 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2026-01-06 13:26:56,972 - INFO - HTTP Request: POST https://aiplatform.googleapis.com/v1beta1/publishers/google/models/gemini-3-flash-preview:generateContent \"HTTP/1.1 200 OK\"\n",
      "2026-01-06 13:26:56,978 - INFO - Response received from the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NEW] Q3 4.jpg: brain power to dory\n",
      "hand-on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-06 13:26:57,219 - INFO - Sending out request, model: gemini-3-flash-preview, backend: GoogleLLMVariant.VERTEX_AI, stream: False\n",
      "2026-01-06 13:26:57,222 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2026-01-06 13:27:01,028 - INFO - HTTP Request: POST https://aiplatform.googleapis.com/v1beta1/publishers/google/models/gemini-3-flash-preview:generateContent \"HTTP/1.1 200 OK\"\n",
      "2026-01-06 13:27:01,032 - INFO - Response received from the model.\n",
      "2026-01-06 13:27:01,234 - INFO - Sending out request, model: gemini-3-flash-preview, backend: GoogleLLMVariant.VERTEX_AI, stream: False\n",
      "2026-01-06 13:27:01,238 - INFO - AFC is enabled with max remote calls: 10.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NEW] Q3 6.jpg: Yeah\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-06 13:27:05,658 - INFO - HTTP Request: POST https://aiplatform.googleapis.com/v1beta1/publishers/google/models/gemini-3-flash-preview:generateContent \"HTTP/1.1 200 OK\"\n",
      "2026-01-06 13:27:05,663 - INFO - Response received from the model.\n",
      "2026-01-06 13:27:05,892 - INFO - Sending out request, model: gemini-3-flash-preview, backend: GoogleLLMVariant.VERTEX_AI, stream: False\n",
      "2026-01-06 13:27:05,896 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2026-01-06 13:27:09,656 - INFO - HTTP Request: POST https://aiplatform.googleapis.com/v1beta1/publishers/google/models/gemini-3-flash-preview:generateContent \"HTTP/1.1 200 OK\"\n",
      "2026-01-06 13:27:09,660 - INFO - Response received from the model.\n",
      "2026-01-06 13:27:09,858 - INFO - Sending out request, model: gemini-3-flash-preview, backend: GoogleLLMVariant.VERTEX_AI, stream: False\n",
      "2026-01-06 13:27:09,860 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2026-01-06 13:27:16,118 - INFO - HTTP Request: POST https://aiplatform.googleapis.com/v1beta1/publishers/google/models/gemini-3-flash-preview:generateContent \"HTTP/1.1 200 OK\"\n",
      "2026-01-06 13:27:16,123 - INFO - Response received from the model.\n",
      "2026-01-06 13:27:16,325 - INFO - Sending out request, model: gemini-3-flash-preview, backend: GoogleLLMVariant.VERTEX_AI, stream: False\n",
      "2026-01-06 13:27:16,328 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2026-01-06 13:27:19,601 - INFO - HTTP Request: POST https://aiplatform.googleapis.com/v1beta1/publishers/google/models/gemini-3-flash-preview:generateContent \"HTTP/1.1 200 OK\"\n",
      "2026-01-06 13:27:19,605 - INFO - Response received from the model.\n",
      "2026-01-06 13:27:19,801 - INFO - Sending out request, model: gemini-3-pro-preview, backend: GoogleLLMVariant.VERTEX_AI, stream: False\n",
      "2026-01-06 13:27:19,806 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2026-01-06 13:27:43,553 - INFO - HTTP Request: POST https://aiplatform.googleapis.com/v1beta1/publishers/google/models/gemini-3-pro-preview:generateContent \"HTTP/1.1 200 OK\"\n",
      "2026-01-06 13:27:43,559 - INFO - Response received from the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 7/8: Q4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-06 13:27:43,832 - INFO - Sending out request, model: gemini-3-flash-preview, backend: GoogleLLMVariant.VERTEX_AI, stream: False\n",
      "2026-01-06 13:27:43,836 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2026-01-06 13:27:47,372 - INFO - HTTP Request: POST https://aiplatform.googleapis.com/v1beta1/publishers/google/models/gemini-3-flash-preview:generateContent \"HTTP/1.1 200 OK\"\n",
      "2026-01-06 13:27:47,377 - INFO - Response received from the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NEW] Q4 1.jpg: DFS ‚Üí Higher Diploma\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-06 13:27:47,623 - INFO - Sending out request, model: gemini-3-flash-preview, backend: GoogleLLMVariant.VERTEX_AI, stream: False\n",
      "2026-01-06 13:27:47,626 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2026-01-06 13:27:52,020 - INFO - HTTP Request: POST https://aiplatform.googleapis.com/v1beta1/publishers/google/models/gemini-3-flash-preview:generateContent \"HTTP/1.1 200 OK\"\n",
      "2026-01-06 13:27:52,027 - INFO - Response received from the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NEW] Q4 3.jpg: No text found!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-06 13:27:52,278 - INFO - Sending out request, model: gemini-3-flash-preview, backend: GoogleLLMVariant.VERTEX_AI, stream: False\n",
      "2026-01-06 13:27:52,281 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2026-01-06 13:27:58,212 - INFO - HTTP Request: POST https://aiplatform.googleapis.com/v1beta1/publishers/google/models/gemini-3-flash-preview:generateContent \"HTTP/1.1 200 OK\"\n",
      "2026-01-06 13:27:58,220 - INFO - Response received from the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NEW] Q4 5.jpg: Ha ha good\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-06 13:27:58,473 - INFO - Sending out request, model: gemini-3-flash-preview, backend: GoogleLLMVariant.VERTEX_AI, stream: False\n",
      "2026-01-06 13:27:58,476 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2026-01-06 13:28:02,125 - INFO - HTTP Request: POST https://aiplatform.googleapis.com/v1beta1/publishers/google/models/gemini-3-flash-preview:generateContent \"HTTP/1.1 200 OK\"\n",
      "2026-01-06 13:28:02,131 - INFO - Response received from the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NEW] Q4 7.jpg: No text found!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-06 13:28:02,351 - INFO - Sending out request, model: gemini-3-flash-preview, backend: GoogleLLMVariant.VERTEX_AI, stream: False\n",
      "2026-01-06 13:28:02,356 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2026-01-06 13:28:08,918 - INFO - HTTP Request: POST https://aiplatform.googleapis.com/v1beta1/publishers/google/models/gemini-3-flash-preview:generateContent \"HTTP/1.1 200 OK\"\n",
      "2026-01-06 13:28:08,924 - INFO - Response received from the model.\n",
      "2026-01-06 13:28:09,126 - INFO - Sending out request, model: gemini-3-flash-preview, backend: GoogleLLMVariant.VERTEX_AI, stream: False\n",
      "2026-01-06 13:28:09,130 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2026-01-06 13:28:12,843 - INFO - HTTP Request: POST https://aiplatform.googleapis.com/v1beta1/publishers/google/models/gemini-3-flash-preview:generateContent \"HTTP/1.1 200 OK\"\n",
      "2026-01-06 13:28:12,848 - INFO - Response received from the model.\n",
      "2026-01-06 13:28:13,038 - INFO - Sending out request, model: gemini-3-flash-preview, backend: GoogleLLMVariant.VERTEX_AI, stream: False\n",
      "2026-01-06 13:28:13,041 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2026-01-06 13:28:16,864 - INFO - HTTP Request: POST https://aiplatform.googleapis.com/v1beta1/publishers/google/models/gemini-3-flash-preview:generateContent \"HTTP/1.1 200 OK\"\n",
      "2026-01-06 13:28:16,868 - INFO - Response received from the model.\n",
      "2026-01-06 13:28:17,058 - INFO - Sending out request, model: gemini-3-pro-preview, backend: GoogleLLMVariant.VERTEX_AI, stream: False\n",
      "2026-01-06 13:28:17,061 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2026-01-06 13:28:30,521 - INFO - HTTP Request: POST https://aiplatform.googleapis.com/v1beta1/publishers/google/models/gemini-3-pro-preview:generateContent \"HTTP/1.1 200 OK\"\n",
      "2026-01-06 13:28:30,526 - INFO - Response received from the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 8/8: Q5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-06 13:28:30,802 - INFO - Sending out request, model: gemini-3-flash-preview, backend: GoogleLLMVariant.VERTEX_AI, stream: False\n",
      "2026-01-06 13:28:30,804 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2026-01-06 13:28:34,231 - INFO - HTTP Request: POST https://aiplatform.googleapis.com/v1beta1/publishers/google/models/gemini-3-flash-preview:generateContent \"HTTP/1.1 200 OK\"\n",
      "2026-01-06 13:28:34,236 - INFO - Response received from the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NEW] Q5 1.jpg: Intenship\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-06 13:28:34,484 - INFO - Sending out request, model: gemini-3-flash-preview, backend: GoogleLLMVariant.VERTEX_AI, stream: False\n",
      "2026-01-06 13:28:34,487 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2026-01-06 13:28:38,536 - INFO - HTTP Request: POST https://aiplatform.googleapis.com/v1beta1/publishers/google/models/gemini-3-flash-preview:generateContent \"HTTP/1.1 200 OK\"\n",
      "2026-01-06 13:28:38,541 - INFO - Response received from the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NEW] Q5 3.jpg: No text found!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-06 13:28:38,818 - INFO - Sending out request, model: gemini-3-flash-preview, backend: GoogleLLMVariant.VERTEX_AI, stream: False\n",
      "2026-01-06 13:28:38,822 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2026-01-06 13:28:42,806 - INFO - HTTP Request: POST https://aiplatform.googleapis.com/v1beta1/publishers/google/models/gemini-3-flash-preview:generateContent \"HTTP/1.1 200 OK\"\n",
      "2026-01-06 13:28:42,810 - INFO - Response received from the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NEW] Q5 5.jpg: Intern, placement, industry\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-06 13:28:43,067 - INFO - Sending out request, model: gemini-3-flash-preview, backend: GoogleLLMVariant.VERTEX_AI, stream: False\n",
      "2026-01-06 13:28:43,070 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2026-01-06 13:28:47,302 - INFO - HTTP Request: POST https://aiplatform.googleapis.com/v1beta1/publishers/google/models/gemini-3-flash-preview:generateContent \"HTTP/1.1 200 OK\"\n",
      "2026-01-06 13:28:47,308 - INFO - Response received from the model.\n",
      "2026-01-06 13:28:47,493 - INFO - Sending out request, model: gemini-3-flash-preview, backend: GoogleLLMVariant.VERTEX_AI, stream: False\n",
      "2026-01-06 13:28:47,496 - INFO - AFC is enabled with max remote calls: 10.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NEW] Q5 7.jpg: No text found!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-06 13:28:56,529 - INFO - HTTP Request: POST https://aiplatform.googleapis.com/v1beta1/publishers/google/models/gemini-3-flash-preview:generateContent \"HTTP/1.1 200 OK\"\n",
      "2026-01-06 13:28:56,534 - INFO - Response received from the model.\n",
      "2026-01-06 13:28:56,748 - INFO - Sending out request, model: gemini-3-flash-preview, backend: GoogleLLMVariant.VERTEX_AI, stream: False\n",
      "2026-01-06 13:28:56,752 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2026-01-06 13:29:00,597 - INFO - HTTP Request: POST https://aiplatform.googleapis.com/v1beta1/publishers/google/models/gemini-3-flash-preview:generateContent \"HTTP/1.1 200 OK\"\n",
      "2026-01-06 13:29:00,602 - INFO - Response received from the model.\n",
      "2026-01-06 13:29:00,799 - INFO - Sending out request, model: gemini-3-flash-preview, backend: GoogleLLMVariant.VERTEX_AI, stream: False\n",
      "2026-01-06 13:29:00,802 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2026-01-06 13:29:08,581 - INFO - HTTP Request: POST https://aiplatform.googleapis.com/v1beta1/publishers/google/models/gemini-3-flash-preview:generateContent \"HTTP/1.1 200 OK\"\n",
      "2026-01-06 13:29:08,587 - INFO - Response received from the model.\n",
      "2026-01-06 13:29:08,820 - INFO - Sending out request, model: gemini-3-pro-preview, backend: GoogleLLMVariant.VERTEX_AI, stream: False\n",
      "2026-01-06 13:29:08,824 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2026-01-06 13:29:45,474 - INFO - HTTP Request: POST https://aiplatform.googleapis.com/v1beta1/publishers/google/models/gemini-3-pro-preview:generateContent \"HTTP/1.1 200 OK\"\n",
      "2026-01-06 13:29:45,481 - INFO - Response received from the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Completed processing 8 questions\n",
      "\n",
      "üìä Performance Stats:\n",
      "   OCR calls: 0\n",
      "   Cache hits: 17\n",
      "   Cache misses: 36\n",
      "   Grading calls: 20\n",
      "   Moderation calls: 5\n"
     ]
    }
   ],
   "source": [
    "# Main Processing Functions\n",
    "from agents.ocr_agent.agent import perform_ocr_with_ai\n",
    "\n",
    "\n",
    "def process_metadata_question(num_rows):\n",
    "    \"\"\"Create default data for metadata questions\"\"\"\n",
    "    return {\n",
    "        \"Similarity\": [0.0] * num_rows,\n",
    "        \"Reasoning\": [\"\"] * num_rows,\n",
    "        \"MarkRaw\": [0.0] * num_rows,\n",
    "        \"Mark\": [0.0] * num_rows,\n",
    "        \"ModeratorFlag\": [False] * num_rows,\n",
    "        \"ModeratorNote\": [\"\"] * num_rows,\n",
    "    }\n",
    "\n",
    "async def process_graded_question(question, answers, row_numbers):\n",
    "    \"\"\"Grade and moderate answers for a regular question\"\"\"\n",
    "    scoring_results = await grade_answers(answers, question)\n",
    "    moderation = await grade_moderator(question, answers, scoring_results, row_numbers)\n",
    "    \n",
    "    return {\n",
    "        \"Similarity\": [result.similarity_score for result in scoring_results],\n",
    "        \"Reasoning\": [result.reasoning for result in scoring_results],\n",
    "        \"MarkRaw\": [result.mark for result in scoring_results],\n",
    "        \"Mark\": [m[\"moderated_mark\"] for m in moderation],\n",
    "        \"ModeratorFlag\": [m[\"flag\"] for m in moderation],\n",
    "        \"ModeratorNote\": [m[\"note\"] for m in moderation],\n",
    "    }\n",
    "\n",
    "async def get_df(question):\n",
    "    \"\"\"Build dataframe with OCR results and grading for a question\"\"\"\n",
    "    annotation = annotations_dict[question].copy()\n",
    "    page_num = annotation[\"page\"]\n",
    "    images_for_page = images_by_page[page_num]\n",
    "    \n",
    "    image_paths = [\"images/\" + img for img in images_for_page]\n",
    "    num_images = len(images_for_page)\n",
    "    \n",
    "    data = pd.DataFrame({key: [annotation[key]] * num_images for key in annotation.keys()})\n",
    "    data[\"Image\"] = image_paths\n",
    "    \n",
    "    # Extract answers via OCR\n",
    "    answers = []\n",
    "    for image in images_for_page:\n",
    "        image_path = os.path.join(base_path, \"images\", image)\n",
    "        answer = await ocr_image_from_file(question, image_path, annotation[\"left\"],\n",
    "                                       annotation[\"top\"], annotation[\"width\"], annotation[\"height\"])\n",
    "        answers.append(answer)\n",
    "    data[\"Answer\"] = answers\n",
    "    \n",
    "    data[\"RowNumber\"] = range(1, num_images + 1)\n",
    "    data[\"maskPage\"] = page_num\n",
    "    \n",
    "    # Process based on question type\n",
    "    if question in [\"ID\", \"NAME\", \"CLASS\"]:\n",
    "        grading_data = process_metadata_question(num_images)\n",
    "    else:\n",
    "        grading_data = await process_graded_question(question, answers, data[\"RowNumber\"].tolist())\n",
    "    \n",
    "    for col, values in grading_data.items():\n",
    "        data[col] = values\n",
    "    \n",
    "    data[\"page\"] = data[\"Image\"].str.replace(\"images/\", \"\").str.replace(\".jpg\", \"\")\n",
    "    return data\n",
    "\n",
    "async def process_single_question(question):\n",
    "    \"\"\"Process one question: OCR, grade, and generate all output files\"\"\"\n",
    "    dataTable = await get_df(question)\n",
    "    save_question_data(question, dataTable)\n",
    "    save_template_output(render_question_html(question, dataTable), question, \"index.html\")\n",
    "    save_template_output(render_question_js(question, dataTable), question, \"question.js\")\n",
    "    save_template_output(render_question_css(dataTable), question, \"style.css\")\n",
    "\n",
    "# Process all questions with progress bar\n",
    "max_count = len(questions)\n",
    "progress_bar = IntProgress(min=0, max=max_count, description='Processing:')\n",
    "display(progress_bar)\n",
    "\n",
    "for idx, question in enumerate(questions, 1):\n",
    "    print(f\"Processing {idx}/{max_count}: {question}\")\n",
    "    await process_single_question(question)\n",
    "    progress_bar.value = idx\n",
    "\n",
    "print(f\"‚úì Completed processing {max_count} questions\")\n",
    "print(f\"\\nüìä Performance Stats:\")\n",
    "print(f\"   OCR calls: {performance_stats['ocr_calls']}\")\n",
    "print(f\"   Cache hits: {performance_stats['cache_hits']}\")\n",
    "print(f\"   Cache misses: {performance_stats['cache_misses']}\")\n",
    "print(f\"   Grading calls: {performance_stats['grading_calls']}\")\n",
    "print(f\"   Moderation calls: {performance_stats['moderation_calls']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f7251f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Re-grading functions available (commented out)\n"
     ]
    }
   ],
   "source": [
    "# Re-grading Functions (Optional)\n",
    "\n",
    "def load_question_data(question):\n",
    "    \"\"\"Load existing question data from CSV\"\"\"\n",
    "    data_path = Path(base_path_questions) / question / \"data.csv\"\n",
    "    return pd.read_csv(data_path)\n",
    "\n",
    "def clean_ocr_errors(dataTable):\n",
    "    \"\"\"Remove OCR error markers from answers\"\"\"\n",
    "    return dataTable.replace(\".*No text found!!!.*\", \"\", regex=True)\n",
    "\n",
    "def regrade_question_data(question, dataTable):\n",
    "    \"\"\"Re-grade answers and update similarity/reasoning\"\"\"\n",
    "    answers = dataTable[\"Answer\"].tolist()\n",
    "    scoring_results = grade_answers(answers, question)\n",
    "    dataTable[\"Similarity\"] = [result.similarity_score for result in scoring_results]\n",
    "    dataTable[\"Reasoning\"] = [result.reasoning for result in scoring_results]\n",
    "    return dataTable\n",
    "\n",
    "def regrade_and_regenerate_question(question):\n",
    "    \"\"\"Re-grade a question and regenerate all output files\"\"\"\n",
    "    dataTable = load_question_data(question)\n",
    "    dataTable = clean_ocr_errors(dataTable)\n",
    "    dataTable = regrade_question_data(question, dataTable)\n",
    "    save_question_data(question, dataTable)\n",
    "    save_template_output(render_question_html(question, dataTable), question, \"index.html\")\n",
    "    save_template_output(render_question_js(question, dataTable), question, \"question.js\")\n",
    "    save_template_output(render_question_css(dataTable), question, \"style.css\")\n",
    "\n",
    "# Uncomment to re-grade questions\n",
    "# questions_to_regrade = [q for q in questions if q not in [\"ID\", \"NAME\", \"CLASS\"]]\n",
    "# for idx, question in enumerate(questions_to_regrade, 1):\n",
    "#     print(f\"Re-grading {idx}/{len(questions_to_regrade)}: {question}\")\n",
    "#     regrade_and_regenerate_question(question)\n",
    "\n",
    "print(\"‚úÖ Re-grading functions available (commented out)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d469d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All student IDs validated successfully!\n"
     ]
    }
   ],
   "source": [
    "# Student ID Validation\n",
    "\n",
    "id_from_oscr = pd.read_csv(base_path_questions + \"/\" + \"ID\" + \"/data.csv\")[\"Answer\"].tolist()\n",
    "id_from_oscr = [str(int(float(x))) if pd.notna(x) else x for x in id_from_oscr]\n",
    "\n",
    "id_from_namelist = name_list_df[\"ID\"].to_list()\n",
    "\n",
    "# Check duplicate IDs\n",
    "duplicate_id = []\n",
    "for id in id_from_oscr:\n",
    "    if id_from_oscr.count(id) > 1:\n",
    "        duplicate_id.append(id)\n",
    "duplicate_id = list(set(duplicate_id))\n",
    "if len(duplicate_id) > 0:\n",
    "    print(colored(\"Duplicate ID: {}\".format(duplicate_id), \"red\"))\n",
    "\n",
    "id_from_oscr = [str(id) for id in id_from_oscr]\n",
    "id_from_namelist = [str(id) for id in id_from_namelist]\n",
    "\n",
    "# Compare OCR ID and name list\n",
    "ocr_missing_id = []\n",
    "name_list_missing_id = []\n",
    "for id in id_from_oscr:\n",
    "    if id not in id_from_namelist:\n",
    "        name_list_missing_id.append(id)\n",
    "\n",
    "for id in id_from_namelist:\n",
    "    if id not in id_from_oscr:\n",
    "        ocr_missing_id.append(id)\n",
    "\n",
    "# Report OCR scan errors\n",
    "if len(name_list_missing_id) > 0:\n",
    "    print(colored(\"Some IDs from OCR are not in NameList - fix manually!\", \"red\"))\n",
    "    for id in name_list_missing_id:\n",
    "        print(colored(id, \"red\"))\n",
    "\n",
    "# Report potential absences\n",
    "if len(ocr_missing_id) > 0:\n",
    "    print(colored(f\"Number of absentees: {len(ocr_missing_id)}\", \"red\"))\n",
    "    print(colored(\"IDs in Name List not found in OCR:\", \"red\"))\n",
    "    for id in ocr_missing_id:\n",
    "        print(colored(id, \"red\"))\n",
    "\n",
    "if not duplicate_id and not name_list_missing_id and not ocr_missing_id:\n",
    "    print(\"‚úÖ All student IDs validated successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c7ee4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üéâ PROCESSING COMPLETE!\n",
      "============================================================\n",
      "\n",
      "To view results, start the web server at root level:\n",
      "  file_name=\"VTC Test\" python server.py 8000\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Start Python HTTP Server\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ PROCESSING COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTo view results, start the web server at root level:\")\n",
    "print(f'  file_name=\"{file_name}\" python server.py 8000')\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üöÄ STEP 4: SCORING PREPROCESSING READY\n",
      "============================================================\n",
      "\n",
      "üìä Configuration Summary:\n",
      "   Dataset: sample\n",
      "   Prefix: VTC Test\n",
      "   Questions: 8 total, 5 for answers\n",
      "   Total marks: 50\n",
      "\n",
      "üîß System Status:\n",
      "   ‚úÖ Gemini client: Initialized\n",
      "   ‚úÖ OCR function: Robust with retry logic\n",
      "   ‚úÖ Grading system: Robust with validation\n",
      "   ‚úÖ Caching: Robust with integrity checks\n",
      "   ‚úÖ Error handling: Comprehensive\n",
      "\n",
      "üìÅ File Status:\n",
      "   ‚úÖ PDF file: VTC Test.pdf\n",
      "   ‚úÖ Name list: VTC Test Name List.xlsx\n",
      "   ‚úÖ Marking scheme: VTC Test Marking Scheme.xlsx\n",
      "   ‚úÖ Annotations: annotations.json\n",
      "   ‚úÖ Index.html: Generated\n",
      "\n",
      "üéØ Next Steps:\n",
      "   1. Run OCR processing on scanned images\n",
      "   2. Execute auto-grading with Gemini\n",
      "   3. Generate review pages for manual verification\n",
      "   4. Proceed to Step 5: Post-Scoring Checks\n",
      "\n",
      "üí° Robust Features Active:\n",
      "   ‚Ä¢ Comprehensive error handling and recovery\n",
      "   ‚Ä¢ Progress tracking with detailed status updates\n",
      "   ‚Ä¢ Robust caching with integrity validation\n",
      "   ‚Ä¢ Detailed logging and performance monitoring\n",
      "   ‚Ä¢ Automatic retry logic for failed operations\n",
      "   ‚Ä¢ Input validation and sanitization\n",
      "\n",
      "============================================================\n",
      "‚úÖ Robust Step 4 initialization completed at 13:33:16\n",
      "Ready for OCR and grading operations!\n",
      "============================================================\n",
      "\n",
      "üí° Robust version includes complete OCR and grading implementation.\n",
      "   Ready to process images and generate review pages!\n"
     ]
    }
   ],
   "source": [
    "# Robust processing summary and next steps\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üöÄ STEP 4: SCORING PREPROCESSING READY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nüìä Configuration Summary:\")\n",
    "print(f\"   Dataset: sample\")\n",
    "print(f\"   Prefix: {prefix}\")\n",
    "print(f\"   Questions: {len(questions)} total, {len(question_with_answer)} for answers\")\n",
    "print(f\"   Total marks: {sum(standard_mark.values()) if 'standard_mark' in locals() else 'N/A'}\")\n",
    "\n",
    "print(f\"\\nüîß System Status:\")\n",
    "print(f\"   ‚úÖ OCR function: Robust with retry logic\")\n",
    "print(f\"   ‚úÖ Grading system: Robust with validation\")\n",
    "print(f\"   ‚úÖ Caching: Robust with integrity checks\")\n",
    "print(f\"   ‚úÖ Error handling: Comprehensive\")\n",
    "\n",
    "print(f\"\\nüìÅ File Status:\")\n",
    "print(f\"   ‚úÖ PDF file: {os.path.basename(pdf_file)}\")\n",
    "print(f\"   ‚úÖ Name list: {os.path.basename(name_list_file)}\")\n",
    "print(f\"   ‚úÖ Marking scheme: {os.path.basename(marking_scheme_file)}\")\n",
    "print(f\"   ‚úÖ Annotations: {os.path.basename(annotations_path)}\")\n",
    "print(f\"   ‚úÖ Index.html: Generated\")\n",
    "\n",
    "print(f\"\\nüéØ Next Steps:\")\n",
    "print(f\"   1. Run OCR processing on scanned images\")\n",
    "print(f\"   2. Execute auto-grading with Gemini\")\n",
    "print(f\"   3. Generate review pages for manual verification\")\n",
    "print(f\"   4. Proceed to Step 5: Post-Scoring Checks\")\n",
    "\n",
    "print(f\"\\nüí° Robust Features Active:\")\n",
    "print(f\"   ‚Ä¢ Comprehensive error handling and recovery\")\n",
    "print(f\"   ‚Ä¢ Progress tracking with detailed status updates\")\n",
    "print(f\"   ‚Ä¢ Robust caching with integrity validation\")\n",
    "print(f\"   ‚Ä¢ Detailed logging and performance monitoring\")\n",
    "print(f\"   ‚Ä¢ Automatic retry logic for failed operations\")\n",
    "print(f\"   ‚Ä¢ Input validation and sanitization\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"‚úÖ Robust Step 4 initialization completed at {datetime.now().strftime('%H:%M:%S')}\")\n",
    "print(\"Ready for OCR and grading operations!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nüí° Robust version includes complete OCR and grading implementation.\")\n",
    "print(\"   Ready to process images and generate review pages!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
