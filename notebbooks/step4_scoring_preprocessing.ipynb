{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Scoring Preprocessing\n",
    "Extract handwritten responses from scanned sheets, run OCR, auto-grade with Gemini, and generate per-question review pages for manual checks.\n",
    "\n",
    "**Features:**\n",
    "- ‚úÖ Comprehensive error handling and validation\n",
    "- ‚úÖ Progress tracking with detailed status updates\n",
    "- ‚úÖ Robust caching system with integrity checks\n",
    "- ‚úÖ Detailed logging and reporting\n",
    "- ‚úÖ Automatic recovery from partial failures\n",
    "- ‚úÖ Performance monitoring and optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Robust Step 4: Scoring Preprocessing initialized\n",
      "‚úì Session started at: 2026-01-09 07:52:39\n",
      "‚úì Paths configured successfully\n"
     ]
    }
   ],
   "source": [
    "from grading_utils import setup_paths, create_directories\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import tempfile\n",
    "import hashlib\n",
    "import shutil\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from PIL import Image, ImageEnhance\n",
    "from jinja2 import Environment, FileSystemLoader\n",
    "import markdown\n",
    "from termcolor import colored\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "from ipywidgets import IntProgress, HTML\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Robust logging setup\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"‚úÖ Robust Step 4: Scoring Preprocessing initialized\")\n",
    "print(f\"‚úì Session started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Configuration\n",
    "prefix = \"VTC Test\"\n",
    "paths = setup_paths(prefix, \"sample\")\n",
    "\n",
    "# Extract commonly used paths\n",
    "pdf_file = paths[\"pdf_file\"]\n",
    "name_list_file = paths[\"name_list_file\"]\n",
    "marking_scheme_file = paths[\"marking_scheme_file\"]\n",
    "standard_answer = marking_scheme_file\n",
    "\n",
    "print(\"‚úì Paths configured successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncomment to reload Cache for Sample for speeding up the demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! cd .. && tar -xzf cache.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-09 07:52:39,672 - INFO - ‚úì All directories created successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Validated 5 required directories\n"
     ]
    }
   ],
   "source": [
    "# Robust directory setup and validation\n",
    "file_name = paths[\"file_name\"]\n",
    "base_path = paths[\"base_path\"]\n",
    "base_path_images = paths[\"base_path_images\"]\n",
    "base_path_annotations = paths[\"base_path_annotations\"]\n",
    "base_path_questions = paths[\"base_path_questions\"]\n",
    "base_path_javascript = paths[\"base_path_javascript\"]\n",
    "\n",
    "# Create all necessary directories with validation\n",
    "try:\n",
    "    create_directories(paths)\n",
    "    logger.info(\"‚úì All directories created successfully\")\n",
    "\n",
    "    # Validate directory creation\n",
    "    required_dirs = [\n",
    "        base_path,\n",
    "        base_path_images,\n",
    "        base_path_annotations,\n",
    "        base_path_questions,\n",
    "        base_path_javascript,\n",
    "    ]\n",
    "    for dir_path in required_dirs:\n",
    "        if not os.path.exists(dir_path):\n",
    "            raise Exception(f\"Failed to create directory: {dir_path}\")\n",
    "\n",
    "    print(f\"‚úì Validated {len(required_dirs)} required directories\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"‚ùå Directory creation failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-09 07:52:39,713 - INFO - ‚úì Annotations loaded successfully from: ../marking_form/VTC Test/annotations/annotations.json\n",
      "2026-01-09 07:52:39,714 - INFO -   Total annotations: 8\n",
      "2026-01-09 07:52:39,716 - INFO -   Questions found: ['NAME', 'ID', 'CLASS', 'Q1', 'Q2', 'Q3', 'Q4', 'Q5']\n",
      "2026-01-09 07:52:39,717 - INFO -   Answer questions: ['Q1', 'Q2', 'Q3', 'Q4', 'Q5']\n",
      "2026-01-09 07:52:39,714 - INFO -   Total annotations: 8\n",
      "2026-01-09 07:52:39,716 - INFO -   Questions found: ['NAME', 'ID', 'CLASS', 'Q1', 'Q2', 'Q3', 'Q4', 'Q5']\n",
      "2026-01-09 07:52:39,717 - INFO -   Answer questions: ['Q1', 'Q2', 'Q3', 'Q4', 'Q5']\n"
     ]
    }
   ],
   "source": [
    "# Robust annotations loading with comprehensive validation\n",
    "from grading_utils import load_annotations\n",
    "\n",
    "annotations_path = base_path_annotations + \"annotations.json\"\n",
    "\n",
    "try:\n",
    "    if not os.path.exists(annotations_path):\n",
    "        raise FileNotFoundError(f\"Annotations file not found: {annotations_path}\")\n",
    "\n",
    "    annotations_list, annotations_dict, questions_from_annotations = load_annotations(\n",
    "        annotations_path\n",
    "    )\n",
    "\n",
    "    # Validate annotations structure\n",
    "    if not annotations_list:\n",
    "        raise ValueError(\"Annotations list is empty\")\n",
    "\n",
    "    # Use questions from loaded annotations\n",
    "    questions = questions_from_annotations\n",
    "\n",
    "    # Extract question_with_answer (excludes NAME, ID, CLASS)\n",
    "    question_with_answer = [q for q in questions if q not in [\"NAME\", \"ID\", \"CLASS\"]]\n",
    "\n",
    "    logger.info(f\"‚úì Annotations loaded successfully from: {annotations_path}\")\n",
    "    logger.info(f\"  Total annotations: {len(annotations_list)}\")\n",
    "    logger.info(f\"  Questions found: {questions}\")\n",
    "    logger.info(f\"  Answer questions: {question_with_answer}\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"‚ùå Failed to load annotations: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-09 07:52:39,770 - INFO - ‚úì Loaded Name List from: ../sample/VTC Test Name List.xlsx\n",
      "2026-01-09 07:52:39,773 - INFO -   Students found: 4\n",
      "2026-01-09 07:52:39,793 - INFO - ‚úì Loaded Marking Scheme from: ../sample/VTC Test Marking Scheme.xlsx\n",
      "2026-01-09 07:52:39,794 - INFO -   Columns: ['question_number', 'question_text', 'marking_scheme', 'marks']\n",
      "2026-01-09 07:52:39,796 - INFO -   Questions in scheme: 5\n",
      "2026-01-09 07:52:39,801 - INFO - ‚úì Prepared standard answer data\n",
      "2026-01-09 07:52:39,808 - INFO - ‚úì Standard answer validation completed successfully\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>QuestionText</th>\n",
       "      <th>Answer</th>\n",
       "      <th>Mark</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q1</td>\n",
       "      <td>The Role of VTC. The VTC is the largest provid...</td>\n",
       "      <td>- Correctly stating **Vocational and Professio...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q2</td>\n",
       "      <td>Member Institutions. Compare IVE (Hong Kong In...</td>\n",
       "      <td>- Correctly identifying that **IVE** primarily...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q3</td>\n",
       "      <td>Educational Philosophy. VTC emphasizes the \"Th...</td>\n",
       "      <td>- Explaining **\"Think\"** as theory or academic...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q4</td>\n",
       "      <td>Study Pathways. If a Secondary 6 student does ...</td>\n",
       "      <td>- Correctly naming the **Diploma of Foundation...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q5</td>\n",
       "      <td>Industry Partnership. Why does the VTC collabo...</td>\n",
       "      <td>- General explanation regarding **curriculum r...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Question                                       QuestionText  \\\n",
       "0       Q1  The Role of VTC. The VTC is the largest provid...   \n",
       "1       Q2  Member Institutions. Compare IVE (Hong Kong In...   \n",
       "2       Q3  Educational Philosophy. VTC emphasizes the \"Th...   \n",
       "3       Q4  Study Pathways. If a Secondary 6 student does ...   \n",
       "4       Q5  Industry Partnership. Why does the VTC collabo...   \n",
       "\n",
       "                                              Answer  Mark  \n",
       "0  - Correctly stating **Vocational and Professio...    10  \n",
       "1  - Correctly identifying that **IVE** primarily...    10  \n",
       "2  - Explaining **\"Think\"** as theory or academic...    10  \n",
       "3  - Correctly naming the **Diploma of Foundation...    10  \n",
       "4  - General explanation regarding **curriculum r...    10  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Standard Answer Summary:\n",
      "   Questions: ['Q1', 'Q2', 'Q3', 'Q4', 'Q5']\n",
      "   Total marks: 50\n"
     ]
    }
   ],
   "source": [
    "# Robust standard answer loading with comprehensive validation\n",
    "try:\n",
    "    # Load Name List\n",
    "    name_list_df = pd.read_excel(name_list_file, sheet_name=\"Name List\")\n",
    "    logger.info(f\"‚úì Loaded Name List from: {name_list_file}\")\n",
    "    logger.info(f\"  Students found: {len(name_list_df)}\")\n",
    "\n",
    "    # Load Marking Scheme\n",
    "    marking_scheme_df = pd.read_excel(standard_answer, sheet_name=\"Marking Scheme\")\n",
    "    logger.info(f\"‚úì Loaded Marking Scheme from: {standard_answer}\")\n",
    "    logger.info(f\"  Columns: {list(marking_scheme_df.columns)}\")\n",
    "    logger.info(f\"  Questions in scheme: {len(marking_scheme_df)}\")\n",
    "\n",
    "    # Create Answer sheet dictionary for backward compatibility\n",
    "    standard_answer_df = marking_scheme_df[\n",
    "        [\"question_number\", \"question_text\", \"marking_scheme\", \"marks\"]\n",
    "    ].copy()\n",
    "    standard_answer_df.columns = [\"Question\", \"QuestionText\", \"Answer\", \"Mark\"]\n",
    "    standard_answer_df[\"Question\"] = standard_answer_df[\"Question\"].astype(str)\n",
    "\n",
    "    logger.info(f\"‚úì Prepared standard answer data\")\n",
    "\n",
    "    # Cross-validate questions\n",
    "    scheme_questions = set(standard_answer_df[\"Question\"].values)\n",
    "    annotation_questions = set(question_with_answer)\n",
    "\n",
    "    missing_in_scheme = annotation_questions - scheme_questions\n",
    "    missing_in_annotations = scheme_questions - annotation_questions\n",
    "\n",
    "    if missing_in_scheme:\n",
    "        logger.error(\n",
    "            f\"Questions in annotations but not in marking scheme: {missing_in_scheme}\"\n",
    "        )\n",
    "        raise ValueError(f\"Missing questions in marking scheme: {missing_in_scheme}\")\n",
    "\n",
    "    if missing_in_annotations:\n",
    "        logger.warning(\n",
    "            f\"Questions in marking scheme but not in annotations: {missing_in_annotations}\"\n",
    "        )\n",
    "\n",
    "    # Create lookup dictionaries\n",
    "    standard_question_text = standard_answer_df.set_index(\"Question\").to_dict()[\n",
    "        \"QuestionText\"\n",
    "    ]\n",
    "    standard_answer_dict = standard_answer_df.set_index(\"Question\").to_dict()[\"Answer\"]\n",
    "    standard_mark = standard_answer_df.set_index(\"Question\").to_dict()[\"Mark\"]\n",
    "\n",
    "    logger.info(\"‚úì Standard answer validation completed successfully\")\n",
    "    display(standard_answer_df.head())\n",
    "\n",
    "    print(f\"\\nüìä Standard Answer Summary:\")\n",
    "    print(f\"   Questions: {list(standard_mark.keys())}\")\n",
    "    print(f\"   Total marks: {sum(standard_mark.values())}\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"‚ùå Failed to load standard answers: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-09 07:52:39,866 - INFO - ‚úì JavaScript files copied to: ../marking_form/VTC Test/javascript\n",
      "2026-01-09 07:52:39,869 - INFO - ‚úì Favicon copied to: ../marking_form/VTC Test/favicon.ico\n",
      "2026-01-09 07:52:39,869 - INFO - ‚úì Favicon copied to: ../marking_form/VTC Test/favicon.ico\n",
      "2026-01-09 07:52:39,875 - INFO - ‚úì Generated index.html: ../marking_form/VTC Test/index.html\n",
      "2026-01-09 07:52:39,877 - INFO -   File size: 1038 bytes\n",
      "2026-01-09 07:52:39,878 - INFO -   Questions included: 8\n"
     ]
    }
   ],
   "source": [
    "# Robust template setup with comprehensive error handling\n",
    "try:\n",
    "    # Copy JavaScript files\n",
    "    from_directory = os.path.join(os.getcwd(), \"..\", \"templates\", \"javascript\")\n",
    "    if not os.path.exists(from_directory):\n",
    "        logger.warning(f\"JavaScript template directory not found: {from_directory}\")\n",
    "    else:\n",
    "        shutil.copytree(from_directory, base_path_javascript, dirs_exist_ok=True)\n",
    "        logger.info(f\"‚úì JavaScript files copied to: {base_path_javascript}\")\n",
    "\n",
    "    # Copy favicon\n",
    "    ico_source = os.path.join(os.getcwd(), \"..\", \"templates\", \"favicon.ico\")\n",
    "    ico_dest = os.path.join(base_path, \"favicon.ico\")\n",
    "\n",
    "    if os.path.exists(ico_source):\n",
    "        shutil.copyfile(ico_source, ico_dest)\n",
    "        logger.info(f\"‚úì Favicon copied to: {ico_dest}\")\n",
    "    else:\n",
    "        logger.warning(f\"Favicon not found: {ico_source}\")\n",
    "\n",
    "    # Generate index.html with error handling\n",
    "    template_dir = \"../templates\"\n",
    "    if not os.path.exists(template_dir):\n",
    "        raise FileNotFoundError(f\"Template directory not found: {template_dir}\")\n",
    "\n",
    "    file_loader = FileSystemLoader(template_dir)\n",
    "    env = Environment(loader=file_loader)\n",
    "\n",
    "    # Add markdown filter\n",
    "    def markdown_filter(text):\n",
    "        if text is None:\n",
    "            return \"\"\n",
    "        return markdown.markdown(text)\n",
    "\n",
    "    env.filters[\"markdown\"] = markdown_filter\n",
    "    template = env.get_template(\"index.html\")\n",
    "\n",
    "    output = template.render(studentsScriptFileName=file_name, textAnswer=questions)\n",
    "\n",
    "    output_path = Path(os.path.join(base_path, \"index.html\"))\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as text_file:\n",
    "        text_file.write(output)\n",
    "\n",
    "    if not output_path.exists():\n",
    "        raise Exception(\"Failed to create index.html file\")\n",
    "\n",
    "    file_size = output_path.stat().st_size\n",
    "    logger.info(f\"‚úì Generated index.html: {output_path}\")\n",
    "    logger.info(f\"  File size: {file_size} bytes\")\n",
    "    logger.info(f\"  Questions included: {len(questions)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"‚ùå Template setup failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c3ce9453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Robust caching system initialized (Logic moved to agents)\n"
     ]
    }
   ],
   "source": [
    "# Performance tracking (Caching logic moved to agents)\n",
    "performance_stats = {\n",
    "    \"grading_calls\": 0,\n",
    "    \"moderation_calls\": 0,\n",
    "    \"total_processing_time\": 0,\n",
    "    \"errors\": [],\n",
    "}\n",
    "\n",
    "# Ensure cache directory exists\n",
    "cache_dir = \"../cache\"\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Robust caching system initialized (Logic moved to agents)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a3554fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Robust OCR functions initialized (Agent-based)\n"
     ]
    }
   ],
   "source": [
    "# Robust OCR Functions with Retry Logic (Agent-based)\n",
    "\n",
    "\n",
    "def get_cropped_image_bytes(image_path, left, top, width, height):\n",
    "    \"\"\"Crop and enhance image, return bytes\"\"\"\n",
    "    try:\n",
    "        with Image.open(image_path) as im:\n",
    "            crop_box = (left, top, left + width, top + height)\n",
    "            im_crop = im.crop(crop_box)\n",
    "            enhancer = ImageEnhance.Sharpness(im_crop)\n",
    "            im_crop = enhancer.enhance(3)\n",
    "            \n",
    "            from io import BytesIO\n",
    "            img_byte_arr = BytesIO()\n",
    "            im_crop.save(img_byte_arr, format='PNG')\n",
    "            return img_byte_arr.getvalue()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Image processing failed: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "async def ocr_image_from_file(question, image_path, left, top, width, height):\n",
    "    \"\"\"Robust OCR processing with caching via AI Agent\"\"\"\n",
    "    if question == \"NAME\":\n",
    "        return \"\"\n",
    "\n",
    "    try:\n",
    "        image_data = get_cropped_image_bytes(image_path, left, top, width, height)\n",
    "        if not image_data:\n",
    "            return \"\"\n",
    "\n",
    "        # Create prompt based on question type\n",
    "        if question == \"ID\":\n",
    "            text_message = \"\"\"Extract text in this image. It is a Student ID in 9 digit number.\n",
    "Return only the 9-digit Student ID with no other words. Strip whitespace.\n",
    "If you cannot extract Student ID, return 'No text found!!!'.\"\"\"\n",
    "        elif question == \"CLASS\":\n",
    "            text_message = \"\"\"Extract the class code from this image.\n",
    "Return only the class value with no other words. Strip whitespace.\n",
    "If you cannot extract the class value, return 'No text found!!!'.\"\"\"\n",
    "        else:\n",
    "            text_message = \"\"\"Extract only the handwritten text from this image.\n",
    "Ignore printed text. Preserve original formatting and line breaks.\n",
    "Return exactly the extracted handwritten text. Strip whitespace.\n",
    "If you cannot extract text, return 'No text found!!!'.\"\"\"\n",
    "\n",
    "        # Use Agent (caching handled internally)\n",
    "        ocr_text = await perform_ocr_with_ai(text_message, image_data=image_data)\n",
    "\n",
    "        print(f\"{question} {os.path.basename(image_path)}: {ocr_text[:50]}\")\n",
    "\n",
    "        return \"\" if ocr_text == \"No text found!!!\" else ocr_text\n",
    "    except Exception as e:\n",
    "        logger.error(f\"OCR failed for {question} {image_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "print(\"‚úÖ Robust OCR functions initialized (Agent-based)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7d544645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Robust grading system initialized\n"
     ]
    }
   ],
   "source": [
    "# Robust Grading System\n",
    "from agents.grading_agent.agent import GradingResult, grade_answer_with_ai, grade_answer_with_ocr_and_ai\n",
    "\n",
    "print(\"‚úÖ Robust grading system initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "45740bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Robust moderation system initialized\n"
     ]
    }
   ],
   "source": [
    "# Robust Moderation System\n",
    "from typing import List\n",
    "from agents.moderation_agent.agent import (\n",
    "    ModerationItem,\n",
    "    ModerationResponse,\n",
    "    moderate_grades_with_ai,\n",
    ")\n",
    "\n",
    "\n",
    "async def grade_moderator(question, answers, grading_results, row_numbers):\n",
    "    \"\"\"Use Gemini to harmonize marks across similar answers (via agent)\"\"\"\n",
    "    performance_stats[\"moderation_calls\"] += 1\n",
    "\n",
    "    question_text = standard_question_text.get(question, \"\")\n",
    "    marking_scheme_text = standard_answer_dict.get(question, \"\")\n",
    "    total_marks = standard_mark.get(question, 0)\n",
    "\n",
    "    entries = []\n",
    "    for row_num, ans, res in zip(row_numbers, answers, grading_results):\n",
    "        entries.append(\n",
    "            {\n",
    "                \"row\": int(row_num),\n",
    "                \"answer\": str(ans or \"\"),\n",
    "                \"mark\": float(res.mark),\n",
    "                \"reasoning\": str(res.reasoning or \"\"),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Use Agent (caching handled internally)\n",
    "    return await moderate_grades_with_ai(\n",
    "        question_text, marking_scheme_text, total_marks, entries\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"‚úÖ Robust moderation system initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2e5f7df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Image organization complete\n",
      "   Total images: 8\n",
      "   Max page: 2\n"
     ]
    }
   ],
   "source": [
    "# Image Processing and Data Organization Functions\n",
    "\n",
    "\n",
    "def get_the_list_of_files(path):\n",
    "    \"\"\"Get the list of files in the directory\"\"\"\n",
    "    files = []\n",
    "    for dirpath, dirnames, filenames in os.walk(path):\n",
    "        files.extend(filenames)\n",
    "        break\n",
    "    return sorted(files)\n",
    "\n",
    "\n",
    "def calculate_max_page(annotations_list):\n",
    "    \"\"\"Calculate maximum page number from annotations\"\"\"\n",
    "    max_page = max((ann[\"page\"] for ann in annotations_list), default=0)\n",
    "    return max_page + (1 if max_page % 2 == 1 else max_page + 2)\n",
    "\n",
    "\n",
    "def organize_images_by_page(images, max_page):\n",
    "    \"\"\"Organize images into page buckets\"\"\"\n",
    "    images_by_page = [[] for _ in range(max_page)]\n",
    "    for image in images:\n",
    "        page_num = int(image.split(\".\")[0])\n",
    "        page_index = page_num % max_page\n",
    "        images_by_page[page_index].append(image)\n",
    "    return images_by_page\n",
    "\n",
    "\n",
    "# Organize images\n",
    "images = get_the_list_of_files(base_path_images)\n",
    "max_page = calculate_max_page(annotations_list)\n",
    "images_by_page = organize_images_by_page(images, max_page)\n",
    "\n",
    "print(f\"‚úÖ Image organization complete\")\n",
    "print(f\"   Total images: {len(images)}\")\n",
    "print(f\"   Max page: {max_page}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0c444dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Template rendering functions initialized\n"
     ]
    }
   ],
   "source": [
    "# Template Rendering Functions\n",
    "\n",
    "\n",
    "def get_template_name(question):\n",
    "    \"\"\"Determine which HTML template to use\"\"\"\n",
    "    if question in [\"ID\", \"NAME\", \"CLASS\"]:\n",
    "        return \"questions/index-answer.html\"\n",
    "    return \"questions/index.html\"\n",
    "\n",
    "\n",
    "def render_question_html(question, dataTable):\n",
    "    \"\"\"Render the main HTML page for a question\"\"\"\n",
    "    current_index = questions.index(question) if question in questions else -1\n",
    "    prev_question = questions[current_index - 1] if current_index > 0 else None\n",
    "    next_question = (\n",
    "        questions[current_index + 1] if current_index < len(questions) - 1 else None\n",
    "    )\n",
    "\n",
    "    template = env.get_template(get_template_name(question))\n",
    "    return template.render(\n",
    "        studentsScriptFileName=file_name,\n",
    "        question=question,\n",
    "        standardAnswer=standard_answer_dict.get(question, \"\"),\n",
    "        standardMark=standard_mark.get(question, \"\"),\n",
    "        estimatedBoundingBox=annotations_dict[question],\n",
    "        dataTable=dataTable,\n",
    "        prev_question=prev_question,\n",
    "        next_question=next_question,\n",
    "    )\n",
    "\n",
    "\n",
    "def render_question_js(question, dataTable):\n",
    "    \"\"\"Render the JavaScript file for a question\"\"\"\n",
    "    template = env.get_template(\"questions/question.js\")\n",
    "    return template.render(\n",
    "        dataTable=dataTable,\n",
    "        estimatedBoundingBox=annotations_dict[question],\n",
    "    )\n",
    "\n",
    "\n",
    "def render_question_css(dataTable):\n",
    "    \"\"\"Render the CSS file for a question\"\"\"\n",
    "    template = env.get_template(\"questions/style.css\")\n",
    "    return template.render(dataTable=dataTable)\n",
    "\n",
    "\n",
    "def save_question_data(question, dataTable):\n",
    "    \"\"\"Save CSV data for a question\"\"\"\n",
    "    question_dir = Path(base_path_questions) / question\n",
    "    question_dir.mkdir(parents=True, exist_ok=True)\n",
    "    dataTable.to_csv(question_dir / \"data.csv\", index=False)\n",
    "\n",
    "\n",
    "def save_template_output(output, question, filename):\n",
    "    \"\"\"Save rendered template to question folder\"\"\"\n",
    "    question_dir = Path(base_path_questions, question)\n",
    "    question_dir.mkdir(parents=True, exist_ok=True)\n",
    "    output_file = question_dir / filename\n",
    "    output_file.write_text(output)\n",
    "\n",
    "\n",
    "print(\"‚úÖ Template rendering functions initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c679ae34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ea7ba07fd264e71ba68117403d269ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, description='Processing:', max=8)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1/8: NAME\n",
      "Processing 2/8: ID\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-09 07:52:40,247 - INFO - OCR cache hit for hash 600d7cec\n",
      "2026-01-09 07:52:40,268 - INFO - OCR cache hit for hash 56e025fa\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID 0.jpg: 123456789\n",
      "ID 2.jpg: 987654321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-09 07:52:40,290 - INFO - OCR cache hit for hash 5614e859\n",
      "2026-01-09 07:52:40,311 - INFO - OCR cache hit for hash d55cd270\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID 4.jpg: 234567890\n",
      "ID 6.jpg: 345678912\n",
      "Processing 3/8: CLASS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-09 07:52:40,342 - INFO - OCR cache hit for hash 38ab9695\n",
      "2026-01-09 07:52:40,363 - INFO - OCR cache hit for hash a5e08b7f\n",
      "2026-01-09 07:52:40,383 - INFO - OCR cache hit for hash 1bb00a3a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASS 0.jpg: A\n",
      "CLASS 2.jpg: B\n",
      "CLASS 4.jpg: C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-09 07:52:40,403 - INFO - OCR cache hit for hash eecbb0bb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASS 6.jpg: D\n",
      "Processing 4/8: Q1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-09 07:52:40,478 - INFO - OCR+Grading cache hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1 0.jpg: Vocational and Professional Ed... -> Mark: 2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-09 07:52:40,541 - INFO - OCR+Grading cache hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1 2.jpg: Vacational and professional Ed... -> Mark: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-09 07:52:40,596 - INFO - OCR+Grading cache hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1 4.jpg: Hong Kong skilled labor force... -> Mark: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-09 07:52:40,669 - INFO - OCR+Grading cache hit\n",
      "2026-01-09 07:52:40,672 - INFO - Moderation cache hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1 6.jpg: Vocational and Professional Ed... -> Mark: 2.0\n",
      "Processing 5/8: Q2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-09 07:52:40,789 - INFO - OCR+Grading cache hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q2 0.jpg: IVE is Highed Diploma\n",
      "THEi is ... -> Mark: 10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-09 07:52:40,852 - INFO - OCR+Grading cache hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q2 2.jpg: HD is IVE\n",
      "Degree is THEi... -> Mark: 10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-09 07:52:40,920 - INFO - OCR+Grading cache hit\n",
      "2026-01-09 07:52:40,987 - INFO - OCR+Grading cache hit\n",
      "2026-01-09 07:52:40,991 - INFO - Moderation cache hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q2 4.jpg: IVE is VTC\n",
      "thei is also VTC... -> Mark: 0.0\n",
      "Q2 6.jpg: higher Diploma for IVE Degree ... -> Mark: 10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-09 07:52:41,055 - INFO - OCR+Grading cache hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 6/8: Q3\n",
      "Q3 0.jpg: thinking and doing... -> Mark: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-09 07:52:41,108 - INFO - OCR+Grading cache hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q3 2.jpg: Sorry I don't know... -> Mark: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-09 07:52:41,162 - INFO - OCR+Grading cache hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q3 4.jpg: brainpowe to doing\n",
      "hand-on... -> Mark: 4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-09 07:52:41,210 - INFO - OCR+Grading cache hit\n",
      "2026-01-09 07:52:41,214 - INFO - Moderation cache hit\n",
      "2026-01-09 07:52:41,287 - INFO - OCR+Grading cache hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q3 6.jpg: Yeah... -> Mark: 0.0\n",
      "Processing 7/8: Q4\n",
      "Q4 1.jpg: DFS -> Higher Diploma... -> Mark: 9.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-09 07:52:41,332 - INFO - OCR+Grading cache hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q4 3.jpg: ... -> Mark: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-09 07:52:41,389 - INFO - OCR+Grading cache hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q4 5.jpg: Ha ha good!... -> Mark: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-09 07:52:41,437 - INFO - OCR+Grading cache hit\n",
      "2026-01-09 07:52:41,440 - INFO - Moderation cache hit\n",
      "2026-01-09 07:52:41,510 - INFO - OCR+Grading cache hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q4 7.jpg: ... -> Mark: 0.0\n",
      "Processing 8/8: Q5\n",
      "Q5 1.jpg: Intenship... -> Mark: 3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-09 07:52:41,558 - INFO - OCR+Grading cache hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q5 3.jpg: ... -> Mark: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-09 07:52:41,633 - INFO - OCR+Grading cache hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q5 5.jpg: Intern, placement, industry... -> Mark: 6.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-09 07:52:41,725 - INFO - OCR+Grading cache hit\n",
      "2026-01-09 07:52:41,731 - INFO - Moderation cache hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q5 7.jpg: ... -> Mark: 0.0\n",
      "‚úì Completed processing 8 questions\n",
      "üìä Performance Stats:\n",
      "   Grading calls: 20\n",
      "   Moderation calls: 5\n"
     ]
    }
   ],
   "source": [
    "# Main Processing Functions\n",
    "from agents.ocr_agent.agent import perform_ocr_with_ai\n",
    "\n",
    "\n",
    "def process_metadata_question(num_rows):\n",
    "    \"\"\"Create default data for metadata questions\"\"\"\n",
    "    return {\n",
    "        \"Similarity\": [0.0] * num_rows,\n",
    "        \"Reasoning\": [\"\"] * num_rows,\n",
    "        \"MarkRaw\": [0.0] * num_rows,\n",
    "        \"Mark\": [0.0] * num_rows,\n",
    "        \"ModeratorFlag\": [False] * num_rows,\n",
    "        \"ModeratorNote\": [\"\"] * num_rows,\n",
    "    }\n",
    "\n",
    "\n",
    "async def get_df(question):\n",
    "    \"\"\"Build dataframe with OCR results and grading for a question\"\"\"\n",
    "    annotation = annotations_dict[question].copy()\n",
    "    page_num = annotation[\"page\"]\n",
    "    images_for_page = images_by_page[page_num]\n",
    "\n",
    "    image_paths = [\"images/\" + img for img in images_for_page]\n",
    "    num_images = len(images_for_page)\n",
    "\n",
    "    data = pd.DataFrame(\n",
    "        {key: [annotation[key]] * num_images for key in annotation.keys()}\n",
    "    )\n",
    "    data[\"Image\"] = image_paths\n",
    "    \n",
    "    data[\"RowNumber\"] = range(1, num_images + 1)\n",
    "    data[\"maskPage\"] = page_num\n",
    "\n",
    "    # Process based on question type\n",
    "    if question in [\"ID\", \"NAME\", \"CLASS\"]:\n",
    "        # Metadata: OCR only\n",
    "        answers = []\n",
    "        for image in images_for_page:\n",
    "            image_path = os.path.join(base_path, \"images\", image)\n",
    "            answer = await ocr_image_from_file(\n",
    "                question,\n",
    "                image_path,\n",
    "                annotation[\"left\"],\n",
    "                annotation[\"top\"],\n",
    "                annotation[\"width\"],\n",
    "                annotation[\"height\"],\n",
    "            )\n",
    "            answers.append(answer)\n",
    "        data[\"Answer\"] = answers\n",
    "        grading_data = process_metadata_question(num_images)\n",
    "    \n",
    "    else:\n",
    "        # Graded questions: Sequential OCR + Grading\n",
    "        question_text = standard_question_text.get(question, \"\")\n",
    "        marking_scheme_text = standard_answer_dict.get(question, \"\")\n",
    "        total_marks = standard_mark.get(question, 0)\n",
    "        \n",
    "        results = []\n",
    "        answers = []\n",
    "        \n",
    "        for image in images_for_page:\n",
    "            image_path = os.path.join(base_path, \"images\", image)\n",
    "            image_data = get_cropped_image_bytes(\n",
    "                image_path,\n",
    "                annotation[\"left\"],\n",
    "                annotation[\"top\"],\n",
    "                annotation[\"width\"],\n",
    "                annotation[\"height\"]\n",
    "            )\n",
    "            \n",
    "            if image_data:\n",
    "                performance_stats[\"grading_calls\"] += 1\n",
    "                # Use Sequential Agent (OCR + Grading)\n",
    "                result = await grade_answer_with_ocr_and_ai(\n",
    "                    question_text, \n",
    "                    marking_scheme_text, \n",
    "                    total_marks, \n",
    "                    image_data\n",
    "                )\n",
    "            else:\n",
    "                result = GradingResult(extracted_text=\"\", similarity_score=0.0, mark=0.0, reasoning=\"Image processing failed\")\n",
    "            \n",
    "            results.append(result)\n",
    "            answers.append(result.extracted_text)\n",
    "            \n",
    "            print(f\"{question} {image}: {result.extracted_text[:30]}... -> Mark: {result.mark}\")\n",
    "        \n",
    "        data[\"Answer\"] = answers\n",
    "        \n",
    "        # Run moderation on the results\n",
    "        moderation = await grade_moderator(question, answers, results, data[\"RowNumber\"].tolist())\n",
    "        \n",
    "        grading_data = {\n",
    "            \"Similarity\": [result.similarity_score for result in results],\n",
    "            \"Reasoning\": [result.reasoning for result in results],\n",
    "            \"MarkRaw\": [result.mark for result in results],\n",
    "            \"Mark\": [m[\"moderated_mark\"] for m in moderation],\n",
    "            \"ModeratorFlag\": [m[\"flag\"] for m in moderation],\n",
    "            \"ModeratorNote\": [m[\"note\"] for m in moderation],\n",
    "        }\n",
    "\n",
    "    for col, values in grading_data.items():\n",
    "        data[col] = values\n",
    "\n",
    "    data[\"page\"] = data[\"Image\"].str.replace(\"images/\", \"\").str.replace(\".jpg\", \"\")\n",
    "    return data\n",
    "\n",
    "\n",
    "async def process_single_question(question):\n",
    "    \"\"\"Process one question: OCR, grade, and generate all output files\"\"\"\n",
    "    dataTable = await get_df(question)\n",
    "    save_question_data(question, dataTable)\n",
    "    save_template_output(\n",
    "        render_question_html(question, dataTable), question, \"index.html\"\n",
    "    )\n",
    "    save_template_output(\n",
    "        render_question_js(question, dataTable), question, \"question.js\"\n",
    "    )\n",
    "    save_template_output(render_question_css(dataTable), question, \"style.css\")\n",
    "\n",
    "\n",
    "# Process all questions with progress bar\n",
    "max_count = len(questions)\n",
    "progress_bar = IntProgress(min=0, max=max_count, description=\"Processing:\")\n",
    "display(progress_bar)\n",
    "\n",
    "for idx, question in enumerate(questions, 1):\n",
    "    print(f\"Processing {idx}/{max_count}: {question}\")\n",
    "    await process_single_question(question)\n",
    "    progress_bar.value = idx\n",
    "\n",
    "print(f\"‚úì Completed processing {max_count} questions\")\n",
    "print(f\"üìä Performance Stats:\")\n",
    "print(f\"   Grading calls: {performance_stats['grading_calls']}\")\n",
    "print(f\"   Moderation calls: {performance_stats['moderation_calls']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9d469d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All student IDs validated successfully!\n"
     ]
    }
   ],
   "source": [
    "# Student ID Validation\n",
    "\n",
    "id_from_oscr = pd.read_csv(base_path_questions + \"/\" + \"ID\" + \"/data.csv\")[\n",
    "    \"Answer\"\n",
    "].tolist()\n",
    "id_from_oscr = [str(int(float(x))) if pd.notna(x) else x for x in id_from_oscr]\n",
    "\n",
    "id_from_namelist = name_list_df[\"ID\"].to_list()\n",
    "\n",
    "# Check duplicate IDs\n",
    "duplicate_id = []\n",
    "for id in id_from_oscr:\n",
    "    if id_from_oscr.count(id) > 1:\n",
    "        duplicate_id.append(id)\n",
    "duplicate_id = list(set(duplicate_id))\n",
    "if len(duplicate_id) > 0:\n",
    "    print(colored(\"Duplicate ID: {}\".format(duplicate_id), \"red\"))\n",
    "\n",
    "id_from_oscr = [str(id) for id in id_from_oscr]\n",
    "id_from_namelist = [str(id) for id in id_from_namelist]\n",
    "\n",
    "# Compare OCR ID and name list\n",
    "ocr_missing_id = []\n",
    "name_list_missing_id = []\n",
    "for id in id_from_oscr:\n",
    "    if id not in id_from_namelist:\n",
    "        name_list_missing_id.append(id)\n",
    "\n",
    "for id in id_from_namelist:\n",
    "    if id not in id_from_oscr:\n",
    "        ocr_missing_id.append(id)\n",
    "\n",
    "# Report OCR scan errors\n",
    "if len(name_list_missing_id) > 0:\n",
    "    print(colored(\"Some IDs from OCR are not in NameList - fix manually!\", \"red\"))\n",
    "    for id in name_list_missing_id:\n",
    "        print(colored(id, \"red\"))\n",
    "\n",
    "# Report potential absences\n",
    "if len(ocr_missing_id) > 0:\n",
    "    print(colored(f\"Number of absentees: {len(ocr_missing_id)}\", \"red\"))\n",
    "    print(colored(\"IDs in Name List not found in OCR:\", \"red\"))\n",
    "    for id in ocr_missing_id:\n",
    "        print(colored(id, \"red\"))\n",
    "\n",
    "if not duplicate_id and not name_list_missing_id and not ocr_missing_id:\n",
    "    print(\"‚úÖ All student IDs validated successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2c7ee4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üéâ PROCESSING COMPLETE!\n",
      "============================================================\n",
      "\n",
      "To view results, start the web server at root level:\n",
      "\n",
      "source .venv/bin/activate && file_name=\"VTC Test\" python server.py 8000\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Start Python HTTP Server\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéâ PROCESSING COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTo view results, start the web server at root level:\\n\")\n",
    "print(f'source .venv/bin/activate && file_name=\"{file_name}\" python server.py 8000')\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üöÄ STEP 4: SCORING PREPROCESSING READY\n",
      "============================================================\n",
      "\n",
      "üìä Configuration Summary:\n",
      "   Dataset: sample\n",
      "   Prefix: VTC Test\n",
      "   Questions: 8 total, 5 for answers\n",
      "   Total marks: 50\n",
      "\n",
      "üîß System Status:\n",
      "   ‚úÖ OCR function: Robust with retry logic\n",
      "   ‚úÖ Grading system: Robust with validation\n",
      "   ‚úÖ Caching: Robust with integrity checks\n",
      "   ‚úÖ Error handling: Comprehensive\n",
      "\n",
      "üìÅ File Status:\n",
      "   ‚úÖ PDF file: VTC Test.pdf\n",
      "   ‚úÖ Name list: VTC Test Name List.xlsx\n",
      "   ‚úÖ Marking scheme: VTC Test Marking Scheme.xlsx\n",
      "   ‚úÖ Annotations: annotations.json\n",
      "   ‚úÖ Index.html: Generated\n",
      "\n",
      "üéØ Next Steps:\n",
      "   1. Run OCR processing on scanned images\n",
      "   2. Execute auto-grading with Gemini\n",
      "   3. Generate review pages for manual verification\n",
      "   4. Proceed to Step 5: Post-Scoring Checks\n",
      "\n",
      "üí° Robust Features Active:\n",
      "   ‚Ä¢ Comprehensive error handling and recovery\n",
      "   ‚Ä¢ Progress tracking with detailed status updates\n",
      "   ‚Ä¢ Robust caching with integrity validation\n",
      "   ‚Ä¢ Detailed logging and performance monitoring\n",
      "   ‚Ä¢ Automatic retry logic for failed operations\n",
      "   ‚Ä¢ Input validation and sanitization\n",
      "\n",
      "============================================================\n",
      "‚úÖ Robust Step 4 initialization completed at 07:52:41\n",
      "Ready for OCR and grading operations!\n",
      "============================================================\n",
      "\n",
      "üí° Robust version includes complete OCR and grading implementation.\n",
      "   Ready to process images and generate review pages!\n"
     ]
    }
   ],
   "source": [
    "# Robust processing summary and next steps\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üöÄ STEP 4: SCORING PREPROCESSING READY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìä Configuration Summary:\")\n",
    "print(f\"   Dataset: sample\")\n",
    "print(f\"   Prefix: {prefix}\")\n",
    "print(f\"   Questions: {len(questions)} total, {len(question_with_answer)} for answers\")\n",
    "print(\n",
    "    f\"   Total marks: {sum(standard_mark.values()) if 'standard_mark' in locals() else 'N/A'}\"\n",
    ")\n",
    "\n",
    "print(f\"\\nüîß System Status:\")\n",
    "print(f\"   ‚úÖ OCR function: Robust with retry logic\")\n",
    "print(f\"   ‚úÖ Grading system: Robust with validation\")\n",
    "print(f\"   ‚úÖ Caching: Robust with integrity checks\")\n",
    "print(f\"   ‚úÖ Error handling: Comprehensive\")\n",
    "\n",
    "print(f\"\\nüìÅ File Status:\")\n",
    "print(f\"   ‚úÖ PDF file: {os.path.basename(pdf_file)}\")\n",
    "print(f\"   ‚úÖ Name list: {os.path.basename(name_list_file)}\")\n",
    "print(f\"   ‚úÖ Marking scheme: {os.path.basename(marking_scheme_file)}\")\n",
    "print(f\"   ‚úÖ Annotations: {os.path.basename(annotations_path)}\")\n",
    "print(f\"   ‚úÖ Index.html: Generated\")\n",
    "\n",
    "print(f\"\\nüéØ Next Steps:\")\n",
    "print(f\"   1. Run OCR processing on scanned images\")\n",
    "print(f\"   2. Execute auto-grading with Gemini\")\n",
    "print(f\"   3. Generate review pages for manual verification\")\n",
    "print(f\"   4. Proceed to Step 5: Post-Scoring Checks\")\n",
    "\n",
    "print(f\"\\nüí° Robust Features Active:\")\n",
    "print(f\"   ‚Ä¢ Comprehensive error handling and recovery\")\n",
    "print(f\"   ‚Ä¢ Progress tracking with detailed status updates\")\n",
    "print(f\"   ‚Ä¢ Robust caching with integrity validation\")\n",
    "print(f\"   ‚Ä¢ Detailed logging and performance monitoring\")\n",
    "print(f\"   ‚Ä¢ Automatic retry logic for failed operations\")\n",
    "print(f\"   ‚Ä¢ Input validation and sanitization\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\n",
    "    f\"‚úÖ Robust Step 4 initialization completed at {datetime.now().strftime('%H:%M:%S')}\"\n",
    ")\n",
    "print(\"Ready for OCR and grading operations!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nüí° Robust version includes complete OCR and grading implementation.\")\n",
    "print(\"   Ready to process images and generate review pages!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
