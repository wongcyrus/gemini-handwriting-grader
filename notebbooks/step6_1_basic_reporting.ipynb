{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a73d9bda",
   "metadata": {},
   "source": [
    "# Step 6.1: Basic Reporting & Archiving\n",
    "Generates score reports, scored PDFs, samples, and performs backup.\n",
    "\n",
    "**Features:**\n",
    "- ‚úÖ Score calculation and Excel reporting\n",
    "- ‚úÖ Scored script image generation\n",
    "- ‚úÖ Individual PDF generation\n",
    "- ‚úÖ Sample collection generation\n",
    "- ‚úÖ Project backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dee5bcbb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T06:56:36.077813Z",
     "iopub.status.busy": "2026-01-08T06:56:36.075811Z",
     "iopub.status.idle": "2026-01-08T06:56:42.088853Z",
     "shell.execute_reply": "2026-01-08T06:56:42.081533Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Robust Step 6: Post-Scoring Packaging initialized\n",
      "‚úì Session started at: 2026-01-08 07:38:28\n",
      "‚úì Paths configured and directories created\n",
      "üí° Metadata questions (NAME, ID, CLASS) will be excluded from answer analysis\n"
     ]
    }
   ],
   "source": [
    "from grading_utils import setup_paths, create_directories, build_student_id_mapping\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from IPython.display import display, clear_output\n",
    "from ipywidgets import IntProgress, HTML\n",
    "import logging\n",
    "from pypdf import PdfReader, PdfWriter\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import hashlib\n",
    "import math\n",
    "from docx import Document\n",
    "from docx.enum.text import WD_ALIGN_PARAGRAPH\n",
    "from docx.shared import Pt, Inches\n",
    "\n",
    "\n",
    "# Robust logging setup\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"‚úÖ Robust Step 6: Post-Scoring Packaging initialized\")\n",
    "print(f\"‚úì Session started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Configuration\n",
    "passingMark = 15  # Adjust as needed\n",
    "prefix = \"VTC Test\"\n",
    "paths = setup_paths(prefix, \"sample\")\n",
    "\n",
    "# Extract commonly used paths\n",
    "pdf_file = paths[\"pdf_file\"]\n",
    "name_list_file = paths[\"name_list_file\"]\n",
    "base_path = paths[\"base_path\"]\n",
    "base_path_images = paths[\"base_path_images\"]\n",
    "base_path_annotations = paths[\"base_path_annotations\"]\n",
    "base_path_questions = paths[\"base_path_questions\"]\n",
    "base_path_marked_images = paths[\"base_path_marked_images\"]\n",
    "base_path_marked_pdfs = paths[\"base_path_marked_pdfs\"]\n",
    "base_path_marked_scripts = paths[\"base_path_marked_scripts\"]\n",
    "CACHE_DIR = paths.get(\"cache_dir\", \"../cache\")\n",
    "\n",
    "# Create all necessary directories\n",
    "create_directories(paths)\n",
    "\n",
    "print(\"‚úì Paths configured and directories created\")\n",
    "\n",
    "# Metadata questions that should be excluded from answer analysis\n",
    "METADATA_QUESTIONS = [\"NAME\", \"ID\", \"CLASS\"]\n",
    "\n",
    "print(\"üí° Metadata questions (NAME, ID, CLASS) will be excluded from answer analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee4ca495",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T06:56:42.099209Z",
     "iopub.status.busy": "2026-01-08T06:56:42.095736Z",
     "iopub.status.idle": "2026-01-08T06:56:42.621608Z",
     "shell.execute_reply": "2026-01-08T06:56:42.617968Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-08 07:38:28,206 - INFO - üìä Generating score report...\n",
      "2026-01-08 07:38:28,392 - INFO - ‚úì Loaded 4 student names from name list\n",
      "2026-01-08 07:38:28,395 - INFO - ‚úì Built student ID mapping for 2 pages\n",
      "2026-01-08 07:38:28,396 - INFO - ‚úì Processed marks for Q5: 4 students\n",
      "2026-01-08 07:38:28,399 - INFO - ‚úì Processed marks for Q4: 4 students\n",
      "2026-01-08 07:38:28,401 - INFO - ‚úì Processed marks for NAME: 4 students\n",
      "2026-01-08 07:38:28,403 - INFO - ‚úì Processed marks for CLASS: 4 students\n",
      "2026-01-08 07:38:28,406 - INFO - ‚úì Processed marks for Q3: 4 students\n",
      "2026-01-08 07:38:28,409 - INFO - ‚úì Processed marks for Q2: 4 students\n",
      "2026-01-08 07:38:28,412 - INFO - ‚úì Processed marks for Q1: 4 students\n",
      "2026-01-08 07:38:28,421 - INFO - ‚úì Processed marks for ID: 4 students\n",
      "2026-01-08 07:38:28,424 - INFO - ‚úì Processed marks from 8 questions\n",
      "2026-01-08 07:38:28,446 - INFO - ‚úì Generated marks report for 4 students\n",
      "2026-01-08 07:38:28,450 - INFO -   Average score: 11.00\n",
      "2026-01-08 07:38:28,452 - INFO -   Score range: 7.0 - 20.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>NAME</th>\n",
       "      <th>CLASS</th>\n",
       "      <th>Q1</th>\n",
       "      <th>Q2</th>\n",
       "      <th>Q3</th>\n",
       "      <th>Q4</th>\n",
       "      <th>Q5</th>\n",
       "      <th>Marks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>234567890</th>\n",
       "      <td>234567890</td>\n",
       "      <td>John</td>\n",
       "      <td>C</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123456789</th>\n",
       "      <td>123456789</td>\n",
       "      <td>Peter</td>\n",
       "      <td>A</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987654321</th>\n",
       "      <td>987654321</td>\n",
       "      <td>Mary</td>\n",
       "      <td>B</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345678912</th>\n",
       "      <td>345678912</td>\n",
       "      <td>Susan</td>\n",
       "      <td>D</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  ID   NAME CLASS   Q1   Q2   Q3   Q4   Q5  Marks\n",
       "234567890  234567890   John     C  0.0  1.0  3.0  0.0  5.0    9.0\n",
       "123456789  123456789  Peter     A  2.0  6.0  1.0  8.0  3.0   20.0\n",
       "987654321  987654321   Mary     B  1.0  6.0  0.0  0.0  0.0    7.0\n",
       "345678912  345678912  Susan     D  2.0  6.0  0.0  0.0  0.0    8.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Robust score report generation with validation and analytics\n",
    "\n",
    "def generate_score_report():\n",
    "    \"\"\"Generate comprehensive score report with validation and analytics.\"\"\"\n",
    "    logger.info(\"üìä Generating score report...\")\n",
    "    try:\n",
    "        name_list_df = pd.read_excel(name_list_file, sheet_name=\"Name List\")\n",
    "        id_col = next((col for col in name_list_df.columns if col.lower() == \"id\"), None)\n",
    "        name_col = next(\n",
    "            (col for col in name_list_df.columns if col.lower() in {\"name\", \"student name\", \"student_name\"}),\n",
    "            None,\n",
    "        )\n",
    "        if id_col is None or name_col is None:\n",
    "            raise ValueError(\"Name list must contain ID and NAME columns.\")\n",
    "\n",
    "        name_map = (\n",
    "            name_list_df.assign(**{id_col: name_list_df[id_col].astype(str)})\n",
    "            .set_index(id_col)[name_col]\n",
    "            .astype(str)\n",
    "            .to_dict()\n",
    "        )\n",
    "        logger.info(f\"‚úì Loaded {len(name_map)} student names from name list\")\n",
    "\n",
    "        pageToStudentId, numberOfPage, getStudentId = build_student_id_mapping(\n",
    "            base_path_questions, base_path_annotations\n",
    "        )\n",
    "        logger.info(f\"‚úì Built student ID mapping for {numberOfPage} pages\")\n",
    "\n",
    "        questionAndMarks = {}\n",
    "        questions_processed = 0\n",
    "        for path, _, files in os.walk(base_path_questions):\n",
    "            for file in files:\n",
    "                if file == \"mark.json\":\n",
    "                    question = path[len(base_path_questions) + 1 :]\n",
    "                    try:\n",
    "                        with open(os.path.join(path, file), \"r\", encoding=\"utf-8\") as f:\n",
    "                            data = json.load(f)\n",
    "                        marks = {}\n",
    "                        for item in data:\n",
    "                            studentId = getStudentId(int(item[\"id\"]))\n",
    "                            marks[studentId] = (\n",
    "                                item[\"overridedMark\"] if item[\"overridedMark\"] != \"\" else item[\"mark\"]\n",
    "                            )\n",
    "                        questionAndMarks[question] = marks\n",
    "                        questions_processed += 1\n",
    "                        logger.info(f\"‚úì Processed marks for {question}: {len(marks)} students\")\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"‚ùå Failed to process marks for {question}: {e}\")\n",
    "                        continue\n",
    "\n",
    "        logger.info(f\"‚úì Processed marks from {questions_processed} questions\")\n",
    "        if not questionAndMarks:\n",
    "            raise ValueError(\"No question marks were processed.\")\n",
    "       \n",
    "        marksDf = pd.DataFrame(questionAndMarks)\n",
    "\n",
    "        # Reorder columns: ID, NAME, CLASS first, then questions sorted\n",
    "        base_cols = [\"ID\", \"NAME\", \"CLASS\"]\n",
    "        question_cols = [\n",
    "            col\n",
    "            for col in sorted(marksDf.columns)\n",
    "            if col not in base_cols\n",
    "        ]\n",
    "        marksDf = marksDf[base_cols + question_cols]\n",
    "\n",
    "        marksDf[\"NAME\"] = marksDf[\"ID\"].map(name_map).fillna(marksDf[\"NAME\"])\n",
    "\n",
    "        \n",
    "\n",
    "        # Calculate total marks from question columns only\n",
    "        marksDf[\"Marks\"] = (\n",
    "            marksDf.loc[:, ~marksDf.columns.isin([\"ID\", \"NAME\", \"CLASS\"])]\n",
    "            .apply(pd.to_numeric, errors=\"coerce\")\n",
    "            .sum(axis=1)\n",
    "        )\n",
    "\n",
    "        invalid_marks = marksDf[marksDf[\"Marks\"].isna()]\n",
    "        if not invalid_marks.empty:\n",
    "            logger.warning(f\"Found {len(invalid_marks)} students with invalid marks\")\n",
    "\n",
    "        logger.info(f\"‚úì Generated marks report for {len(marksDf)} students\")\n",
    "        logger.info(f\"  Average score: {marksDf['Marks'].mean():.2f}\")\n",
    "        logger.info(f\"  Score range: {marksDf['Marks'].min():.1f} - {marksDf['Marks'].max():.1f}\")\n",
    "\n",
    "        return marksDf\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Score report generation failed: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "marksDf = generate_score_report()\n",
    "display(marksDf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36db3625",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T06:56:42.627411Z",
     "iopub.status.busy": "2026-01-08T06:56:42.625753Z",
     "iopub.status.idle": "2026-01-08T06:56:43.779831Z",
     "shell.execute_reply": "2026-01-08T06:56:43.777982Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-08 07:38:28,829 - INFO - ‚úì Copied 8 images to marked folder\n",
      "2026-01-08 07:38:28,835 - INFO - ‚úì Processed 8 annotations\n",
      "2026-01-08 07:38:28,838 - INFO - ‚úì Built student-to-page mapping for 4 students\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Creating scored scripts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eca65f1208be4cfd8050886da5386f8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, description='Adding marks', max=4)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-08 07:38:30,022 - INFO - ‚úì Added marks to images for 4 students\n"
     ]
    }
   ],
   "source": [
    "# Robust scored scripts creation with comprehensive validationdef create_scored_scripts():    \"\"\"Create scored scripts with validation and error handling\"\"\"    print(\"üìÑ Creating scored scripts...\")        try:        # Copy raw images to marked folder with validation        if os.path.exists(base_path_marked_images):            shutil.rmtree(base_path_marked_images)                copied_path = shutil.copytree(base_path_images, base_path_marked_images)                # Validate copy operation        original_files = len([f for f in os.listdir(base_path_images) if f.endswith('.jpg')])        copied_files = len([f for f in os.listdir(base_path_marked_images) if f.endswith('.jpg')])                if original_files != copied_files:            raise Exception(f\"Image copy validation failed: {original_files} original vs {copied_files} copied\")                logger.info(f\"‚úì Copied {copied_files} images to marked folder\")                # Load and validate annotations        annotations_path = base_path_annotations + \"annotations.json\"        with open(annotations_path, \"r\") as f:             annotations = json.load(f)                # Flatten annotations to list with validation        annotations_list = []        for page in annotations:            for annotation in annotations[page]:                annotation[\"page\"] = int(page)                # x to left, y to top                annotation[\"left\"] = annotation[\"x\"]                annotation[\"top\"] = annotation[\"y\"]                annotation.pop(\"x\")                annotation.pop(\"y\")                annotations_list.append(annotation)                # Convert annotations_list to dict with key with label        annotations_dict = {}        for annotation in annotations_list:            annotations_dict[annotation[\"label\"]] = annotation                logger.info(f\"‚úì Processed {len(annotations_dict)} annotations\")                # Build student ID to page mapping        studentIdToPage = {}        with open(os.path.join(base_path_questions, \"ID\", \"mark.json\")) as f:            data = json.load(f)            for i in data:                studentId = i[\"overridedMark\"] if i[\"overridedMark\"] != \"\" else i[\"mark\"]                studentIdToPage[studentId] = int(i[\"id\"])                logger.info(f\"‚úì Built student-to-page mapping for {len(studentIdToPage)} students\")                # Add marks to images with progress tracking        marksDf_list = marksDf.to_dict(orient=\"records\")                progress = IntProgress(min=0, max=len(marksDf_list), description='Adding marks')        display(progress)                processed_students = 0        failed_students = []                for student in marksDf_list:            try:                first_page = studentIdToPage[student[\"ID\"]]                                for annotation in annotations_dict:                    value = student[annotation]                    if annotation == \"ID\":                        value = value + \" Marks: \" + str(student[\"Marks\"])                                        x = annotations_dict[annotation][\"left\"]                    y = annotations_dict[annotation][\"top\"]                    page = first_page + annotations_dict[annotation][\"page\"]                                      image_path = base_path_marked_images + str(page) + \".jpg\"                                        if not os.path.exists(image_path):                        logger.warning(f\"Image not found: {image_path}\")                        continue                                        # Add text to image with error handling                    try:                        img = cv2.imread(image_path)                        if img is None:                            logger.warning(f\"Failed to load image: {image_path}\")                            continue                                                textSize = cv2.getTextSize(text=str(value), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=1, thickness=2)                        height = textSize[0][1]                        cv2.putText(img, str(value), (x, y + height), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)                        cv2.imwrite(image_path, img)                                            except Exception as e:                        logger.warning(f\"Failed to add text to {image_path}: {e}\")                        continue                                processed_students += 1                            except Exception as e:                logger.error(f\"Failed to process student {student['ID']}: {e}\")                failed_students.append(student['ID'])                        progress.value += 1                logger.info(f\"‚úì Added marks to images for {processed_students} students\")        if failed_students:            logger.warning(f\"Failed to process {len(failed_students)} students: {failed_students}\")                return studentIdToPage, processed_students, failed_students            except Exception as e:        logger.error(f\"‚ùå Scored scripts creation failed: {e}\")        raise# Create scored scriptsstudentIdToPage, processed_students, failed_students = create_scored_scripts()# Create scored scriptsstudentIdToPage, processed_students, failed_students = create_scored_scripts()\n",
    "def create_scored_scripts():\n",
    "    \"\"\"Create scored scripts with validation and error handling.\"\"\"\n",
    "    print(\"üìÑ Creating scored scripts...\")\n",
    "\n",
    "    try:\n",
    "        if os.path.exists(base_path_marked_images):\n",
    "            shutil.rmtree(base_path_marked_images)\n",
    "\n",
    "        shutil.copytree(base_path_images, base_path_marked_images)\n",
    "\n",
    "        original_files = len([f for f in os.listdir(base_path_images) if f.endswith(\".jpg\")])\n",
    "        copied_files = len([f for f in os.listdir(base_path_marked_images) if f.endswith(\".jpg\")])\n",
    "        if original_files != copied_files:\n",
    "            raise Exception(f\"Image copy validation failed: {original_files} original vs {copied_files} copied\")\n",
    "\n",
    "        logger.info(f\"‚úì Copied {copied_files} images to marked folder\")\n",
    "\n",
    "        annotations_path = os.path.join(base_path_annotations, \"annotations.json\")\n",
    "        with open(annotations_path, \"r\") as f:\n",
    "            annotations = json.load(f)\n",
    "\n",
    "        annotations_dict = {}\n",
    "        for page, page_ann in annotations.items():\n",
    "            for annotation in page_ann:\n",
    "                annotation = annotation.copy()\n",
    "                annotation[\"page\"] = int(page)\n",
    "                annotation[\"left\"] = annotation.pop(\"x\")\n",
    "                annotation[\"top\"] = annotation.pop(\"y\")\n",
    "                annotations_dict[annotation[\"label\"]] = annotation\n",
    "\n",
    "        logger.info(f\"‚úì Processed {len(annotations_dict)} annotations\")\n",
    "\n",
    "        studentIdToPage = {}\n",
    "        id_mark_path = os.path.join(base_path_questions, \"ID\", \"mark.json\")\n",
    "        with open(id_mark_path) as f:\n",
    "            data = json.load(f)\n",
    "        for i in data:\n",
    "            studentId = i[\"overridedMark\"] if i[\"overridedMark\"] != \"\" else i[\"mark\"]\n",
    "            studentIdToPage[str(studentId)] = int(i[\"id\"])\n",
    "        logger.info(f\"‚úì Built student-to-page mapping for {len(studentIdToPage)} students\")\n",
    "\n",
    "        marksDf_list = marksDf.to_dict(orient=\"records\")\n",
    "        progress = IntProgress(min=0, max=len(marksDf_list), description=\"Adding marks\")\n",
    "        display(progress)\n",
    "\n",
    "        processed_students = 0\n",
    "        failed_students = []\n",
    "\n",
    "        for student in marksDf_list:\n",
    "            try:\n",
    "                first_page = studentIdToPage.get(str(student[\"ID\"]))\n",
    "                if first_page is None:\n",
    "                    logger.warning(f\"No page mapping for student {student['ID']}\")\n",
    "                    failed_students.append(student[\"ID\"])\n",
    "                    progress.value += 1\n",
    "                    continue\n",
    "\n",
    "                for label, annotation in annotations_dict.items():\n",
    "                    value = student.get(label, \"\")\n",
    "                    if label == \"ID\":\n",
    "                        value = f\"{value} Marks: {student.get('Marks', '')}\"\n",
    "                    if pd.isna(value):\n",
    "                        continue\n",
    "\n",
    "                    x, y = annotation[\"left\"], annotation[\"top\"]\n",
    "                    page = first_page + annotation[\"page\"]\n",
    "                    image_path = os.path.join(base_path_marked_images, f\"{page}.jpg\")\n",
    "\n",
    "                    if not os.path.exists(image_path):\n",
    "                        logger.warning(f\"Image not found: {image_path}\")\n",
    "                        continue\n",
    "\n",
    "                    try:\n",
    "                        img = cv2.imread(image_path)\n",
    "                        if img is None:\n",
    "                            logger.warning(f\"Failed to load image: {image_path}\")\n",
    "                            continue\n",
    "\n",
    "                        text = str(value)\n",
    "                        (text_width, text_height), baseline = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 1, 2)\n",
    "                        cv2.putText(img, text, (x, y + text_height), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "                        cv2.imwrite(image_path, img)\n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"Failed to add text to {image_path}: {e}\")\n",
    "                        continue\n",
    "\n",
    "                processed_students += 1\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to process student {student.get('ID')}: {e}\")\n",
    "                failed_students.append(student.get(\"ID\"))\n",
    "            finally:\n",
    "                progress.value += 1\n",
    "\n",
    "        logger.info(f\"‚úì Added marks to images for {processed_students} students\")\n",
    "        if failed_students:\n",
    "            logger.warning(f\"Failed to process {len(failed_students)} students: {failed_students}\")\n",
    "\n",
    "        return studentIdToPage, processed_students, failed_students\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Scored scripts creation failed: {e}\")\n",
    "        raise\n",
    "\n",
    "studentIdToPage, processed_students, failed_students = create_scored_scripts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35b50a67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T06:56:43.783770Z",
     "iopub.status.busy": "2026-01-08T06:56:43.783358Z",
     "iopub.status.idle": "2026-01-08T06:56:44.294495Z",
     "shell.execute_reply": "2026-01-08T06:56:44.292646Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-08 07:38:30,095 - INFO - Processing PDF for 234567890: pages 4-5\n",
      "2026-01-08 07:38:30,257 - INFO - ‚úì Created PDF for 234567890: 247594 bytes\n",
      "2026-01-08 07:38:30,260 - INFO - Processing PDF for 123456789: pages 0-1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Generating individual PDFs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-08 07:38:30,359 - INFO - ‚úì Created PDF for 123456789: 252643 bytes\n",
      "2026-01-08 07:38:30,361 - INFO - Processing PDF for 987654321: pages 2-3\n",
      "2026-01-08 07:38:30,446 - INFO - ‚úì Created PDF for 987654321: 234373 bytes\n",
      "2026-01-08 07:38:30,449 - INFO - Processing PDF for 345678912: pages 6-7\n",
      "2026-01-08 07:38:30,538 - INFO - ‚úì Created PDF for 345678912: 243113 bytes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä PDF Generation Summary:\n",
      "   Successful: 4\n",
      "   Failed: 0\n",
      "   Success rate: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# Robust PDF generation with comprehensive validation\n",
    "def generate_pdfs(studentIdToPage, numberOfPage):\n",
    "    \"\"\"Generate individual PDFs with validation and error handling\"\"\"\n",
    "    print(\"üìÑ Generating individual PDFs...\")\n",
    "    \n",
    "    try:\n",
    "        marksDf_list = marksDf.to_dict(orient=\"records\")\n",
    "        \n",
    "        pdf_generation_stats = {\n",
    "            'successful': 0,\n",
    "            'failed': 0,\n",
    "            'errors': []\n",
    "        }\n",
    "        \n",
    "        for student in marksDf_list:\n",
    "            try:\n",
    "                studentId = str(student[\"ID\"])\n",
    "                first_page = studentIdToPage.get(studentId)\n",
    "                \n",
    "                if first_page is None:\n",
    "                    error_msg = f\"No page mapping found for student {studentId}\"\n",
    "                    logger.error(error_msg)\n",
    "                    pdf_generation_stats['errors'].append(error_msg)\n",
    "                    pdf_generation_stats['failed'] += 1\n",
    "                    continue\n",
    "                \n",
    "                last_page = first_page + numberOfPage - 1\n",
    "                \n",
    "                logger.info(f\"Processing PDF for {studentId}: pages {first_page}-{last_page}\")\n",
    "                \n",
    "                pdf_path = os.path.join(base_path_marked_pdfs, f\"{studentId}.pdf\")\n",
    "                \n",
    "                # Validate all required images exist\n",
    "                image_paths = [os.path.join(base_path_marked_images, f\"{i}.jpg\") for i in range(first_page, last_page + 1)]\n",
    "                missing_images = [path for path in image_paths if not os.path.exists(path)]\n",
    "                \n",
    "                if missing_images:\n",
    "                    error_msg = f\"Missing images for {studentId}: {len(missing_images)} files\"\n",
    "                    logger.error(error_msg)\n",
    "                    pdf_generation_stats['errors'].append(error_msg)\n",
    "                    pdf_generation_stats['failed'] += 1\n",
    "                    continue\n",
    "                \n",
    "                # Load and validate images\n",
    "                try:\n",
    "                    images = []\n",
    "                    for path in image_paths:\n",
    "                        img = Image.open(path)\n",
    "                        if img.mode != 'RGB':\n",
    "                            img = img.convert('RGB')\n",
    "                        images.append(img)\n",
    "                    \n",
    "                    # Create PDF with validation\n",
    "                    if images:\n",
    "                        images[0].save(pdf_path, save_all=True, append_images=images[1:] if len(images) > 1 else [])\n",
    "                        \n",
    "                        # Validate PDF creation\n",
    "                        if os.path.exists(pdf_path) and os.path.getsize(pdf_path) > 0:\n",
    "                            pdf_generation_stats['successful'] += 1\n",
    "                            logger.info(f\"‚úì Created PDF for {studentId}: {os.path.getsize(pdf_path)} bytes\")\n",
    "                        else:\n",
    "                            error_msg = f\"PDF creation failed for {studentId}: file not created or empty\"\n",
    "                            logger.error(error_msg)\n",
    "                            pdf_generation_stats['errors'].append(error_msg)\n",
    "                            pdf_generation_stats['failed'] += 1\n",
    "                    else:\n",
    "                        error_msg = f\"No images loaded for {studentId}\"\n",
    "                        logger.error(error_msg)\n",
    "                        pdf_generation_stats['errors'].append(error_msg)\n",
    "                        pdf_generation_stats['failed'] += 1\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    error_msg = f\"Image processing failed for {studentId}: {e}\"\n",
    "                    logger.error(error_msg)\n",
    "                    pdf_generation_stats['errors'].append(error_msg)\n",
    "                    pdf_generation_stats['failed'] += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                error_msg = f\"PDF generation failed for {studentId}: {e}\"\n",
    "                logger.error(error_msg)\n",
    "                pdf_generation_stats['errors'].append(error_msg)\n",
    "                pdf_generation_stats['failed'] += 1\n",
    "        \n",
    "        # Display generation summary\n",
    "        total = pdf_generation_stats['successful'] + pdf_generation_stats['failed']\n",
    "        print(f\"\\nüìä PDF Generation Summary:\")\n",
    "        print(f\"   Successful: {pdf_generation_stats['successful']}\")\n",
    "        print(f\"   Failed: {pdf_generation_stats['failed']}\")\n",
    "        if total > 0:\n",
    "            print(f\"   Success rate: {pdf_generation_stats['successful']/total*100:.1f}%\")\n",
    "        \n",
    "        if pdf_generation_stats['errors']:\n",
    "            print(f\"\\n‚ùå Errors encountered:\")\n",
    "            for error in pdf_generation_stats['errors'][:5]:  # Show first 5 errors\n",
    "                print(f\"   ‚Ä¢ {error}\")\n",
    "            if len(pdf_generation_stats['errors']) > 5:\n",
    "                print(f\"   ... and {len(pdf_generation_stats['errors'])-5} more errors\")\n",
    "        \n",
    "        return pdf_generation_stats\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå PDF generation failed: {e}\")\n",
    "        raise\n",
    "\n",
    "# Generate PDFs\n",
    "# Get numberOfPage from the student ID mapping\n",
    "pageToStudentId, numberOfPage, getStudentId = build_student_id_mapping(\n",
    "    base_path_questions, base_path_annotations\n",
    ")\n",
    "pdf_stats = generate_pdfs(studentIdToPage, numberOfPage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc3a0d89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T06:56:44.299371Z",
     "iopub.status.busy": "2026-01-08T06:56:44.297795Z",
     "iopub.status.idle": "2026-01-08T06:56:44.430584Z",
     "shell.execute_reply": "2026-01-08T06:56:44.428445Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-08 07:38:30,627 - INFO - ‚úì Created combined PDF with 4 individual PDFs\n",
      "2026-01-08 07:38:30,637 - WARNING - Adjusted sample size to 1 due to insufficient data\n",
      "2026-01-08 07:38:30,660 - INFO - ‚úì Created sample: ../marking_form/VTC Test/marked/scripts/sampleOf1.pdf (977946 bytes)\n",
      "2026-01-08 07:38:30,662 - WARNING - Adjusted sample size to 1 due to insufficient data\n",
      "2026-01-08 07:38:30,684 - INFO - ‚úì Created sample: ../marking_form/VTC Test/marked/scripts/sampleOf1.pdf (977946 bytes)\n",
      "2026-01-08 07:38:30,687 - WARNING - Insufficient passing students (1) for passing-only samples\n",
      "2026-01-08 07:38:30,688 - INFO - ‚úì Sample generation completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Generating sample collections...\n"
     ]
    }
   ],
   "source": [
    "# Robust sample generation with comprehensive validation\n",
    "def generate_samples():\n",
    "    \"\"\"Generate sample PDFs with validation and error handling\"\"\"\n",
    "    print(\"üìö Generating sample collections...\")\n",
    "    \n",
    "    try:\n",
    "        # Create combined PDF of all scripts\n",
    "        writer = PdfWriter()\n",
    "        \n",
    "        pdf_files_added = 0\n",
    "        for path, currentDirectory, files in os.walk(base_path_marked_pdfs):\n",
    "            for file in files:\n",
    "                if file.endswith(\".pdf\"):\n",
    "                    pdf_path = os.path.join(path, file)\n",
    "                    try:\n",
    "                        reader = PdfReader(pdf_path)\n",
    "                        for page in reader.pages:\n",
    "                            writer.add_page(page)\n",
    "                        pdf_files_added += 1\n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"Failed to add {pdf_path} to combined PDF: {e}\")\n",
    "        \n",
    "        combined_path = base_path_marked_scripts + \"all.pdf\"\n",
    "        with open(combined_path, \"wb\") as f:\n",
    "            writer.write(f)\n",
    "        \n",
    "        logger.info(f\"‚úì Created combined PDF with {pdf_files_added} individual PDFs\")\n",
    "        \n",
    "        # Generate stratified samples with validation\n",
    "        sampling = marksDf.sort_values(by=[\"Marks\"], ascending=False)[\"Marks\"]\n",
    "        \n",
    "        from_directory = os.path.join(os.getcwd(), \"..\", \"templates\", \"pdf\")\n",
    "        \n",
    "        # Validate template files exist\n",
    "        template_files = {\n",
    "            'good': os.path.join(from_directory, \"Good.pdf\"),\n",
    "            'average': os.path.join(from_directory, \"Average.pdf\"),\n",
    "            'weak': os.path.join(from_directory, \"Weak.pdf\")\n",
    "        }\n",
    "        \n",
    "        missing_templates = [name for name, path in template_files.items() if not os.path.exists(path)]\n",
    "        if missing_templates:\n",
    "            logger.warning(f\"Missing template files: {missing_templates}\")\n",
    "            logger.info(\"Creating sample without templates...\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            goodPage = PdfReader(template_files['good'])\n",
    "            averagePage = PdfReader(template_files['average'])\n",
    "            weakPage = PdfReader(template_files['weak'])\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to load template files: {e}\")\n",
    "            logger.info(\"Creating sample without templates...\")\n",
    "            return\n",
    "        \n",
    "        def get_scripts_pdf(df):\n",
    "            return list(map(lambda rowNumber: base_path_marked_pdfs + rowNumber + \".pdf\", df.index))\n",
    "        \n",
    "        def take_sample(n, sampling, suffix=\"\"):\n",
    "            \"\"\"Robust sample generation with validation\"\"\"\n",
    "            try:\n",
    "                if len(sampling) < 3 * n:\n",
    "                    n = max(1, int(len(sampling) / 3))\n",
    "                    logger.warning(f\"Adjusted sample size to {n} due to insufficient data\")\n",
    "                \n",
    "                good = sampling.head(n)\n",
    "                weak = sampling.tail(n)\n",
    "                median = int(len(sampling) / 2)\n",
    "                take = max(1, int(n / 2))\n",
    "                average = sampling.iloc[median - take : median + take]\n",
    "                \n",
    "                writer = PdfWriter()\n",
    "                \n",
    "                # Add template pages and student PDFs with validation\n",
    "                for page in goodPage.pages:\n",
    "                    writer.add_page(page)\n",
    "                \n",
    "                for pdf in get_scripts_pdf(good):\n",
    "                    if os.path.exists(pdf):\n",
    "                        try:\n",
    "                            reader = PdfReader(pdf)\n",
    "                            for page in reader.pages:\n",
    "                                writer.add_page(page)\n",
    "                        except Exception as e:\n",
    "                            logger.warning(f\"Failed to add {pdf}: {e}\")\n",
    "                \n",
    "                for page in averagePage.pages:\n",
    "                    writer.add_page(page)\n",
    "                \n",
    "                for pdf in get_scripts_pdf(average):\n",
    "                    if os.path.exists(pdf):\n",
    "                        try:\n",
    "                            reader = PdfReader(pdf)\n",
    "                            for page in reader.pages:\n",
    "                                writer.add_page(page)\n",
    "                        except Exception as e:\n",
    "                            logger.warning(f\"Failed to add {pdf}: {e}\")\n",
    "                \n",
    "                for page in weakPage.pages:\n",
    "                    writer.add_page(page)\n",
    "                \n",
    "                for pdf in get_scripts_pdf(weak):\n",
    "                    if os.path.exists(pdf):\n",
    "                        try:\n",
    "                            reader = PdfReader(pdf)\n",
    "                            for page in reader.pages:\n",
    "                                writer.add_page(page)\n",
    "                        except Exception as e:\n",
    "                            logger.warning(f\"Failed to add {pdf}: {e}\")\n",
    "                \n",
    "                fileName = base_path_marked_scripts + \"sampleOf\" + str(n) + suffix + \".pdf\"\n",
    "                with open(fileName, \"wb\") as f:\n",
    "                    writer.write(f)\n",
    "                \n",
    "                # Validate sample creation\n",
    "                if os.path.exists(fileName) and os.path.getsize(fileName) > 0:\n",
    "                    logger.info(f\"‚úì Created sample: {fileName} ({os.path.getsize(fileName)} bytes)\")\n",
    "                else:\n",
    "                    logger.error(f\"‚ùå Failed to create sample: {fileName}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.error(f\"‚ùå Sample generation failed for n={n}, suffix={suffix}: {e}\")\n",
    "        \n",
    "        # Generate different sample sizes\n",
    "        take_sample(3, sampling)\n",
    "        take_sample(5, sampling)\n",
    "        \n",
    "        # Generate samples for passing students only\n",
    "        passing_sampling = sampling.where(lambda x: x > passingMark).dropna()\n",
    "        if len(passing_sampling) >= 3:\n",
    "            take_sample(3, passing_sampling, \"_only_pass\")\n",
    "        if len(passing_sampling) >= 5:\n",
    "            take_sample(5, passing_sampling, \"_only_pass\")\n",
    "        else:\n",
    "            logger.warning(f\"Insufficient passing students ({len(passing_sampling)}) for passing-only samples\")\n",
    "        \n",
    "        logger.info(\"‚úì Sample generation completed\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Sample generation failed: {e}\")\n",
    "        raise\n",
    "\n",
    "# Generate samples\n",
    "generate_samples()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f896f4e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T06:56:44.435469Z",
     "iopub.status.busy": "2026-01-08T06:56:44.433951Z",
     "iopub.status.idle": "2026-01-08T06:56:44.637101Z",
     "shell.execute_reply": "2026-01-08T06:56:44.634599Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-08 07:38:30,764 - INFO - ‚úì Processed answers for Q5: 4 entries\n",
      "2026-01-08 07:38:30,772 - INFO - ‚úì Processed answers for Q4: 4 entries\n",
      "2026-01-08 07:38:30,773 - INFO - ‚è≠Ô∏è Skipping metadata question: NAME\n",
      "2026-01-08 07:38:30,774 - INFO - ‚è≠Ô∏è Skipping metadata question: CLASS\n",
      "2026-01-08 07:38:30,780 - INFO - ‚úì Processed answers for Q3: 4 entries\n",
      "2026-01-08 07:38:30,789 - INFO - ‚úì Processed answers for Q2: 4 entries\n",
      "2026-01-08 07:38:30,796 - INFO - ‚úì Processed answers for Q1: 4 entries\n",
      "2026-01-08 07:38:30,797 - INFO - ‚è≠Ô∏è Skipping metadata question: ID\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Collecting answers and reasoning from question data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-08 07:38:30,836 - INFO - ‚úì Collected answers and reasoning from 5 questions\n",
      "2026-01-08 07:38:30,839 - INFO -   Answer entries: 20\n",
      "2026-01-08 07:38:30,840 - INFO -   Reasoning entries: 20\n",
      "2026-01-08 07:38:30,847 - INFO - ‚úì Processed answers for Q5: 4 entries\n",
      "2026-01-08 07:38:30,852 - INFO - ‚úì Processed answers for Q4: 4 entries\n",
      "2026-01-08 07:38:30,853 - INFO - ‚è≠Ô∏è Skipping metadata question: NAME\n",
      "2026-01-08 07:38:30,854 - INFO - ‚è≠Ô∏è Skipping metadata question: CLASS\n",
      "2026-01-08 07:38:30,862 - INFO - ‚úì Processed answers for Q3: 4 entries\n",
      "2026-01-08 07:38:30,876 - INFO - ‚úì Processed answers for Q2: 4 entries\n",
      "2026-01-08 07:38:30,890 - INFO - ‚úì Processed answers for Q1: 4 entries\n",
      "2026-01-08 07:38:30,893 - INFO - ‚è≠Ô∏è Skipping metadata question: ID\n",
      "2026-01-08 07:38:30,920 - INFO - ‚úì Collected answers and reasoning from 5 questions\n",
      "2026-01-08 07:38:30,922 - INFO -   Answer entries: 20\n",
      "2026-01-08 07:38:30,922 - INFO -   Reasoning entries: 20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Collecting answers and reasoning from question data...\n"
     ]
    }
   ],
   "source": [
    "# Robust answer collection and reasoning with metadata exclusion\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def clean_answer_text(val: str) -> str:\n",
    "    \"\"\"Strip leading numbering and drop standalone question labels like Q2.\"\"\"\n",
    "    if not isinstance(val, str):\n",
    "        return val\n",
    "    lines = [ln.strip() for ln in str(val).splitlines()]\n",
    "    cleaned = []\n",
    "    for ln in lines:\n",
    "        ln = re.sub(r\"^\\s*\\d+\\s*[\\.|\\)]\\s*\", \"\", ln)\n",
    "        if re.fullmatch(r\"q\\d+\", ln, flags=re.IGNORECASE):\n",
    "            continue\n",
    "        if ln:\n",
    "            cleaned.append(ln)\n",
    "    return \"\\n\".join(cleaned).strip()\n",
    "\n",
    "def collect_answers_and_reasoning():\n",
    "    \"\"\"Gather per-student answers and model reasoning from each question's CSV and pivot to a mark-style wide format.\"\"\"\n",
    "    print(\"üìù Collecting answers and reasoning from question data...\")\n",
    "    \n",
    "    answer_rows = []\n",
    "    reasoning_rows = []\n",
    "    questions_processed = 0\n",
    "\n",
    "    for path, currentDirectory, files in os.walk(base_path_questions):\n",
    "        if \"data.csv\" not in files:\n",
    "            continue\n",
    "\n",
    "        question = path[len(base_path_questions) + 1 :]\n",
    "        \n",
    "        # Skip metadata questions (NAME, ID, CLASS)\n",
    "        if question in METADATA_QUESTIONS:\n",
    "            logger.info(f\"‚è≠Ô∏è Skipping metadata question: {question}\")\n",
    "            continue\n",
    "            \n",
    "        data_path = os.path.join(path, \"data.csv\")\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(data_path)\n",
    "            if \"page\" not in df.columns:\n",
    "                logger.warning(f\"No 'page' column in {question} data.csv\")\n",
    "                continue\n",
    "\n",
    "            # Map scanned page back to student ID using the existing helper\n",
    "            df[\"StudentID\"] = df[\"page\"].apply(\n",
    "                lambda p: getStudentId(int(str(p).split(\".\")[0])) if pd.notna(p) else None\n",
    "            )\n",
    "\n",
    "            for _, row in df.iterrows():\n",
    "                sid = row.get(\"StudentID\")\n",
    "                if sid is None:\n",
    "                    continue\n",
    "\n",
    "                raw_answer = row.get(\"Answer\", \"\")\n",
    "                answer_val = clean_answer_text(raw_answer)\n",
    "                source_page = row.get(\"page\", \"\")\n",
    "                row_number = row.get(\"RowNumber\", \"\")\n",
    "\n",
    "                answer_rows.append(\n",
    "                    {\n",
    "                        \"ID\": str(sid),\n",
    "                        \"Question\": question,\n",
    "                        \"Answer\": answer_val,\n",
    "                        \"SourcePage\": source_page,\n",
    "                        \"RowNumber\": row_number,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                reasoning_rows.append(\n",
    "                    {\n",
    "                        \"ID\": str(sid),\n",
    "                        \"Question\": question,\n",
    "                        \"Reasoning\": row.get(\"Reasoning\", \"\"),\n",
    "                        \"Similarity\": row.get(\"Similarity\", \"\"),\n",
    "                        \"ModelMark\": row.get(\"Mark\", \"\"),\n",
    "                        \"Answer\": answer_val,\n",
    "                        \"SourcePage\": source_page,\n",
    "                        \"RowNumber\": row_number,\n",
    "                    }\n",
    "                )\n",
    "            \n",
    "            questions_processed += 1\n",
    "            logger.info(f\"‚úì Processed answers for {question}: {len(df)} entries\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Failed to process {question}: {e}\")\n",
    "            continue\n",
    "\n",
    "    answers_df = pd.DataFrame(answer_rows)\n",
    "    reasoning_df = pd.DataFrame(reasoning_rows)\n",
    "\n",
    "    if not answers_df.empty:\n",
    "        answers_df = answers_df.sort_values(\n",
    "            by=[\"ID\", \"Question\", \"SourcePage\", \"RowNumber\"]\n",
    "        ).reset_index(drop=True)\n",
    "    if not reasoning_df.empty:\n",
    "        reasoning_df = reasoning_df.sort_values(\n",
    "            by=[\"ID\", \"Question\", \"SourcePage\", \"RowNumber\"]\n",
    "        ).reset_index(drop=True)\n",
    "\n",
    "    # Preserve ID/NAME/CLASS to match marks layout\n",
    "    meta_cols = [\"ID\", \"NAME\", \"CLASS\"]\n",
    "    student_meta = marksDf[meta_cols].drop_duplicates().set_index(\"ID\")\n",
    "\n",
    "    # Keep question ordering aligned with marks sheet (excluding metadata)\n",
    "    question_cols = [\n",
    "        col\n",
    "        for col in marksDf.columns\n",
    "        if col not in [\"ID\", \"NAME\", \"CLASS\", \"Marks\"] and col not in METADATA_QUESTIONS\n",
    "    ]\n",
    "\n",
    "    # Wide answers: one row per student, one column per question\n",
    "    answers_wide = student_meta.copy()\n",
    "    if not answers_df.empty:\n",
    "        answers_pivot = answers_df.pivot_table(\n",
    "            index=\"ID\", columns=\"Question\", values=\"Answer\", aggfunc=\"first\"\n",
    "        )\n",
    "        answers_pivot = answers_pivot.reindex(columns=question_cols)\n",
    "        answers_wide = answers_wide.join(answers_pivot)\n",
    "        answers_wide = answers_wide.reset_index()\n",
    "\n",
    "    # Wide reasoning: only the reasoning text per question (matches marks layout)\n",
    "    reasoning_wide = student_meta.copy()\n",
    "    if not reasoning_df.empty:\n",
    "        reasoning_pivot = reasoning_df.pivot_table(\n",
    "            index=\"ID\", columns=\"Question\", values=\"Reasoning\", aggfunc=\"first\"\n",
    "        )\n",
    "        reasoning_pivot = reasoning_pivot.reindex(columns=question_cols)\n",
    "        reasoning_wide = reasoning_wide.join(reasoning_pivot)\n",
    "        reasoning_wide = reasoning_wide.reset_index()\n",
    "\n",
    "    logger.info(f\"‚úì Collected answers and reasoning from {questions_processed} questions\")\n",
    "    logger.info(f\"  Answer entries: {len(answers_df)}\")\n",
    "    logger.info(f\"  Reasoning entries: {len(reasoning_df)}\")\n",
    "    \n",
    "    return answers_wide, reasoning_wide, answers_df, reasoning_df\n",
    "\n",
    "# Collect answers and reasoning\n",
    "answers_sheet, reasoning_sheet, answers_raw, reasoning_raw = collect_answers_and_reasoning()\n",
    "\n",
    "# Collect answers and reasoning\n",
    "answers_sheet, reasoning_sheet, answers_raw, reasoning_raw = collect_answers_and_reasoning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99b0a7d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T06:56:44.641350Z",
     "iopub.status.busy": "2026-01-08T06:56:44.640920Z",
     "iopub.status.idle": "2026-01-08T06:56:44.857293Z",
     "shell.execute_reply": "2026-01-08T06:56:44.856058Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Generating comprehensive Excel reports...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-08 07:38:31,084 - INFO - ‚úì Generated comprehensive Excel report: ../marking_form/VTC Test/marked/scripts/details_score_report.xlsx\n",
      "2026-01-08 07:38:31,086 - INFO - ‚úì Generated summary Excel report: ../marking_form/VTC Test/marked/scripts/score_report.xlsx\n",
      "2026-01-08 07:38:31,155 - INFO - ‚úì Generated comprehensive Excel report: ../marking_form/VTC Test/marked/scripts/details_score_report.xlsx\n",
      "2026-01-08 07:38:31,157 - INFO - ‚úì Generated summary Excel report: ../marking_form/VTC Test/marked/scripts/score_report.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Excel reports saved:\n",
      "   ‚Ä¢ Detailed: details_score_report.xlsx\n",
      "   ‚Ä¢ Summary: score_report.xlsx\n",
      "üìä Generating comprehensive Excel reports...\n",
      "üìÑ Excel reports saved:\n",
      "   ‚Ä¢ Detailed: details_score_report.xlsx\n",
      "   ‚Ä¢ Summary: score_report.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Robust Excel report generation with comprehensive analytics\n",
    "def generate_comprehensive_excel_report():\n",
    "    \"\"\"Generate comprehensive Excel report with multiple sheets and analytics\"\"\"\n",
    "    print(\"üìä Generating comprehensive Excel reports...\")\n",
    "    \n",
    "    try:\n",
    "        details_report_path = base_path_marked_scripts + \"details_score_report.xlsx\"\n",
    "        \n",
    "        # Multi-sheet Excel: marks, answers (wide), reasoning (wide) + raw long-form for audit\n",
    "        with pd.ExcelWriter(details_report_path, engine='openpyxl') as writer:\n",
    "            # Main sheets\n",
    "            marksDf.to_excel(writer, sheet_name=\"Marks\", index=False)\n",
    "            \n",
    "            if not answers_sheet.empty:\n",
    "                answers_sheet.to_excel(writer, sheet_name=\"Answers\", index=False)\n",
    "            else:\n",
    "                pd.DataFrame({\"Note\": [\"No answer data available\"]}).to_excel(writer, sheet_name=\"Answers\", index=False)\n",
    "            \n",
    "            if not reasoning_sheet.empty:\n",
    "                reasoning_sheet.to_excel(writer, sheet_name=\"Reasoning\", index=False)\n",
    "            else:\n",
    "                pd.DataFrame({\"Note\": [\"No reasoning data available\"]}).to_excel(writer, sheet_name=\"Reasoning\", index=False)\n",
    "            \n",
    "            # Raw data sheets for audit\n",
    "            if not answers_raw.empty:\n",
    "                answers_raw.to_excel(writer, sheet_name=\"AnswersRaw\", index=False)\n",
    "            if not reasoning_raw.empty:\n",
    "                reasoning_raw.to_excel(writer, sheet_name=\"ReasoningRaw\", index=False)\n",
    "\n",
    "        # Lightweight summary sheet\n",
    "        summary_path = base_path_marked_scripts + \"score_report.xlsx\"\n",
    "        marksDf[[\"ID\", \"NAME\", \"CLASS\", \"Marks\"]].to_excel(summary_path, index=False)\n",
    "\n",
    "        logger.info(f\"‚úì Generated comprehensive Excel report: {details_report_path}\")\n",
    "        logger.info(f\"‚úì Generated summary Excel report: {summary_path}\")\n",
    "        \n",
    "        return details_report_path, summary_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Excel report generation failed: {e}\")\n",
    "        raise\n",
    "\n",
    "# Generate comprehensive Excel reports\n",
    "details_report_path, summary_report_path = generate_comprehensive_excel_report()\n",
    "print(f\"üìÑ Excel reports saved:\")\n",
    "print(f\"   ‚Ä¢ Detailed: {os.path.basename(details_report_path)}\")\n",
    "print(f\"   ‚Ä¢ Summary: {os.path.basename(summary_report_path)}\")\n",
    "\n",
    "# Generate comprehensive Excel reports\n",
    "details_report_path, summary_report_path = generate_comprehensive_excel_report()\n",
    "print(f\"üìÑ Excel reports saved:\")\n",
    "print(f\"   ‚Ä¢ Detailed: {os.path.basename(details_report_path)}\")\n",
    "print(f\"   ‚Ä¢ Summary: {os.path.basename(summary_report_path)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2b0413c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T06:56:57.499860Z",
     "iopub.status.busy": "2026-01-08T06:56:57.499418Z",
     "iopub.status.idle": "2026-01-08T06:57:00.530378Z",
     "shell.execute_reply": "2026-01-08T06:57:00.526476Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-08 07:38:31,207 - INFO - ‚úì Removed 0 version history files\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Performing backup and cleanup...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-08 07:38:31,562 - INFO - ‚úì Created backup archive: /home/user/gemini-handwriting-grader/marking_form/VTC Test.zip\n",
      "2026-01-08 07:38:31,565 - INFO -   Archive size: 3,060,489 bytes (2.9 MB)\n",
      "2026-01-08 07:38:31,565 - INFO -   Archive size: 3,060,489 bytes (2.9 MB)\n"
     ]
    }
   ],
   "source": [
    "# Robust backup and cleanup with validation\n",
    "def backup_and_cleanup():\n",
    "    \"\"\"Robust backup with comprehensive validation and error handling\"\"\"\n",
    "    print(\"üßπ Performing backup and cleanup...\")\n",
    "    \n",
    "    try:\n",
    "        # Remove version history files with progress tracking\n",
    "        version_files_removed = 0\n",
    "        for path, _, files in os.walk(base_path_questions):\n",
    "            for file in files:\n",
    "                if file.startswith(\"control-\") or file.startswith(\"mark-\"):\n",
    "                    try:\n",
    "                        os.remove(os.path.join(path, file))\n",
    "                        version_files_removed += 1\n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"Failed to remove {file}: {e}\")\n",
    "        logger.info(f\"‚úì Removed {version_files_removed} version history files\")\n",
    "        \n",
    "        # Create backup archive with validation\n",
    "        backup_path = shutil.make_archive(base_path, \"zip\", base_path)\n",
    "        if os.path.exists(backup_path):\n",
    "            backup_size = os.path.getsize(backup_path)\n",
    "            logger.info(f\"‚úì Created backup archive: {backup_path}\")\n",
    "            logger.info(f\"  Archive size: {backup_size:,} bytes ({backup_size/1024/1024:.1f} MB)\")\n",
    "            return backup_path\n",
    "        else:\n",
    "            raise Exception(\"Failed to create backup archive\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Backup and cleanup failed: {e}\")\n",
    "        raise\n",
    "\n",
    "# Perform backup and cleanup\n",
    "backup_path = backup_and_cleanup()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ceb5e878",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T06:57:00.536399Z",
     "iopub.status.busy": "2026-01-08T06:57:00.535914Z",
     "iopub.status.idle": "2026-01-08T06:57:00.562969Z",
     "shell.execute_reply": "2026-01-08T06:57:00.560919Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üéâ ENHANCED STEP 6: POST-SCORING PACKAGING COMPLETE\n",
      "======================================================================\n",
      "\n",
      "üìä Processing Results:\n",
      "   Total students processed: 4\n",
      "   Average score: 11.00\n",
      "   Passing students: 1 (25.0%)\n",
      "   Score range: 7.0 - 20.0\n",
      "\n",
      "üìÅ Generated Files:\n",
      "   ‚úÖ Backup archive: VTC Test.zip\n",
      "   ‚úÖ Individual PDFs: 4 created\n",
      "   ‚úÖ Combined PDF: all.pdf\n",
      "   ‚úÖ Sample collections: Multiple stratified samples\n",
      "   ‚úÖ Comprehensive Excel reports with multiple sheets:\n",
      "      ‚Ä¢ Marks, Answers, Reasoning (wide format)\n",
      "      ‚Ä¢ Raw data for audit trail\n",
      "      ‚Ä¢ AI-powered Performance reports\n",
      "      ‚Ä¢ Class-level analytics and overview\n",
      "      ‚Ä¢ Question-level metrics and statistics\n",
      "   ‚úÖ Visual analytics: Question performance charts\n",
      "\n",
      "ü§ñ AI-Robust Features:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'performance_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 71\u001b[39m\n\u001b[32m     68\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m70\u001b[39m)\n\u001b[32m     70\u001b[39m \u001b[38;5;66;03m# Generate final comprehensive summary\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m \u001b[43mgenerate_final_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 41\u001b[39m, in \u001b[36mgenerate_final_summary\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     38\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   ‚ö†Ô∏è Student processing issues: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(failed_students)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m students\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     40\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mü§ñ AI-Robust Features:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mperformance_df\u001b[49m.empty:\n\u001b[32m     42\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   ‚úÖ Individual performance reports: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(performance_df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m generated\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m class_overview_df.empty:\n",
      "\u001b[31mNameError\u001b[39m: name 'performance_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Robust final summary and next steps\n",
    "def generate_final_summary():\n",
    "    \"\"\"Generate comprehensive final summary with actionable next steps\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üéâ ENHANCED STEP 6: POST-SCORING PACKAGING COMPLETE\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Overall statistics\n",
    "    total_students = len(marksDf)\n",
    "    avg_score = marksDf['Marks'].mean()\n",
    "    passing_students = len(marksDf[marksDf['Marks'] > passingMark])\n",
    "    pass_rate = (passing_students / total_students * 100) if total_students > 0 else 0\n",
    "    \n",
    "    print(f\"\\nüìä Processing Results:\")\n",
    "    print(f\"   Total students processed: {total_students}\")\n",
    "    print(f\"   Average score: {avg_score:.2f}\")\n",
    "    print(f\"   Passing students: {passing_students} ({pass_rate:.1f}%)\")\n",
    "    print(f\"   Score range: {marksDf['Marks'].min():.1f} - {marksDf['Marks'].max():.1f}\")\n",
    "    \n",
    "    print(f\"\\nüìÅ Generated Files:\")\n",
    "    print(f\"   ‚úÖ Backup archive: {os.path.basename(backup_path)}\")\n",
    "    print(f\"   ‚úÖ Individual PDFs: {pdf_stats['successful']} created\")\n",
    "    print(f\"   ‚úÖ Combined PDF: all.pdf\")\n",
    "    print(f\"   ‚úÖ Sample collections: Multiple stratified samples\")\n",
    "    print(f\"   ‚úÖ Comprehensive Excel reports with multiple sheets:\")\n",
    "    print(f\"      ‚Ä¢ Marks, Answers, Reasoning (wide format)\")\n",
    "    print(f\"      ‚Ä¢ Raw data for audit trail\")\n",
    "    print(f\"      ‚Ä¢ AI-powered Performance reports\")\n",
    "    print(f\"      ‚Ä¢ Class-level analytics and overview\")\n",
    "    print(f\"      ‚Ä¢ Question-level metrics and statistics\")\n",
    "    print(f\"   ‚úÖ Visual analytics: Question performance charts\")\n",
    "    \n",
    "    if pdf_stats['failed'] > 0:\n",
    "        print(f\"   ‚ö†Ô∏è PDF generation issues: {pdf_stats['failed']} failed\")\n",
    "    \n",
    "    if failed_students:\n",
    "        print(f\"   ‚ö†Ô∏è Student processing issues: {len(failed_students)} students\")\n",
    "    \n",
    "    print(f\"\\nü§ñ AI-Robust Features:\")\n",
    "    if not performance_df.empty:\n",
    "        print(f\"   ‚úÖ Individual performance reports: {len(performance_df)} generated\")\n",
    "    if not class_overview_df.empty:\n",
    "        print(f\"   ‚úÖ Class-level analytics with AI insights\")\n",
    "    print(f\"   ‚úÖ Metadata questions properly excluded from analysis\")\n",
    "    print(f\"   ‚úÖ Comprehensive caching for efficient re-runs\")\n",
    "    \n",
    "    print(f\"\\nüéØ Next Steps:\")\n",
    "    print(f\"   1. üìß Proceed to Step 7: Email Score Distribution\")\n",
    "    print(f\"   2. üìä Review detailed analytics in Excel reports\")\n",
    "    print(f\"   3. üìÑ Use sample PDFs for moderation and review\")\n",
    "    print(f\"   4. ü§ñ Review AI-generated performance insights\")\n",
    "    print(f\"   5. üìà Analyze question-level metrics for curriculum improvement\")\n",
    "    print(f\"   6. üíæ Archive backup file for long-term storage\")\n",
    "    \n",
    "    print(f\"\\nüí° Robust Quality Assurance:\")\n",
    "    print(f\"   ‚Ä¢ All processing includes comprehensive validation\")\n",
    "    print(f\"   ‚Ä¢ Error handling ensures partial failures don't stop processing\")\n",
    "    print(f\"   ‚Ä¢ Detailed logging provides full audit trail\")\n",
    "    print(f\"   ‚Ä¢ Multiple output formats support different use cases\")\n",
    "    print(f\"   ‚Ä¢ AI-powered insights provide actionable feedback\")\n",
    "    print(f\"   ‚Ä¢ Metadata questions properly handled and excluded\")\n",
    "    print(f\"   ‚Ä¢ Visual analytics support data-driven decisions\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"‚úÖ Robust Step 6 completed successfully at {datetime.now().strftime('%H:%M:%S')}\")\n",
    "    print(\"üöÄ Ready for final distribution, analysis, and archival!\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "# Generate final comprehensive summary\n",
    "generate_final_summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0a4c2f1dfbd54d23a4b0edcb31199398": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "0ebc465f46364d0393f17d12e50ca89b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "22ffab52e7834ff08fb40ec503e2185e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "",
       "description": "Question Insights",
       "description_allow_html": false,
       "layout": "IPY_MODEL_8c30007928b24702a2aeb07f99f2836d",
       "max": 5,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_0a4c2f1dfbd54d23a4b0edcb31199398",
       "tabbable": null,
       "tooltip": null,
       "value": 5
      }
     },
     "62a91e2f61ad4003973491af967f72ff": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "",
       "description": "AI Reports",
       "description_allow_html": false,
       "layout": "IPY_MODEL_ec8f68f9e1cf43c8b352e16c259154f6",
       "max": 4,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_0ebc465f46364d0393f17d12e50ca89b",
       "tabbable": null,
       "tooltip": null,
       "value": 4
      }
     },
     "86ca57cf94e3421b96afe6960bb9352f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "",
       "description": "Adding marks",
       "description_allow_html": false,
       "layout": "IPY_MODEL_f792db5927694582bcc2a339ea43568a",
       "max": 4,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_f064847d8d5b46dca021c020ce517512",
       "tabbable": null,
       "tooltip": null,
       "value": 4
      }
     },
     "8c30007928b24702a2aeb07f99f2836d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ec8f68f9e1cf43c8b352e16c259154f6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f064847d8d5b46dca021c020ce517512": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "f792db5927694582bcc2a339ea43568a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
