{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d61b374",
   "metadata": {},
   "source": [
    "# Step 2: Extract Marking Scheme to Excel\n",
    "\n",
    "Use Gemini to parse the Word marking scheme into structured Excel sheets with error handling, validation, and progress tracking.\n",
    "\n",
    "**Features:**\n",
    "- ‚úÖ Comprehensive error handling and validation\n",
    "- ‚úÖ Robust progress tracking and logging\n",
    "- ‚úÖ Improved markdown formatting and structure\n",
    "- ‚úÖ Robust file handling and backup\n",
    "- ‚úÖ Detailed validation and quality checks\n",
    "- ‚úÖ Professional output formatting\n",
    "\n",
    "Configure the exam `prefix` and dataset folder in the next cell before running.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f987fbd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bad pipe message: %s [b' 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Sa']\n",
      "Bad pipe message: %s [b'ri/537.36\\r\\nAccept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/', b'ng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\\r\\nAccept-Encoding: gzip, deflate, br, zstd\\r\\nA']\n",
      "Bad pipe message: %s [b'ept-Language: en-US,en;q=0.9,zh-TW;q=0.8,zh;q=0.7\\r\\nPriority: u=0, i\\r\\nReferer: https://studio.fireb', b'e.google.com/\\r\\nSec-Ch-Ua: \"Google Chrome\";v=\"143\", \"Chromium\";v=\"143\", \"Not A(Brand\";v=\"24\"\\r\\nSec', b'h-Ua-Arch: \"x86\"\\r\\nSec-Ch-Ua-Bitness: \"64\"\\r\\nS', b'-Ch-Ua-Form-Factors: \"Desktop\"\\r\\nSec-Ch-Ua-Full-Version: \"143.0.7499.170\"\\r\\nSec-Ch-Ua-Full-Version-Lis', b' \"Google Chrome\";v=\"143.0.7499.170\", \"Chromium\";v=\"143.0.7499.170\", \"Not A(Brand\";v=\"24.0.0.0\"\\r\\nSec-Ch-Ua-Mobile: ?']\n",
      "Bad pipe message: %s [b'\\nSec-Ch-Ua-Model: \"\"\\r\\nSec-Ch-Ua-Platform: \"Wind', b's\"\\r\\nSec-Ch-Ua-Platform-Version: \"19.0.0\"\\r\\nSec-Ch-Ua-Wow64: ?0\\r\\nSec-Fetch-Dest: iframe\\r\\nSec-Fetch-Mode: navigat', b'\\nSec-Fetch-Site: cross-site\\r\\nSec-Fetch-Storage-Access: active\\r\\nSec-Fetch-User: ?1\\r\\nSec-User-Ip: 10.2', b'20.203\\r\\nUpgrade-Insecure-Requests: 1\\r\\nX-Forward']\n",
      "Bad pipe message: %s [b'-Host: 32881-firebase-gemini-ai-grader-1767589248216.cluster-ejd22kqny5htuv5dfowoyipt52.cloudworksta']\n",
      "ERROR:tornado.general:Uncaught exception in ZMQStream callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user/gemini-handwriting-grader/.venv/lib/python3.12/site-packages/zmq/eventloop/zmqstream.py\", line 565, in _log_error\n",
      "    f.result()\n",
      "  File \"/home/user/gemini-handwriting-grader/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 341, in dispatch_control\n",
      "    await self.process_control(msg)\n",
      "  File \"/home/user/gemini-handwriting-grader/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 347, in process_control\n",
      "    idents, msg = self.session.feed_identities(msg, copy=False)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/gemini-handwriting-grader/.venv/lib/python3.12/site-packages/jupyter_client/session.py\", line 993, in feed_identities\n",
      "    raise ValueError(msg)\n",
      "ValueError: DELIM not in msg_list\n",
      "ERROR:tornado.general:Uncaught exception in ZMQStream callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user/gemini-handwriting-grader/.venv/lib/python3.12/site-packages/zmq/eventloop/zmqstream.py\", line 565, in _log_error\n",
      "    f.result()\n",
      "  File \"/home/user/gemini-handwriting-grader/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 577, in shell_channel_thread_main\n",
      "    _, msg2 = self.session.feed_identities(msg, copy=False)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/gemini-handwriting-grader/.venv/lib/python3.12/site-packages/jupyter_client/session.py\", line 993, in feed_identities\n",
      "    raise ValueError(msg)\n",
      "ValueError: DELIM not in msg_list\n",
      "ERROR:tornado.general:Uncaught exception in ZMQStream callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user/gemini-handwriting-grader/.venv/lib/python3.12/site-packages/zmq/eventloop/zmqstream.py\", line 565, in _log_error\n",
      "    f.result()\n",
      "  File \"/home/user/gemini-handwriting-grader/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 577, in shell_channel_thread_main\n",
      "    _, msg2 = self.session.feed_identities(msg, copy=False)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/gemini-handwriting-grader/.venv/lib/python3.12/site-packages/jupyter_client/session.py\", line 993, in feed_identities\n",
      "    raise ValueError(msg)\n",
      "ValueError: DELIM not in msg_list\n",
      "ERROR:tornado.general:Uncaught exception in ZMQStream callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user/gemini-handwriting-grader/.venv/lib/python3.12/site-packages/zmq/eventloop/zmqstream.py\", line 565, in _log_error\n",
      "    f.result()\n",
      "  File \"/home/user/gemini-handwriting-grader/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 577, in shell_channel_thread_main\n",
      "    _, msg2 = self.session.feed_identities(msg, copy=False)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/gemini-handwriting-grader/.venv/lib/python3.12/site-packages/jupyter_client/session.py\", line 993, in feed_identities\n",
      "    raise ValueError(msg)\n",
      "ValueError: DELIM not in msg_list\n",
      "ERROR:tornado.general:Uncaught exception in ZMQStream callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user/gemini-handwriting-grader/.venv/lib/python3.12/site-packages/zmq/eventloop/zmqstream.py\", line 565, in _log_error\n",
      "    f.result()\n",
      "  File \"/home/user/gemini-handwriting-grader/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 577, in shell_channel_thread_main\n",
      "    _, msg2 = self.session.feed_identities(msg, copy=False)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/gemini-handwriting-grader/.venv/lib/python3.12/site-packages/jupyter_client/session.py\", line 993, in feed_identities\n",
      "    raise ValueError(msg)\n",
      "ValueError: DELIM not in msg_list\n",
      "ERROR:tornado.general:Uncaught exception in ZMQStream callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user/gemini-handwriting-grader/.venv/lib/python3.12/site-packages/zmq/eventloop/zmqstream.py\", line 565, in _log_error\n",
      "    f.result()\n",
      "  File \"/home/user/gemini-handwriting-grader/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 341, in dispatch_control\n",
      "    await self.process_control(msg)\n",
      "  File \"/home/user/gemini-handwriting-grader/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 347, in process_control\n",
      "    idents, msg = self.session.feed_identities(msg, copy=False)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/gemini-handwriting-grader/.venv/lib/python3.12/site-packages/jupyter_client/session.py\", line 993, in feed_identities\n",
      "    raise ValueError(msg)\n",
      "ValueError: DELIM not in msg_list\n",
      "ERROR:tornado.general:Uncaught exception in ZMQStream callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user/gemini-handwriting-grader/.venv/lib/python3.12/site-packages/zmq/eventloop/zmqstream.py\", line 565, in _log_error\n",
      "    f.result()\n",
      "  File \"/home/user/gemini-handwriting-grader/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 341, in dispatch_control\n",
      "    await self.process_control(msg)\n",
      "  File \"/home/user/gemini-handwriting-grader/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 347, in process_control\n",
      "    idents, msg = self.session.feed_identities(msg, copy=False)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/gemini-handwriting-grader/.venv/lib/python3.12/site-packages/jupyter_client/session.py\", line 993, in feed_identities\n",
      "    raise ValueError(msg)\n",
      "ValueError: DELIM not in msg_list\n",
      "ERROR:tornado.general:Uncaught exception in ZMQStream callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user/gemini-handwriting-grader/.venv/lib/python3.12/site-packages/zmq/eventloop/zmqstream.py\", line 565, in _log_error\n",
      "    f.result()\n",
      "  File \"/home/user/gemini-handwriting-grader/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 341, in dispatch_control\n",
      "    await self.process_control(msg)\n",
      "  File \"/home/user/gemini-handwriting-grader/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 347, in process_control\n",
      "    idents, msg = self.session.feed_identities(msg, copy=False)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/gemini-handwriting-grader/.venv/lib/python3.12/site-packages/jupyter_client/session.py\", line 993, in feed_identities\n",
      "    raise ValueError(msg)\n",
      "ValueError: DELIM not in msg_list\n",
      "ERROR:tornado.general:Uncaught exception in ZMQStream callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user/gemini-handwriting-grader/.venv/lib/python3.12/site-packages/zmq/eventloop/zmqstream.py\", line 565, in _log_error\n",
      "    f.result()\n",
      "  File \"/home/user/gemini-handwriting-grader/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 341, in dispatch_control\n",
      "    await self.process_control(msg)\n",
      "  File \"/home/user/gemini-handwriting-grader/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 347, in process_control\n",
      "    idents, msg = self.session.feed_identities(msg, copy=False)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/gemini-handwriting-grader/.venv/lib/python3.12/site-packages/jupyter_client/session.py\", line 993, in feed_identities\n",
      "    raise ValueError(msg)\n",
      "ValueError: DELIM not in msg_list\n",
      "ERROR:tornado.general:Uncaught exception in ZMQStream callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user/gemini-handwriting-grader/.venv/lib/python3.12/site-packages/zmq/eventloop/zmqstream.py\", line 565, in _log_error\n",
      "    f.result()\n",
      "  File \"/home/user/gemini-handwriting-grader/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 577, in shell_channel_thread_main\n",
      "    _, msg2 = self.session.feed_identities(msg, copy=False)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/gemini-handwriting-grader/.venv/lib/python3.12/site-packages/jupyter_client/session.py\", line 993, in feed_identities\n",
      "    raise ValueError(msg)\n",
      "ValueError: DELIM not in msg_list\n",
      "2026-01-06 06:19:21,937 - INFO - Mapped GOOGLE_GENAI_API_KEY to GOOGLE_API_KEY for ADK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Vertex AI Express Mode initialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-06 06:19:24,018 - INFO - ‚úì Input file validated: ../sample/VTC Test Marking Scheme.docx\n",
      "2026-01-06 06:19:24,019 - INFO - ‚úì Output file will be: ../sample/VTC Test Marking Scheme.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Robust Step 2: Extract Marking Scheme to Excel initialized\n",
      "‚úì Session started at: 2026-01-06 06:19:24\n",
      "üìÅ Input: ../sample/VTC Test Marking Scheme.docx\n",
      "üìÅ Output: ../sample/VTC Test Marking Scheme.xlsx\n"
     ]
    }
   ],
   "source": [
    "from grading_utils import setup_paths, init_gemini_client\n",
    "from agents.marking_scheme_agent.agent import extract_marking_scheme_with_ai\n",
    "from google.genai import types\n",
    "import mammoth\n",
    "import html2text\n",
    "from IPython.display import Markdown, display, clear_output\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "import json\n",
    "import pandas as pd\n",
    "from termcolor import colored\n",
    "import os\n",
    "import shutil\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Robust logging setup\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"‚úÖ Robust Step 2: Extract Marking Scheme to Excel initialized\")\n",
    "print(f\"‚úì Session started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Configuration\n",
    "prefix = \"VTC Test\"  # Change this to your exam name\n",
    "dataset = \"sample\"   # Change to your dataset folder\n",
    "\n",
    "# Setup paths with validation\n",
    "try:\n",
    "    paths = setup_paths(prefix, dataset)\n",
    "    marking_scheme_word_file = f\"../{dataset}/{prefix} Marking Scheme.docx\"\n",
    "    marking_scheme_excel_file = paths[\"marking_scheme_file\"]\n",
    "    \n",
    "    # Validate input file exists\n",
    "    if not os.path.exists(marking_scheme_word_file):\n",
    "        raise FileNotFoundError(f\"Marking scheme file not found: {marking_scheme_word_file}\")\n",
    "    \n",
    "    logger.info(f\"‚úì Input file validated: {marking_scheme_word_file}\")\n",
    "    logger.info(f\"‚úì Output file will be: {marking_scheme_excel_file}\")\n",
    "    \n",
    "    print(f\"üìÅ Input: {marking_scheme_word_file}\")\n",
    "    print(f\"üìÅ Output: {marking_scheme_excel_file}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"‚ùå Setup failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a64616fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-06 06:19:24,498 - INFO - ‚úì Gemini client initialized successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Vertex AI Express Mode initialized\n",
      "ü§ñ Gemini AI client ready\n"
     ]
    }
   ],
   "source": [
    "# Robust Gemini client initialization with error handling\n",
    "try:\n",
    "    client = init_gemini_client()\n",
    "    logger.info(\"‚úì Gemini client initialized successfully\")\n",
    "    print(\"ü§ñ Gemini AI client ready\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"‚ùå Failed to initialize Gemini client: {e}\")\n",
    "    print(f\"‚ùå Gemini initialization failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24d68d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-06 06:19:24,591 - INFO - Processing Word document: ../sample/VTC Test Marking Scheme.docx\n",
      "2026-01-06 06:19:24,597 - INFO - File size: 18,126 bytes\n",
      "2026-01-06 06:19:25,197 - INFO - ‚úì Extracted 4,271 characters of markdown content\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Document processed successfully\n",
      "üìÑ Content length: 4,271 characters\n",
      "\n",
      "üìã Document Preview:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Detailed Marking Scheme (0-10 Scale)**\n",
       "\n",
       "Use the following rubric to determine the score for each q.\n",
       "\n",
       "**Q1: The Role of VTC**  \n",
       "The VTC is the largest provider of **VPET** in Hong Kong. Briefly explain what **VPET** stands for and why it is important for Hong Kong‚Äôs workforce development.\n",
       "\n",
       "  * **Answer:** VPET stands for **Vocational and Professional Education and Training**. It is important because it provides students with practical skills and specialized knowledge needed by industries, ensuring Hong Kong has a skilled labor force to support the economy.\n",
       "  * **Marking Breakdown:**\n",
       "    * **[2 marks]** Correctly stating \"Vocational and Professional Education and Training\".\n",
       "    * **[4 marks]** Explaining that it focuses on _practical skills_ or _specialized trades_.\n",
       "    * **[4 marks]** Explaining the benefit to the workforce (reducing skills gap, employment readiness).\n",
       "\n",
       "\n",
       "\n",
       "**Q2: Member Institutions**  \n",
       "Compare **IVE (Hong Kong Institute of Vocational Education)** and **THEi (Technologic..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Robust document processing with comprehensive error handling\n",
    "def process_word_document(file_path):\n",
    "    \"\"\"Process Word document with error handling and validation\"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Processing Word document: {file_path}\")\n",
    "        \n",
    "        # Validate file exists and is readable\n",
    "        if not os.path.exists(file_path):\n",
    "            raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "        \n",
    "        file_size = os.path.getsize(file_path)\n",
    "        logger.info(f\"File size: {file_size:,} bytes\")\n",
    "        \n",
    "        # Convert .docx to HTML using mammoth\n",
    "        with open(file_path, \"rb\") as docx_file:\n",
    "            result = mammoth.convert_to_html(docx_file)\n",
    "            html_content = result.value\n",
    "            \n",
    "            # Check for conversion warnings\n",
    "            if result.messages:\n",
    "                logger.warning(f\"Mammoth conversion warnings: {len(result.messages)}\")\n",
    "                for msg in result.messages[:5]:  # Show first 5 warnings\n",
    "                    logger.warning(f\"  - {msg}\")\n",
    "        \n",
    "        # Convert HTML to markdown using html2text\n",
    "        h = html2text.HTML2Text()\n",
    "        h.ignore_links = False\n",
    "        h.body_width = 0  # Don't wrap text\n",
    "        h.ignore_images = True\n",
    "        h.ignore_tables = False\n",
    "        \n",
    "        markdown_content = h.handle(html_content)\n",
    "        \n",
    "        # Validate content\n",
    "        if not markdown_content.strip():\n",
    "            raise ValueError(\"No content extracted from document\")\n",
    "        \n",
    "        content_length = len(markdown_content)\n",
    "        logger.info(f\"‚úì Extracted {content_length:,} characters of markdown content\")\n",
    "        \n",
    "        return markdown_content, html_content\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Document processing failed: {e}\")\n",
    "        raise\n",
    "\n",
    "# Process the document\n",
    "try:\n",
    "    markdown_content, html_content = process_word_document(marking_scheme_word_file)\n",
    "    print(\"‚úÖ Document processed successfully\")\n",
    "    print(f\"üìÑ Content length: {len(markdown_content):,} characters\")\n",
    "    \n",
    "    # Display preview of markdown content\n",
    "    print(\"\\nüìã Document Preview:\")\n",
    "    display(Markdown(markdown_content[:1000] + \"...\" if len(markdown_content) > 1000 else markdown_content))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Document processing failed: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab2d26ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-06 06:19:25,338 - INFO - ‚úì Robust data models loaded from agent module\n"
     ]
    }
   ],
   "source": [
    "# Robust Pydantic models are now defined in agents/marking_scheme_agent/agent.py\n",
    "logger.info(\"‚úì Robust data models loaded from agent module\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d17f884b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-06 06:19:25,409 - INFO - AI extraction attempt 1/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Processing document with Gemini AI...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-06 06:19:26,073 - INFO - Sending out request, model: gemini-3-flash-preview, backend: GoogleLLMVariant.VERTEX_AI, stream: False\n",
      "2026-01-06 06:19:26,093 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2026-01-06 06:20:03,745 - INFO - HTTP Request: POST https://aiplatform.googleapis.com/v1beta1/publishers/google/models/gemini-3-flash-preview:generateContent \"HTTP/1.1 200 OK\"\n",
      "2026-01-06 06:20:03,754 - INFO - Response received from the model.\n",
      "2026-01-06 06:20:03,763 - INFO - ‚úì Successfully extracted 5 questions via ADK output state!\n",
      "2026-01-06 06:20:03,765 - INFO - ‚úì General grading guide extracted (444 characters)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ AI extraction successful!\n",
      "üìä Extracted 5 questions\n",
      "üìã General grading guide: 444 characters\n"
     ]
    }
   ],
   "source": [
    "# Execute AI extraction using the imported agent\n",
    "try:\n",
    "    print(\"ü§ñ Processing document with Gemini AI...\")\n",
    "    # extract_marking_scheme_with_ai is imported from agents.marking_scheme_agent.agent\n",
    "    questions_data, general_guide = await extract_marking_scheme_with_ai(markdown_content)\n",
    "    \n",
    "    print(f\"‚úÖ AI extraction successful!\")\n",
    "    print(f\"üìä Extracted {len(questions_data)} questions\")\n",
    "    if general_guide:\n",
    "        print(f\"üìã General grading guide: {len(general_guide)} characters\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå AI extraction failed: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0504e269",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-06 06:20:03,817 - INFO - Validating extracted questions...\n",
      "2026-01-06 06:20:03,820 - INFO - Appending general grading guide to marking schemes\n",
      "2026-01-06 06:20:03,826 - INFO - ‚úì Validation passed: 5 questions, 50 total marks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[32m‚úÖ VALIDATION SUCCESSFUL!\u001b[0m\n",
      "üìä Questions: 5\n",
      "üìä Total marks: 50\n",
      "üìä Average marks per question: 10.0\n",
      "\n",
      "üìã Extracted Questions:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_number</th>\n",
       "      <th>question_text</th>\n",
       "      <th>marks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q1</td>\n",
       "      <td>The Role of VTC. The VTC is the largest provid...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q2</td>\n",
       "      <td>Member Institutions. Compare IVE (Hong Kong In...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q3</td>\n",
       "      <td>Educational Philosophy. VTC emphasizes the \"Th...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q4</td>\n",
       "      <td>Study Pathways. If a Secondary 6 student does ...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q5</td>\n",
       "      <td>Industry Partnership. Why does the VTC collabo...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  question_number                                      question_text  marks\n",
       "0              Q1  The Role of VTC. The VTC is the largest provid...     10\n",
       "1              Q2  Member Institutions. Compare IVE (Hong Kong In...     10\n",
       "2              Q3  Educational Philosophy. VTC emphasizes the \"Th...     10\n",
       "3              Q4  Study Pathways. If a Secondary 6 student does ...     10\n",
       "4              Q5  Industry Partnership. Why does the VTC collabo...     10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Robust validation and data processing\n",
    "def validate_and_process_questions(questions_data, general_guide):\n",
    "    \"\"\"Comprehensive validation and processing of extracted questions\"\"\"\n",
    "    try:\n",
    "        logger.info(\"Validating extracted questions...\")\n",
    "        \n",
    "        # Comprehensive validation\n",
    "        validation_errors = []\n",
    "        warnings = []\n",
    "        \n",
    "        if not questions_data:\n",
    "            validation_errors.append(\"No questions extracted from document\")\n",
    "            \n",
    "        for i, question in enumerate(questions_data):\n",
    "            q_num = question.get('question_number', f'Question {i+1}')\n",
    "            \n",
    "            # Validate required fields\n",
    "            if not question.get('question_text', '').strip():\n",
    "                validation_errors.append(f\"{q_num}: Missing question text\")\n",
    "            \n",
    "            if not question.get('marking_scheme', '').strip():\n",
    "                validation_errors.append(f\"{q_num}: Missing marking scheme\")\n",
    "            \n",
    "            # Validate marks\n",
    "            marks = question.get('marks', 0)\n",
    "            if not isinstance(marks, int) or marks <= 0:\n",
    "                validation_errors.append(f\"{q_num}: Invalid marks value ({marks})\")\n",
    "            elif marks > 50:\n",
    "                warnings.append(f\"{q_num}: High marks value ({marks}) - please verify\")\n",
    "        \n",
    "        # Report validation results\n",
    "        if validation_errors:\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(colored(\"‚ùå VALIDATION ERRORS DETECTED!\", \"red\", attrs=['bold']))\n",
    "            print(\"=\"*60)\n",
    "            for error in validation_errors:\n",
    "                print(colored(f\"  ‚Ä¢ {error}\", \"red\"))\n",
    "            print(\"=\"*60)\n",
    "            raise ValueError(f\"Validation failed: {len(validation_errors)} error(s) found\")\n",
    "        \n",
    "        if warnings:\n",
    "            print(\"\\n\" + \"‚ö†Ô∏è  VALIDATION WARNINGS:\")\n",
    "            for warning in warnings:\n",
    "                print(colored(f\"  ‚Ä¢ {warning}\", \"yellow\"))\n",
    "            print()\n",
    "        \n",
    "        # Process questions - append general guide if available\n",
    "        if general_guide and general_guide.strip():\n",
    "            logger.info(\"Appending general grading guide to marking schemes\")\n",
    "            for question in questions_data:\n",
    "                question['marking_scheme'] = f\"{question['marking_scheme']}\\n\\n---\\n\\n**General Grading Guide:**\\n{general_guide}\"\n",
    "        \n",
    "        # Create DataFrame with formatting\n",
    "        df = pd.DataFrame(questions_data)\n",
    "        \n",
    "        # Calculate statistics\n",
    "        total_questions = len(questions_data)\n",
    "        total_marks = df['marks'].sum()\n",
    "        avg_marks = df['marks'].mean()\n",
    "        \n",
    "        logger.info(f\"‚úì Validation passed: {total_questions} questions, {total_marks} total marks\")\n",
    "        \n",
    "        print(colored(\"‚úÖ VALIDATION SUCCESSFUL!\", \"green\", attrs=['bold']))\n",
    "        print(f\"üìä Questions: {total_questions}\")\n",
    "        print(f\"üìä Total marks: {total_marks}\")\n",
    "        print(f\"üìä Average marks per question: {avg_marks:.1f}\")\n",
    "        \n",
    "        return df, total_questions, total_marks\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Validation failed: {e}\")\n",
    "        raise\n",
    "\n",
    "# Execute validation and processing\n",
    "try:\n",
    "    df, total_questions, total_marks = validate_and_process_questions(questions_data, general_guide)\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\nüìã Extracted Questions:\")\n",
    "    display(df[['question_number', 'question_text', 'marks']].head(10))\n",
    "    \n",
    "    if len(df) > 10:\n",
    "        print(f\"... and {len(df) - 10} more questions\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Validation failed: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65aa866e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-06 06:20:03,885 - INFO - Creating Excel report: ../sample/VTC Test Marking Scheme.xlsx\n",
      "2026-01-06 06:20:03,888 - INFO - ‚úì Created backup: ../sample/VTC Test Marking Scheme.xlsx.backup.20260106_062003\n",
      "2026-01-06 06:20:04,077 - INFO - ‚úì Created 'Marking Scheme' sheet\n",
      "2026-01-06 06:20:04,085 - INFO - ‚úì Created 'Summary' sheet\n",
      "2026-01-06 06:20:04,089 - INFO - ‚úì Created 'Question Overview' sheet\n",
      "2026-01-06 06:20:04,092 - INFO - ‚úì Created 'Validation' sheet\n",
      "2026-01-06 06:20:04,145 - INFO - ‚úì Excel file created successfully (9,063 bytes)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "\u001b[1m\u001b[32müéâ STEP 2 COMPLETED SUCCESSFULLY!\u001b[0m\n",
      "============================================================\n",
      "üìÅ Output file: ../sample/VTC Test Marking Scheme.xlsx\n",
      "üìä Questions processed: 5\n",
      "üìä Total marks: 50\n",
      "üìã Sheets created: Marking Scheme, Summary, Question Overview, Validation\n",
      "‚è∞ Processing completed at: 2026-01-06 06:20:04\n",
      "============================================================\n",
      "\n",
      "‚úÖ Ready for Step 3: Question Annotations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bad pipe message: %s [b' 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Sa']\n",
      "Bad pipe message: %s [b'ri/537.36\\r\\nAccept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/', b'ng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\\r\\nAccept-Encoding: gzip, deflate, br, zstd\\r\\nA']\n",
      "Bad pipe message: %s [b'ept-Language: en-US,en;q=0.9,zh-TW;q=0.8,zh;q=0.7\\r\\nPriority: u=0, i\\r\\nReferer: https://studio.fireb', b'e.google.com/\\r\\nSec-Ch-Ua: \"Google Chrome\";v=\"143\", \"Chromium\";v=\"143\", \"Not A(Brand\";v=\"24\"\\r\\nSec', b'h-Ua-Arch: \"x86\"\\r\\nSec-Ch-Ua-Bitness: \"64\"\\r\\nS', b'-Ch-Ua-Form-Factors: \"Desktop\"\\r\\nSec-Ch-Ua-Full-Version: \"143.0.7499.170\"\\r\\nSec-Ch-Ua-Full-Version-Lis', b' \"Google Chrome\";v=\"143.0.7499.170\", \"Chromium\";v=\"143.0.7499.170\", \"Not A(Brand\";v=\"24.0.0.0\"\\r\\nSec-Ch-Ua-Mobile: ?']\n",
      "Bad pipe message: %s [b'\\nSec-Ch-Ua-Model: \"\"\\r\\nSec-Ch-Ua-Platform: \"Wind', b's\"\\r\\nSec-Ch-Ua-Platform-Version: \"19.0.0\"\\r\\nSec-Ch-Ua-Wow64: ?0\\r\\nSec-Fetch-Dest: iframe\\r\\nSec-Fetch-Mode: navigat', b'\\nSec-Fetch-Site: cross-site\\r\\nSec-Fetch-Storage-Access: active\\r\\nSec-Fetch-User: ?1\\r\\nSec-User-Ip: 10.2', b'20.216\\r\\nUpgrade-Insecure-Requests: 1\\r\\nX-Forward']\n",
      "Bad pipe message: %s [b'-Host: 32881-firebase-gemini-ai-grader-1767589248216.cluster-ejd22kqny5htuv5dfowoyipt52.cloudworksta']\n",
      "ERROR:tornado.general:Uncaught exception in ZMQStream callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user/gemini-handwriting-grader/.venv/lib/python3.12/site-packages/zmq/eventloop/zmqstream.py\", line 565, in _log_error\n",
      "    f.result()\n",
      "  File \"/home/user/gemini-handwriting-grader/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 341, in dispatch_control\n",
      "    await self.process_control(msg)\n",
      "  File \"/home/user/gemini-handwriting-grader/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 347, in process_control\n",
      "    idents, msg = self.session.feed_identities(msg, copy=False)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/gemini-handwriting-grader/.venv/lib/python3.12/site-packages/jupyter_client/session.py\", line 993, in feed_identities\n",
      "    raise ValueError(msg)\n",
      "ValueError: DELIM not in msg_list\n",
      "ERROR:tornado.general:Uncaught exception in ZMQStream callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user/gemini-handwriting-grader/.venv/lib/python3.12/site-packages/zmq/eventloop/zmqstream.py\", line 565, in _log_error\n",
      "    f.result()\n",
      "  File \"/home/user/gemini-handwriting-grader/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 577, in shell_channel_thread_main\n",
      "    _, msg2 = self.session.feed_identities(msg, copy=False)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/gemini-handwriting-grader/.venv/lib/python3.12/site-packages/jupyter_client/session.py\", line 993, in feed_identities\n",
      "    raise ValueError(msg)\n",
      "ValueError: DELIM not in msg_list\n",
      "ERROR:tornado.general:Uncaught exception in ZMQStream callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user/gemini-handwriting-grader/.venv/lib/python3.12/site-packages/zmq/eventloop/zmqstream.py\", line 565, in _log_error\n",
      "    f.result()\n",
      "  File \"/home/user/gemini-handwriting-grader/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 341, in dispatch_control\n",
      "    await self.process_control(msg)\n",
      "  File \"/home/user/gemini-handwriting-grader/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 347, in process_control\n",
      "    idents, msg = self.session.feed_identities(msg, copy=False)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/gemini-handwriting-grader/.venv/lib/python3.12/site-packages/jupyter_client/session.py\", line 993, in feed_identities\n",
      "    raise ValueError(msg)\n",
      "ValueError: DELIM not in msg_list\n",
      "ERROR:tornado.general:Uncaught exception in ZMQStream callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user/gemini-handwriting-grader/.venv/lib/python3.12/site-packages/zmq/eventloop/zmqstream.py\", line 565, in _log_error\n",
      "    f.result()\n",
      "  File \"/home/user/gemini-handwriting-grader/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 577, in shell_channel_thread_main\n",
      "    _, msg2 = self.session.feed_identities(msg, copy=False)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/gemini-handwriting-grader/.venv/lib/python3.12/site-packages/jupyter_client/session.py\", line 993, in feed_identities\n",
      "    raise ValueError(msg)\n",
      "ValueError: DELIM not in msg_list\n",
      "ERROR:tornado.general:Uncaught exception in ZMQStream callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user/gemini-handwriting-grader/.venv/lib/python3.12/site-packages/zmq/eventloop/zmqstream.py\", line 565, in _log_error\n",
      "    f.result()\n",
      "  File \"/home/user/gemini-handwriting-grader/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 341, in dispatch_control\n",
      "    await self.process_control(msg)\n",
      "  File \"/home/user/gemini-handwriting-grader/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 347, in process_control\n",
      "    idents, msg = self.session.feed_identities(msg, copy=False)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/gemini-handwriting-grader/.venv/lib/python3.12/site-packages/jupyter_client/session.py\", line 993, in feed_identities\n",
      "    raise ValueError(msg)\n",
      "ValueError: DELIM not in msg_list\n",
      "ERROR:tornado.general:Uncaught exception in ZMQStream callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user/gemini-handwriting-grader/.venv/lib/python3.12/site-packages/zmq/eventloop/zmqstream.py\", line 565, in _log_error\n",
      "    f.result()\n",
      "  File \"/home/user/gemini-handwriting-grader/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 577, in shell_channel_thread_main\n",
      "    _, msg2 = self.session.feed_identities(msg, copy=False)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/gemini-handwriting-grader/.venv/lib/python3.12/site-packages/jupyter_client/session.py\", line 993, in feed_identities\n",
      "    raise ValueError(msg)\n",
      "ValueError: DELIM not in msg_list\n",
      "ERROR:tornado.general:Uncaught exception in ZMQStream callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user/gemini-handwriting-grader/.venv/lib/python3.12/site-packages/zmq/eventloop/zmqstream.py\", line 565, in _log_error\n",
      "    f.result()\n",
      "  File \"/home/user/gemini-handwriting-grader/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 577, in shell_channel_thread_main\n",
      "    _, msg2 = self.session.feed_identities(msg, copy=False)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/gemini-handwriting-grader/.venv/lib/python3.12/site-packages/jupyter_client/session.py\", line 993, in feed_identities\n",
      "    raise ValueError(msg)\n",
      "ValueError: DELIM not in msg_list\n",
      "ERROR:tornado.general:Uncaught exception in ZMQStream callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user/gemini-handwriting-grader/.venv/lib/python3.12/site-packages/zmq/eventloop/zmqstream.py\", line 565, in _log_error\n",
      "    f.result()\n",
      "  File \"/home/user/gemini-handwriting-grader/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 341, in dispatch_control\n",
      "    await self.process_control(msg)\n",
      "  File \"/home/user/gemini-handwriting-grader/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 347, in process_control\n",
      "    idents, msg = self.session.feed_identities(msg, copy=False)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/gemini-handwriting-grader/.venv/lib/python3.12/site-packages/jupyter_client/session.py\", line 993, in feed_identities\n",
      "    raise ValueError(msg)\n",
      "ValueError: DELIM not in msg_list\n",
      "ERROR:tornado.general:Uncaught exception in ZMQStream callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user/gemini-handwriting-grader/.venv/lib/python3.12/site-packages/zmq/eventloop/zmqstream.py\", line 565, in _log_error\n",
      "    f.result()\n",
      "  File \"/home/user/gemini-handwriting-grader/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 577, in shell_channel_thread_main\n",
      "    _, msg2 = self.session.feed_identities(msg, copy=False)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/gemini-handwriting-grader/.venv/lib/python3.12/site-packages/jupyter_client/session.py\", line 993, in feed_identities\n",
      "    raise ValueError(msg)\n",
      "ValueError: DELIM not in msg_list\n",
      "ERROR:tornado.general:Uncaught exception in ZMQStream callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user/gemini-handwriting-grader/.venv/lib/python3.12/site-packages/zmq/eventloop/zmqstream.py\", line 565, in _log_error\n",
      "    f.result()\n",
      "  File \"/home/user/gemini-handwriting-grader/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 341, in dispatch_control\n",
      "    await self.process_control(msg)\n",
      "  File \"/home/user/gemini-handwriting-grader/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 347, in process_control\n",
      "    idents, msg = self.session.feed_identities(msg, copy=False)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/gemini-handwriting-grader/.venv/lib/python3.12/site-packages/jupyter_client/session.py\", line 993, in feed_identities\n",
      "    raise ValueError(msg)\n",
      "ValueError: DELIM not in msg_list\n"
     ]
    }
   ],
   "source": [
    "# Robust Excel export with comprehensive formatting and backupdef create_excel_report(df, marking_scheme_excel_file, total_questions, total_marks):    \"\"\"Create comprehensive Excel report with multiple sheets and formatting\"\"\"    try:        logger.info(f\"Creating Excel report: {marking_scheme_excel_file}\")                # Create backup of existing file if it exists        if os.path.exists(marking_scheme_excel_file):            backup_file = f\"{marking_scheme_excel_file}.backup.{datetime.now().strftime('%Y%m%d_%H%M%S')}\"            shutil.copy2(marking_scheme_excel_file, backup_file)            logger.info(f\"‚úì Created backup: {backup_file}\")                # Ensure output directory exists        os.makedirs(os.path.dirname(marking_scheme_excel_file), exist_ok=True)                # Create Excel writer with multiple sheets        with pd.ExcelWriter(marking_scheme_excel_file, engine='openpyxl') as writer:                        # Sheet 1: Marking Scheme (detailed rubric)            df.to_excel(writer, sheet_name='Marking Scheme', index=False)            logger.info(\"‚úì Created 'Marking Scheme' sheet\")                        # Sheet 2: Summary with statistics            summary_data = {                'Metric': [                    'Total Questions',                    'Total Marks',                    'Average Marks per Question',                    'Min Marks per Question',                    'Max Marks per Question',                    'Generated On',                    'Input File',                    'Output File'                ],                'Value': [                    total_questions,                    total_marks,                    f\"{df['marks'].mean():.1f}\",                    df['marks'].min(),                    df['marks'].max(),                    datetime.now().strftime('%Y-%m-%d %H:%M:%S'),                    os.path.basename(marking_scheme_word_file),                    os.path.basename(marking_scheme_excel_file)                ]            }            summary_df = pd.DataFrame(summary_data)            summary_df.to_excel(writer, sheet_name='Summary', index=False)            logger.info(\"‚úì Created 'Summary' sheet\")                        # Sheet 3: Question Overview (simplified view)            overview_df = df[['question_number', 'question_text', 'marks']].copy()            overview_df.to_excel(writer, sheet_name='Question Overview', index=False)            logger.info(\"‚úì Created 'Question Overview' sheet\")                        # Sheet 4: Validation Report            validation_data = {                'Check': [                    'All questions have marking schemes',                    'All questions have valid marks',                    'Question numbers are unique',                    'Total marks calculated',                    'General grading guide processed'                ],                'Status': [                    '‚úÖ PASS' if all(q.get('marking_scheme', '').strip() for q in questions_data) else '‚ùå FAIL',                    '‚úÖ PASS' if all(isinstance(q.get('marks', 0), int) and q.get('marks', 0) > 0 for q in questions_data) else '‚ùå FAIL',                    '‚úÖ PASS' if len(set(q.get('question_number', '') for q in questions_data)) == len(questions_data) else '‚ö†Ô∏è WARNING',                    f'‚úÖ PASS ({total_marks} marks)',                    '‚úÖ PROCESSED' if general_guide else '‚ÑπÔ∏è NOT FOUND'                ]            }            validation_df = pd.DataFrame(validation_data)            validation_df.to_excel(writer, sheet_name='Validation', index=False)            logger.info(\"‚úì Created 'Validation' sheet\")                # Verify file was created successfully        if os.path.exists(marking_scheme_excel_file):            file_size = os.path.getsize(marking_scheme_excel_file)            logger.info(f\"‚úì Excel file created successfully ({file_size:,} bytes)\")            return True        else:            raise FileNotFoundError(\"Excel file was not created\")                except Exception as e:        logger.error(f\"‚ùå Excel export failed: {e}\")        raise# Execute Excel exporttry:    success = create_excel_report(df, marking_scheme_excel_file, total_questions, total_marks)        if success:        print(\"\\n\" + \"=\"*60)        print(colored(\"üéâ STEP 2 COMPLETED SUCCESSFULLY!\", \"green\", attrs=['bold']))        print(\"=\"*60)        print(f\"üìÅ Output file: {marking_scheme_excel_file}\")        print(f\"üìä Questions processed: {total_questions}\")        print(f\"üìä Total marks: {total_marks}\")        print(f\"üìã Sheets created: Marking Scheme, Summary, Question Overview, Validation\")        print(f\"‚è∞ Processing completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")        print(\"=\"*60)        print(\"\\n‚úÖ Ready for Step 3: Question Annotations\")        except Exception as e:    print(f\"‚ùå Excel export failed: {e}\")    raise\n",
    "# Robust Excel export with comprehensive formatting and backup\n",
    "def create_excel_report(df, marking_scheme_excel_file, total_questions, total_marks):\n",
    "    \"\"\"Create comprehensive Excel report with multiple sheets and formatting\"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Creating Excel report: {marking_scheme_excel_file}\")\n",
    "        \n",
    "        # Create backup of existing file if it exists\n",
    "        if os.path.exists(marking_scheme_excel_file):\n",
    "            backup_file = f\"{marking_scheme_excel_file}.backup.{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "            shutil.copy2(marking_scheme_excel_file, backup_file)\n",
    "            logger.info(f\"‚úì Created backup: {backup_file}\")\n",
    "        \n",
    "        # Ensure output directory exists\n",
    "        os.makedirs(os.path.dirname(marking_scheme_excel_file), exist_ok=True)\n",
    "        \n",
    "        # Create Excel writer with multiple sheets\n",
    "        with pd.ExcelWriter(marking_scheme_excel_file, engine='openpyxl') as writer:\n",
    "            # Sheet 1: Marking Scheme (detailed rubric)\n",
    "            df.to_excel(writer, sheet_name='Marking Scheme', index=False)\n",
    "            logger.info(\"‚úì Created 'Marking Scheme' sheet\")\n",
    "            \n",
    "            # Sheet 2: Summary with statistics\n",
    "            summary_data = {\n",
    "                'Metric': [\n",
    "                    'Total Questions',\n",
    "                    'Total Marks',\n",
    "                    'Average Marks per Question',\n",
    "                    'Min Marks per Question',\n",
    "                    'Max Marks per Question',\n",
    "                    'Generated On',\n",
    "                    'Input File',\n",
    "                    'Output File'\n",
    "                ],\n",
    "                'Value': [\n",
    "                    total_questions,\n",
    "                    total_marks,\n",
    "                    f\"{df['marks'].mean():.1f}\",\n",
    "                    df['marks'].min(),\n",
    "                    df['marks'].max(),\n",
    "                    datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                    os.path.basename(marking_scheme_word_file),\n",
    "                    os.path.basename(marking_scheme_excel_file)\n",
    "                ]\n",
    "            }\n",
    "            summary_df = pd.DataFrame(summary_data)\n",
    "            summary_df.to_excel(writer, sheet_name='Summary', index=False)\n",
    "            logger.info(\"‚úì Created 'Summary' sheet\")\n",
    "            \n",
    "            # Sheet 3: Question Overview (simplified view)\n",
    "            overview_df = df[['question_number', 'question_text', 'marks']].copy()\n",
    "            overview_df.to_excel(writer, sheet_name='Question Overview', index=False)\n",
    "            logger.info(\"‚úì Created 'Question Overview' sheet\")\n",
    "            \n",
    "            # Sheet 4: Validation Report\n",
    "            validation_data = {\n",
    "                'Check': [\n",
    "                    'All questions have marking schemes',\n",
    "                    'All questions have valid marks',\n",
    "                    'Question numbers are unique',\n",
    "                    'Total marks calculated',\n",
    "                    'General grading guide processed'\n",
    "                ],\n",
    "                'Status': [\n",
    "                    '‚úÖ PASS' if all(q.get('marking_scheme', '').strip() for q in questions_data) else '‚ùå FAIL',\n",
    "                    '‚úÖ PASS' if all(isinstance(q.get('marks', 0), int) and q.get('marks', 0) > 0 for q in questions_data) else '‚ùå FAIL',\n",
    "                    '‚úÖ PASS' if len(set(q.get('question_number', '') for q in questions_data)) == len(questions_data) else '‚ö†Ô∏è WARNING',\n",
    "                    f'‚úÖ PASS ({total_marks} marks)',\n",
    "                    '‚úÖ PROCESSED' if general_guide else '‚ÑπÔ∏è NOT FOUND'\n",
    "                ]\n",
    "            }\n",
    "            validation_df = pd.DataFrame(validation_data)\n",
    "            validation_df.to_excel(writer, sheet_name='Validation', index=False)\n",
    "            logger.info(\"‚úì Created 'Validation' sheet\")\n",
    "        \n",
    "        # Verify file was created successfully\n",
    "        if os.path.exists(marking_scheme_excel_file):\n",
    "            file_size = os.path.getsize(marking_scheme_excel_file)\n",
    "            logger.info(f\"‚úì Excel file created successfully ({file_size:,} bytes)\")\n",
    "            return True\n",
    "        else:\n",
    "            raise FileNotFoundError(\"Excel file was not created\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Excel export failed: {e}\")\n",
    "        raise\n",
    "\n",
    "# Execute Excel export\n",
    "try:\n",
    "    success = create_excel_report(df, marking_scheme_excel_file, total_questions, total_marks)\n",
    "    \n",
    "    if success:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(colored(\"üéâ STEP 2 COMPLETED SUCCESSFULLY!\", \"green\", attrs=['bold']))\n",
    "        print(\"=\"*60)\n",
    "        print(f\"üìÅ Output file: {marking_scheme_excel_file}\")\n",
    "        print(f\"üìä Questions processed: {total_questions}\")\n",
    "        print(f\"üìä Total marks: {total_marks}\")\n",
    "        print(f\"üìã Sheets created: Marking Scheme, Summary, Question Overview, Validation\")\n",
    "        print(f\"‚è∞ Processing completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"\\n‚úÖ Ready for Step 3: Question Annotations\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Excel export failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ba75e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
