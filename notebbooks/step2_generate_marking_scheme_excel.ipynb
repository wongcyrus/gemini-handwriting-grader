{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d61b374",
   "metadata": {},
   "source": [
    "# Step 2: Extract Marking Scheme to Excel\n",
    "\n",
    "Use Gemini to parse the Word marking scheme into structured Excel sheets with error handling, validation, and progress tracking.\n",
    "\n",
    "**Features:**\n",
    "- ‚úÖ Comprehensive error handling and validation\n",
    "- ‚úÖ Robust progress tracking and logging\n",
    "- ‚úÖ Improved markdown formatting and structure\n",
    "- ‚úÖ Robust file handling and backup\n",
    "- ‚úÖ Detailed validation and quality checks\n",
    "- ‚úÖ Professional output formatting\n",
    "\n",
    "Configure the exam `prefix` and dataset folder in the next cell before running.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f987fbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from grading_utils import setup_paths, init_gemini_client\n",
    "from google.genai import types\n",
    "import mammoth\n",
    "import html2text\n",
    "from IPython.display import Markdown, display, clear_output\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "import json\n",
    "import pandas as pd\n",
    "from termcolor import colored\n",
    "import os\n",
    "import shutil\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Robust logging setup\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"‚úÖ Robust Step 2: Extract Marking Scheme to Excel initialized\")\n",
    "print(f\"‚úì Session started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Configuration\n",
    "prefix = \"VTC Test\"  # Change this to your exam name\n",
    "dataset = \"sample\"   # Change to your dataset folder\n",
    "\n",
    "# Setup paths with validation\n",
    "try:\n",
    "    paths = setup_paths(prefix, dataset)\n",
    "    marking_scheme_word_file = f\"../{dataset}/{prefix} Marking Scheme.docx\"\n",
    "    marking_scheme_excel_file = paths[\"marking_scheme_file\"]\n",
    "    \n",
    "    # Validate input file exists\n",
    "    if not os.path.exists(marking_scheme_word_file):\n",
    "        raise FileNotFoundError(f\"Marking scheme file not found: {marking_scheme_word_file}\")\n",
    "    \n",
    "    logger.info(f\"‚úì Input file validated: {marking_scheme_word_file}\")\n",
    "    logger.info(f\"‚úì Output file will be: {marking_scheme_excel_file}\")\n",
    "    \n",
    "    print(f\"üìÅ Input: {marking_scheme_word_file}\")\n",
    "    print(f\"üìÅ Output: {marking_scheme_excel_file}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"‚ùå Setup failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64616fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust Gemini client initialization with error handling\n",
    "try:\n",
    "    client = init_gemini_client()\n",
    "    logger.info(\"‚úì Gemini client initialized successfully\")\n",
    "    print(\"ü§ñ Gemini AI client ready\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"‚ùå Failed to initialize Gemini client: {e}\")\n",
    "    print(f\"‚ùå Gemini initialization failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d68d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust document processing with comprehensive error handling\n",
    "def process_word_document(file_path):\n",
    "    \"\"\"Process Word document with error handling and validation\"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Processing Word document: {file_path}\")\n",
    "        \n",
    "        # Validate file exists and is readable\n",
    "        if not os.path.exists(file_path):\n",
    "            raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "        \n",
    "        file_size = os.path.getsize(file_path)\n",
    "        logger.info(f\"File size: {file_size:,} bytes\")\n",
    "        \n",
    "        # Convert .docx to HTML using mammoth\n",
    "        with open(file_path, \"rb\") as docx_file:\n",
    "            result = mammoth.convert_to_html(docx_file)\n",
    "            html_content = result.value\n",
    "            \n",
    "            # Check for conversion warnings\n",
    "            if result.messages:\n",
    "                logger.warning(f\"Mammoth conversion warnings: {len(result.messages)}\")\n",
    "                for msg in result.messages[:5]:  # Show first 5 warnings\n",
    "                    logger.warning(f\"  - {msg}\")\n",
    "        \n",
    "        # Convert HTML to markdown using html2text\n",
    "        h = html2text.HTML2Text()\n",
    "        h.ignore_links = False\n",
    "        h.body_width = 0  # Don't wrap text\n",
    "        h.ignore_images = True\n",
    "        h.ignore_tables = False\n",
    "        \n",
    "        markdown_content = h.handle(html_content)\n",
    "        \n",
    "        # Validate content\n",
    "        if not markdown_content.strip():\n",
    "            raise ValueError(\"No content extracted from document\")\n",
    "        \n",
    "        content_length = len(markdown_content)\n",
    "        logger.info(f\"‚úì Extracted {content_length:,} characters of markdown content\")\n",
    "        \n",
    "        return markdown_content, html_content\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Document processing failed: {e}\")\n",
    "        raise\n",
    "\n",
    "# Process the document\n",
    "try:\n",
    "    markdown_content, html_content = process_word_document(marking_scheme_word_file)\n",
    "    print(\"‚úÖ Document processed successfully\")\n",
    "    print(f\"üìÑ Content length: {len(markdown_content):,} characters\")\n",
    "    \n",
    "    # Display preview of markdown content\n",
    "    print(\"\\nüìã Document Preview:\")\n",
    "    display(Markdown(markdown_content[:1000] + \"...\" if len(markdown_content) > 1000 else markdown_content))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Document processing failed: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2d26ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust Pydantic models with comprehensive validation\n",
    "class Question(BaseModel):\n",
    "    \"\"\"Robust question model with comprehensive validation\"\"\"\n",
    "    question_number: str = Field(\n",
    "        description=\"The question number (e.g., '1', '2', '22a', '22b', 'Q1','Q2')\"\n",
    "    )\n",
    "    question_text: str = Field(\n",
    "        description=\"The full question text\"\n",
    "    )\n",
    "    marking_scheme: str = Field(\n",
    "        description=\"Well-formatted marking scheme using markdown. Use bullet points (-), numbered lists (1., 2.), bold (**text**) for key terms, and clear line breaks. Include point allocations in parentheses (e.g., '- Key concept explained (2 marks)'). Structure should be clear and scannable.\"\n",
    "    )\n",
    "    marks: int = Field(\n",
    "        description=\"Total marks available for this question\"\n",
    "    )\n",
    "\n",
    "class MarkingSchemeResponse(BaseModel):\n",
    "    \"\"\"Robust wrapper class with validation\"\"\"\n",
    "    general_grading_guide: str = Field(\n",
    "        default=\"\",\n",
    "        description=\"General grading guide for partial marks applicable to all questions, formatted in markdown\"\n",
    "    )\n",
    "    questions: List[Question] = Field(\n",
    "        description=\"List of questions with marking schemes and marks\"\n",
    "    )\n",
    "\n",
    "logger.info(\"‚úì Robust data models defined with validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17f884b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust AI processing with comprehensive error handling and retry logic\n",
    "def extract_marking_scheme_with_ai(markdown_content, max_retries=3):\n",
    "    \"\"\"Extract marking scheme using AI with error handling\"\"\"\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            logger.info(f\"AI extraction attempt {attempt + 1}/{max_retries}\")\n",
    "            \n",
    "            # Create prompt\n",
    "            prompt = f\"\"\"Please analyze this marking scheme document and extract structured, well-formatted data.\n",
    "\n",
    "**FORMATTING REQUIREMENTS for marking_scheme:**\n",
    "- Use markdown formatting (bullet points -, numbered lists 1., 2., bold **text**)\n",
    "- Each marking criterion should be on its own line\n",
    "- Show point allocations clearly (e.g., \"- Correct formula (2 marks)\")\n",
    "- Use clear hierarchy with proper indentation for sub-points\n",
    "- Add line breaks between major sections\n",
    "- Bold important terms or key concepts\n",
    "- Make it scannable and easy to read\n",
    "\n",
    "**EXTRACT:**\n",
    "\n",
    "1. **GENERAL GRADING GUIDE**: Extract any general grading guide or guidance for partial marks that applies to all/multiple questions (use markdown formatting)\n",
    "\n",
    "2. **FOR EACH QUESTION**: Extract:\n",
    "   - Question number (normalize to consistent format)\n",
    "   - Question text (complete question statement)\n",
    "   - **Marking scheme** (well-formatted with markdown, bullets, numbering, clear point allocation)\n",
    "   - Total marks available (must be a positive integer)\n",
    "\n",
    "**Important Guidelines:**\n",
    "- When extracting the marking_scheme for each question, incorporate any general grading principles that apply to that question's scoring\n",
    "- Ensure all questions have non-empty marking schemes\n",
    "- Validate that mark totals are reasonable (1-100 marks per question)\n",
    "- Use consistent formatting throughout\n",
    "\n",
    "**Document Content:**\n",
    "\n",
    "{markdown_content}\n",
    "\"\"\"\n",
    "\n",
    "            # Create configuration with structured output\n",
    "            config = types.GenerateContentConfig(\n",
    "                temperature=0.1,  # Lower temperature for more consistent extraction\n",
    "                top_p=0.5,\n",
    "                max_output_tokens=8192,  # Increased for larger documents\n",
    "                response_mime_type=\"application/json\",\n",
    "                response_schema=MarkingSchemeResponse,\n",
    "            )\n",
    "\n",
    "            # Send to Gemini using structured output\n",
    "            response = client.models.generate_content(\n",
    "                model=\"gemini-3-pro-preview\",\n",
    "                contents=[{\"role\": \"user\", \"parts\": [{\"text\": prompt}]}],\n",
    "                config=config,\n",
    "            )\n",
    "\n",
    "            # Robust response processing\n",
    "            if hasattr(response, 'parsed') and response.parsed is not None:\n",
    "                result = response.parsed\n",
    "                general_guide = result.general_grading_guide\n",
    "                questions_data = [q.model_dump() for q in result.questions]\n",
    "                \n",
    "                logger.info(f\"‚úì Successfully extracted {len(questions_data)} questions with structured output!\")\n",
    "                if general_guide:\n",
    "                    logger.info(f\"‚úì General grading guide extracted ({len(general_guide)} characters)\")\n",
    "                \n",
    "                return questions_data, general_guide\n",
    "                \n",
    "            else:\n",
    "                # Robust fallback to text parsing\n",
    "                response_text = response.text\n",
    "                logger.warning(\"Structured output not available, attempting text parsing\")\n",
    "                \n",
    "                try:\n",
    "                    parsed_json = json.loads(response_text)\n",
    "                    general_guide = parsed_json.get('general_grading_guide', '')\n",
    "                    questions_data = parsed_json.get('questions', [])\n",
    "                    \n",
    "                    if not questions_data:\n",
    "                        raise ValueError(\"No questions found in response\")\n",
    "                    \n",
    "                    logger.info(f\"‚úì Successfully extracted {len(questions_data)} questions from text!\")\n",
    "                    return questions_data, general_guide\n",
    "                    \n",
    "                except json.JSONDecodeError as e:\n",
    "                    logger.error(f\"JSON parsing failed: {e}\")\n",
    "                    if attempt < max_retries - 1:\n",
    "                        logger.info(\"Retrying with adjusted parameters...\")\n",
    "                        continue\n",
    "                    else:\n",
    "                        raise ValueError(f\"Failed to parse AI response after {max_retries} attempts\")\n",
    "                        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"AI extraction attempt {attempt + 1} failed: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                logger.info(\"Retrying...\")\n",
    "                continue\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "# Execute AI extraction\n",
    "try:\n",
    "    print(\"ü§ñ Processing document with Gemini AI...\")\n",
    "    questions_data, general_guide = extract_marking_scheme_with_ai(markdown_content)\n",
    "    \n",
    "    print(f\"‚úÖ AI extraction successful!\")\n",
    "    print(f\"üìä Extracted {len(questions_data)} questions\")\n",
    "    if general_guide:\n",
    "        print(f\"üìã General grading guide: {len(general_guide)} characters\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå AI extraction failed: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0504e269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust validation and data processing\n",
    "def validate_and_process_questions(questions_data, general_guide):\n",
    "    \"\"\"Comprehensive validation and processing of extracted questions\"\"\"\n",
    "    try:\n",
    "        logger.info(\"Validating extracted questions...\")\n",
    "        \n",
    "        # Comprehensive validation\n",
    "        validation_errors = []\n",
    "        warnings = []\n",
    "        \n",
    "        if not questions_data:\n",
    "            validation_errors.append(\"No questions extracted from document\")\n",
    "            \n",
    "        for i, question in enumerate(questions_data):\n",
    "            q_num = question.get('question_number', f'Question {i+1}')\n",
    "            \n",
    "            # Validate required fields\n",
    "            if not question.get('question_text', '').strip():\n",
    "                validation_errors.append(f\"{q_num}: Missing question text\")\n",
    "            \n",
    "            if not question.get('marking_scheme', '').strip():\n",
    "                validation_errors.append(f\"{q_num}: Missing marking scheme\")\n",
    "            \n",
    "            # Validate marks\n",
    "            marks = question.get('marks', 0)\n",
    "            if not isinstance(marks, int) or marks <= 0:\n",
    "                validation_errors.append(f\"{q_num}: Invalid marks value ({marks})\")\n",
    "            elif marks > 50:\n",
    "                warnings.append(f\"{q_num}: High marks value ({marks}) - please verify\")\n",
    "        \n",
    "        # Report validation results\n",
    "        if validation_errors:\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(colored(\"‚ùå VALIDATION ERRORS DETECTED!\", \"red\", attrs=['bold']))\n",
    "            print(\"=\"*60)\n",
    "            for error in validation_errors:\n",
    "                print(colored(f\"  ‚Ä¢ {error}\", \"red\"))\n",
    "            print(\"=\"*60)\n",
    "            raise ValueError(f\"Validation failed: {len(validation_errors)} error(s) found\")\n",
    "        \n",
    "        if warnings:\n",
    "            print(\"\\n\" + \"‚ö†Ô∏è  VALIDATION WARNINGS:\")\n",
    "            for warning in warnings:\n",
    "                print(colored(f\"  ‚Ä¢ {warning}\", \"yellow\"))\n",
    "            print()\n",
    "        \n",
    "        # Process questions - append general guide if available\n",
    "        if general_guide and general_guide.strip():\n",
    "            logger.info(\"Appending general grading guide to marking schemes\")\n",
    "            for question in questions_data:\n",
    "                question['marking_scheme'] = f\"{question['marking_scheme']}\\n\\n---\\n\\n**General Grading Guide:**\\n{general_guide}\"\n",
    "        \n",
    "        # Create DataFrame with formatting\n",
    "        df = pd.DataFrame(questions_data)\n",
    "        \n",
    "        # Calculate statistics\n",
    "        total_questions = len(questions_data)\n",
    "        total_marks = df['marks'].sum()\n",
    "        avg_marks = df['marks'].mean()\n",
    "        \n",
    "        logger.info(f\"‚úì Validation passed: {total_questions} questions, {total_marks} total marks\")\n",
    "        \n",
    "        print(colored(\"‚úÖ VALIDATION SUCCESSFUL!\", \"green\", attrs=['bold']))\n",
    "        print(f\"üìä Questions: {total_questions}\")\n",
    "        print(f\"üìä Total marks: {total_marks}\")\n",
    "        print(f\"üìä Average marks per question: {avg_marks:.1f}\")\n",
    "        \n",
    "        return df, total_questions, total_marks\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Validation failed: {e}\")\n",
    "        raise\n",
    "\n",
    "# Execute validation and processing\n",
    "try:\n",
    "    df, total_questions, total_marks = validate_and_process_questions(questions_data, general_guide)\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\nüìã Extracted Questions:\")\n",
    "    display(df[['question_number', 'question_text', 'marks']].head(10))\n",
    "    \n",
    "    if len(df) > 10:\n",
    "        print(f\"... and {len(df) - 10} more questions\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Validation failed: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65aa866e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust Excel export with comprehensive formatting and backupdef create_excel_report(df, marking_scheme_excel_file, total_questions, total_marks):    \"\"\"Create comprehensive Excel report with multiple sheets and formatting\"\"\"    try:        logger.info(f\"Creating Excel report: {marking_scheme_excel_file}\")                # Create backup of existing file if it exists        if os.path.exists(marking_scheme_excel_file):            backup_file = f\"{marking_scheme_excel_file}.backup.{datetime.now().strftime('%Y%m%d_%H%M%S')}\"            shutil.copy2(marking_scheme_excel_file, backup_file)            logger.info(f\"‚úì Created backup: {backup_file}\")                # Ensure output directory exists        os.makedirs(os.path.dirname(marking_scheme_excel_file), exist_ok=True)                # Create Excel writer with multiple sheets        with pd.ExcelWriter(marking_scheme_excel_file, engine='openpyxl') as writer:                        # Sheet 1: Marking Scheme (detailed rubric)            df.to_excel(writer, sheet_name='Marking Scheme', index=False)            logger.info(\"‚úì Created 'Marking Scheme' sheet\")                        # Sheet 2: Summary with statistics            summary_data = {                'Metric': [                    'Total Questions',                    'Total Marks',                    'Average Marks per Question',                    'Min Marks per Question',                    'Max Marks per Question',                    'Generated On',                    'Input File',                    'Output File'                ],                'Value': [                    total_questions,                    total_marks,                    f\"{df['marks'].mean():.1f}\",                    df['marks'].min(),                    df['marks'].max(),                    datetime.now().strftime('%Y-%m-%d %H:%M:%S'),                    os.path.basename(marking_scheme_word_file),                    os.path.basename(marking_scheme_excel_file)                ]            }            summary_df = pd.DataFrame(summary_data)            summary_df.to_excel(writer, sheet_name='Summary', index=False)            logger.info(\"‚úì Created 'Summary' sheet\")                        # Sheet 3: Question Overview (simplified view)            overview_df = df[['question_number', 'question_text', 'marks']].copy()            overview_df.to_excel(writer, sheet_name='Question Overview', index=False)            logger.info(\"‚úì Created 'Question Overview' sheet\")                        # Sheet 4: Validation Report            validation_data = {                'Check': [                    'All questions have marking schemes',                    'All questions have valid marks',                    'Question numbers are unique',                    'Total marks calculated',                    'General grading guide processed'                ],                'Status': [                    '‚úÖ PASS' if all(q.get('marking_scheme', '').strip() for q in questions_data) else '‚ùå FAIL',                    '‚úÖ PASS' if all(isinstance(q.get('marks', 0), int) and q.get('marks', 0) > 0 for q in questions_data) else '‚ùå FAIL',                    '‚úÖ PASS' if len(set(q.get('question_number', '') for q in questions_data)) == len(questions_data) else '‚ö†Ô∏è WARNING',                    f'‚úÖ PASS ({total_marks} marks)',                    '‚úÖ PROCESSED' if general_guide else '‚ÑπÔ∏è NOT FOUND'                ]            }            validation_df = pd.DataFrame(validation_data)            validation_df.to_excel(writer, sheet_name='Validation', index=False)            logger.info(\"‚úì Created 'Validation' sheet\")                # Verify file was created successfully        if os.path.exists(marking_scheme_excel_file):            file_size = os.path.getsize(marking_scheme_excel_file)            logger.info(f\"‚úì Excel file created successfully ({file_size:,} bytes)\")            return True        else:            raise FileNotFoundError(\"Excel file was not created\")                except Exception as e:        logger.error(f\"‚ùå Excel export failed: {e}\")        raise# Execute Excel exporttry:    success = create_excel_report(df, marking_scheme_excel_file, total_questions, total_marks)        if success:        print(\"\\n\" + \"=\"*60)        print(colored(\"üéâ STEP 2 COMPLETED SUCCESSFULLY!\", \"green\", attrs=['bold']))        print(\"=\"*60)        print(f\"üìÅ Output file: {marking_scheme_excel_file}\")        print(f\"üìä Questions processed: {total_questions}\")        print(f\"üìä Total marks: {total_marks}\")        print(f\"üìã Sheets created: Marking Scheme, Summary, Question Overview, Validation\")        print(f\"‚è∞ Processing completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")        print(\"=\"*60)        print(\"\\n‚úÖ Ready for Step 3: Question Annotations\")        except Exception as e:    print(f\"‚ùå Excel export failed: {e}\")    raise\n",
    "# Robust Excel export with comprehensive formatting and backup\n",
    "def create_excel_report(df, marking_scheme_excel_file, total_questions, total_marks):\n",
    "    \"\"\"Create comprehensive Excel report with multiple sheets and formatting\"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Creating Excel report: {marking_scheme_excel_file}\")\n",
    "        \n",
    "        # Create backup of existing file if it exists\n",
    "        if os.path.exists(marking_scheme_excel_file):\n",
    "            backup_file = f\"{marking_scheme_excel_file}.backup.{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "            shutil.copy2(marking_scheme_excel_file, backup_file)\n",
    "            logger.info(f\"‚úì Created backup: {backup_file}\")\n",
    "        \n",
    "        # Ensure output directory exists\n",
    "        os.makedirs(os.path.dirname(marking_scheme_excel_file), exist_ok=True)\n",
    "        \n",
    "        # Create Excel writer with multiple sheets\n",
    "        with pd.ExcelWriter(marking_scheme_excel_file, engine='openpyxl') as writer:\n",
    "            # Sheet 1: Marking Scheme (detailed rubric)\n",
    "            df.to_excel(writer, sheet_name='Marking Scheme', index=False)\n",
    "            logger.info(\"‚úì Created 'Marking Scheme' sheet\")\n",
    "            \n",
    "            # Sheet 2: Summary with statistics\n",
    "            summary_data = {\n",
    "                'Metric': [\n",
    "                    'Total Questions',\n",
    "                    'Total Marks',\n",
    "                    'Average Marks per Question',\n",
    "                    'Min Marks per Question',\n",
    "                    'Max Marks per Question',\n",
    "                    'Generated On',\n",
    "                    'Input File',\n",
    "                    'Output File'\n",
    "                ],\n",
    "                'Value': [\n",
    "                    total_questions,\n",
    "                    total_marks,\n",
    "                    f\"{df['marks'].mean():.1f}\",\n",
    "                    df['marks'].min(),\n",
    "                    df['marks'].max(),\n",
    "                    datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                    os.path.basename(marking_scheme_word_file),\n",
    "                    os.path.basename(marking_scheme_excel_file)\n",
    "                ]\n",
    "            }\n",
    "            summary_df = pd.DataFrame(summary_data)\n",
    "            summary_df.to_excel(writer, sheet_name='Summary', index=False)\n",
    "            logger.info(\"‚úì Created 'Summary' sheet\")\n",
    "            \n",
    "            # Sheet 3: Question Overview (simplified view)\n",
    "            overview_df = df[['question_number', 'question_text', 'marks']].copy()\n",
    "            overview_df.to_excel(writer, sheet_name='Question Overview', index=False)\n",
    "            logger.info(\"‚úì Created 'Question Overview' sheet\")\n",
    "            \n",
    "            # Sheet 4: Validation Report\n",
    "            validation_data = {\n",
    "                'Check': [\n",
    "                    'All questions have marking schemes',\n",
    "                    'All questions have valid marks',\n",
    "                    'Question numbers are unique',\n",
    "                    'Total marks calculated',\n",
    "                    'General grading guide processed'\n",
    "                ],\n",
    "                'Status': [\n",
    "                    '‚úÖ PASS' if all(q.get('marking_scheme', '').strip() for q in questions_data) else '‚ùå FAIL',\n",
    "                    '‚úÖ PASS' if all(isinstance(q.get('marks', 0), int) and q.get('marks', 0) > 0 for q in questions_data) else '‚ùå FAIL',\n",
    "                    '‚úÖ PASS' if len(set(q.get('question_number', '') for q in questions_data)) == len(questions_data) else '‚ö†Ô∏è WARNING',\n",
    "                    f'‚úÖ PASS ({total_marks} marks)',\n",
    "                    '‚úÖ PROCESSED' if general_guide else '‚ÑπÔ∏è NOT FOUND'\n",
    "                ]\n",
    "            }\n",
    "            validation_df = pd.DataFrame(validation_data)\n",
    "            validation_df.to_excel(writer, sheet_name='Validation', index=False)\n",
    "            logger.info(\"‚úì Created 'Validation' sheet\")\n",
    "        \n",
    "        # Verify file was created successfully\n",
    "        if os.path.exists(marking_scheme_excel_file):\n",
    "            file_size = os.path.getsize(marking_scheme_excel_file)\n",
    "            logger.info(f\"‚úì Excel file created successfully ({file_size:,} bytes)\")\n",
    "            return True\n",
    "        else:\n",
    "            raise FileNotFoundError(\"Excel file was not created\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Excel export failed: {e}\")\n",
    "        raise\n",
    "\n",
    "# Execute Excel export\n",
    "try:\n",
    "    success = create_excel_report(df, marking_scheme_excel_file, total_questions, total_marks)\n",
    "    \n",
    "    if success:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(colored(\"üéâ STEP 2 COMPLETED SUCCESSFULLY!\", \"green\", attrs=['bold']))\n",
    "        print(\"=\"*60)\n",
    "        print(f\"üìÅ Output file: {marking_scheme_excel_file}\")\n",
    "        print(f\"üìä Questions processed: {total_questions}\")\n",
    "        print(f\"üìä Total marks: {total_marks}\")\n",
    "        print(f\"üìã Sheets created: Marking Scheme, Summary, Question Overview, Validation\")\n",
    "        print(f\"‚è∞ Processing completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"\\n‚úÖ Ready for Step 3: Question Annotations\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Excel export failed: {e}\")\n",
    "    raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
