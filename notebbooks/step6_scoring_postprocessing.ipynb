{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Post-Scoring Packaging\n",
    "Backup results, generate reports, produce scored PDFs, and collect samples for sharing. Run after completing scoring and checks.\n",
    "\n",
    "**Features:**\n",
    "- ‚úÖ Comprehensive backup and archiving system\n",
    "- ‚úÖ Robust report generation with detailed analytics\n",
    "- ‚úÖ Automated PDF processing with validation\n",
    "- ‚úÖ Performance report generation with AI insights\n",
    "- ‚úÖ Class-level analytics and recommendations\n",
    "- ‚úÖ Robust error handling and progress tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grading_utils import setup_paths, create_directories, build_student_id_mapping\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from IPython.display import display, clear_output\n",
    "from ipywidgets import IntProgress, HTML\n",
    "import logging\n",
    "from pypdf import PdfReader, PdfWriter\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import hashlib\n",
    "import math\n",
    "from docx import Document\n",
    "from docx.enum.text import WD_ALIGN_PARAGRAPH\n",
    "from docx.shared import Pt, Inches\n",
    "\n",
    "\n",
    "# Robust logging setup\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"‚úÖ Robust Step 6: Post-Scoring Packaging initialized\")\n",
    "print(f\"‚úì Session started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Configuration\n",
    "passingMark = 15  # Adjust as needed\n",
    "prefix = \"VTC Test\"\n",
    "paths = setup_paths(prefix, \"sample\")\n",
    "\n",
    "# Extract commonly used paths\n",
    "pdf_file = paths[\"pdf_file\"]\n",
    "name_list_file = paths[\"name_list_file\"]\n",
    "base_path = paths[\"base_path\"]\n",
    "base_path_images = paths[\"base_path_images\"]\n",
    "base_path_annotations = paths[\"base_path_annotations\"]\n",
    "base_path_questions = paths[\"base_path_questions\"]\n",
    "base_path_marked_images = paths[\"base_path_marked_images\"]\n",
    "base_path_marked_pdfs = paths[\"base_path_marked_pdfs\"]\n",
    "base_path_marked_scripts = paths[\"base_path_marked_scripts\"]\n",
    "CACHE_DIR = paths.get(\"cache_dir\", \"../cache\")\n",
    "\n",
    "# Create all necessary directories\n",
    "create_directories(paths)\n",
    "\n",
    "print(\"‚úì Paths configured and directories created\")\n",
    "\n",
    "# Metadata questions that should be excluded from answer analysis\n",
    "METADATA_QUESTIONS = [\"NAME\", \"ID\", \"CLASS\"]\n",
    "\n",
    "print(\"üí° Metadata questions (NAME, ID, CLASS) will be excluded from answer analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4ca495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust score report generation with validation and analytics\n",
    "\n",
    "def generate_score_report():\n",
    "    \"\"\"Generate comprehensive score report with validation and analytics.\"\"\"\n",
    "    logger.info(\"üìä Generating score report...\")\n",
    "    try:\n",
    "        name_list_df = pd.read_excel(name_list_file, sheet_name=\"Name List\")\n",
    "        id_col = next((col for col in name_list_df.columns if col.lower() == \"id\"), None)\n",
    "        name_col = next(\n",
    "            (col for col in name_list_df.columns if col.lower() in {\"name\", \"student name\", \"student_name\"}),\n",
    "            None,\n",
    "        )\n",
    "        if id_col is None or name_col is None:\n",
    "            raise ValueError(\"Name list must contain ID and NAME columns.\")\n",
    "\n",
    "        name_map = (\n",
    "            name_list_df.assign(**{id_col: name_list_df[id_col].astype(str)})\n",
    "            .set_index(id_col)[name_col]\n",
    "            .astype(str)\n",
    "            .to_dict()\n",
    "        )\n",
    "        logger.info(f\"‚úì Loaded {len(name_map)} student names from name list\")\n",
    "\n",
    "        pageToStudentId, numberOfPage, getStudentId = build_student_id_mapping(\n",
    "            base_path_questions, base_path_annotations\n",
    "        )\n",
    "        logger.info(f\"‚úì Built student ID mapping for {numberOfPage} pages\")\n",
    "\n",
    "        questionAndMarks = {}\n",
    "        questions_processed = 0\n",
    "        for path, _, files in os.walk(base_path_questions):\n",
    "            for file in files:\n",
    "                if file == \"mark.json\":\n",
    "                    question = path[len(base_path_questions) + 1 :]\n",
    "                    try:\n",
    "                        with open(os.path.join(path, file), \"r\", encoding=\"utf-8\") as f:\n",
    "                            data = json.load(f)\n",
    "                        marks = {}\n",
    "                        for item in data:\n",
    "                            studentId = getStudentId(int(item[\"id\"]))\n",
    "                            marks[studentId] = (\n",
    "                                item[\"overridedMark\"] if item[\"overridedMark\"] != \"\" else item[\"mark\"]\n",
    "                            )\n",
    "                        questionAndMarks[question] = marks\n",
    "                        questions_processed += 1\n",
    "                        logger.info(f\"‚úì Processed marks for {question}: {len(marks)} students\")\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"‚ùå Failed to process marks for {question}: {e}\")\n",
    "                        continue\n",
    "\n",
    "        logger.info(f\"‚úì Processed marks from {questions_processed} questions\")\n",
    "        if not questionAndMarks:\n",
    "            raise ValueError(\"No question marks were processed.\")\n",
    "       \n",
    "        marksDf = pd.DataFrame(questionAndMarks)\n",
    "\n",
    "        # Reorder columns: ID, NAME, CLASS first, then questions sorted\n",
    "        base_cols = [\"ID\", \"NAME\", \"CLASS\"]\n",
    "        question_cols = [\n",
    "            col\n",
    "            for col in sorted(marksDf.columns)\n",
    "            if col not in base_cols\n",
    "        ]\n",
    "        marksDf = marksDf[base_cols + question_cols]\n",
    "\n",
    "        marksDf[\"NAME\"] = marksDf[\"ID\"].map(name_map).fillna(marksDf[\"NAME\"])\n",
    "\n",
    "        \n",
    "\n",
    "        # Calculate total marks from question columns only\n",
    "        marksDf[\"Marks\"] = (\n",
    "            marksDf.loc[:, ~marksDf.columns.isin([\"ID\", \"NAME\", \"CLASS\"])]\n",
    "            .apply(pd.to_numeric, errors=\"coerce\")\n",
    "            .sum(axis=1)\n",
    "        )\n",
    "\n",
    "        invalid_marks = marksDf[marksDf[\"Marks\"].isna()]\n",
    "        if not invalid_marks.empty:\n",
    "            logger.warning(f\"Found {len(invalid_marks)} students with invalid marks\")\n",
    "\n",
    "        logger.info(f\"‚úì Generated marks report for {len(marksDf)} students\")\n",
    "        logger.info(f\"  Average score: {marksDf['Marks'].mean():.2f}\")\n",
    "        logger.info(f\"  Score range: {marksDf['Marks'].min():.1f} - {marksDf['Marks'].max():.1f}\")\n",
    "\n",
    "        return marksDf\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Score report generation failed: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "marksDf = generate_score_report()\n",
    "display(marksDf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust scored scripts creation with comprehensive validationdef create_scored_scripts():    \"\"\"Create scored scripts with validation and error handling\"\"\"    print(\"üìÑ Creating scored scripts...\")        try:        # Copy raw images to marked folder with validation        if os.path.exists(base_path_marked_images):            shutil.rmtree(base_path_marked_images)                copied_path = shutil.copytree(base_path_images, base_path_marked_images)                # Validate copy operation        original_files = len([f for f in os.listdir(base_path_images) if f.endswith('.jpg')])        copied_files = len([f for f in os.listdir(base_path_marked_images) if f.endswith('.jpg')])                if original_files != copied_files:            raise Exception(f\"Image copy validation failed: {original_files} original vs {copied_files} copied\")                logger.info(f\"‚úì Copied {copied_files} images to marked folder\")                # Load and validate annotations        annotations_path = base_path_annotations + \"annotations.json\"        with open(annotations_path, \"r\") as f:             annotations = json.load(f)                # Flatten annotations to list with validation        annotations_list = []        for page in annotations:            for annotation in annotations[page]:                annotation[\"page\"] = int(page)                # x to left, y to top                annotation[\"left\"] = annotation[\"x\"]                annotation[\"top\"] = annotation[\"y\"]                annotation.pop(\"x\")                annotation.pop(\"y\")                annotations_list.append(annotation)                # Convert annotations_list to dict with key with label        annotations_dict = {}        for annotation in annotations_list:            annotations_dict[annotation[\"label\"]] = annotation                logger.info(f\"‚úì Processed {len(annotations_dict)} annotations\")                # Build student ID to page mapping        studentIdToPage = {}        with open(os.path.join(base_path_questions, \"ID\", \"mark.json\")) as f:            data = json.load(f)            for i in data:                studentId = i[\"overridedMark\"] if i[\"overridedMark\"] != \"\" else i[\"mark\"]                studentIdToPage[studentId] = int(i[\"id\"])                logger.info(f\"‚úì Built student-to-page mapping for {len(studentIdToPage)} students\")                # Add marks to images with progress tracking        marksDf_list = marksDf.to_dict(orient=\"records\")                progress = IntProgress(min=0, max=len(marksDf_list), description='Adding marks')        display(progress)                processed_students = 0        failed_students = []                for student in marksDf_list:            try:                first_page = studentIdToPage[student[\"ID\"]]                                for annotation in annotations_dict:                    value = student[annotation]                    if annotation == \"ID\":                        value = value + \" Marks: \" + str(student[\"Marks\"])                                        x = annotations_dict[annotation][\"left\"]                    y = annotations_dict[annotation][\"top\"]                    page = first_page + annotations_dict[annotation][\"page\"]                                      image_path = base_path_marked_images + str(page) + \".jpg\"                                        if not os.path.exists(image_path):                        logger.warning(f\"Image not found: {image_path}\")                        continue                                        # Add text to image with error handling                    try:                        img = cv2.imread(image_path)                        if img is None:                            logger.warning(f\"Failed to load image: {image_path}\")                            continue                                                textSize = cv2.getTextSize(text=str(value), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=1, thickness=2)                        height = textSize[0][1]                        cv2.putText(img, str(value), (x, y + height), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)                        cv2.imwrite(image_path, img)                                            except Exception as e:                        logger.warning(f\"Failed to add text to {image_path}: {e}\")                        continue                                processed_students += 1                            except Exception as e:                logger.error(f\"Failed to process student {student['ID']}: {e}\")                failed_students.append(student['ID'])                        progress.value += 1                logger.info(f\"‚úì Added marks to images for {processed_students} students\")        if failed_students:            logger.warning(f\"Failed to process {len(failed_students)} students: {failed_students}\")                return studentIdToPage, processed_students, failed_students            except Exception as e:        logger.error(f\"‚ùå Scored scripts creation failed: {e}\")        raise# Create scored scriptsstudentIdToPage, processed_students, failed_students = create_scored_scripts()# Create scored scriptsstudentIdToPage, processed_students, failed_students = create_scored_scripts()\n",
    "def create_scored_scripts():\n",
    "    \"\"\"Create scored scripts with validation and error handling.\"\"\"\n",
    "    print(\"üìÑ Creating scored scripts...\")\n",
    "\n",
    "    try:\n",
    "        if os.path.exists(base_path_marked_images):\n",
    "            shutil.rmtree(base_path_marked_images)\n",
    "\n",
    "        shutil.copytree(base_path_images, base_path_marked_images)\n",
    "\n",
    "        original_files = len([f for f in os.listdir(base_path_images) if f.endswith(\".jpg\")])\n",
    "        copied_files = len([f for f in os.listdir(base_path_marked_images) if f.endswith(\".jpg\")])\n",
    "        if original_files != copied_files:\n",
    "            raise Exception(f\"Image copy validation failed: {original_files} original vs {copied_files} copied\")\n",
    "\n",
    "        logger.info(f\"‚úì Copied {copied_files} images to marked folder\")\n",
    "\n",
    "        annotations_path = os.path.join(base_path_annotations, \"annotations.json\")\n",
    "        with open(annotations_path, \"r\") as f:\n",
    "            annotations = json.load(f)\n",
    "\n",
    "        annotations_dict = {}\n",
    "        for page, page_ann in annotations.items():\n",
    "            for annotation in page_ann:\n",
    "                annotation = annotation.copy()\n",
    "                annotation[\"page\"] = int(page)\n",
    "                annotation[\"left\"] = annotation.pop(\"x\")\n",
    "                annotation[\"top\"] = annotation.pop(\"y\")\n",
    "                annotations_dict[annotation[\"label\"]] = annotation\n",
    "\n",
    "        logger.info(f\"‚úì Processed {len(annotations_dict)} annotations\")\n",
    "\n",
    "        studentIdToPage = {}\n",
    "        id_mark_path = os.path.join(base_path_questions, \"ID\", \"mark.json\")\n",
    "        with open(id_mark_path) as f:\n",
    "            data = json.load(f)\n",
    "        for i in data:\n",
    "            studentId = i[\"overridedMark\"] if i[\"overridedMark\"] != \"\" else i[\"mark\"]\n",
    "            studentIdToPage[str(studentId)] = int(i[\"id\"])\n",
    "        logger.info(f\"‚úì Built student-to-page mapping for {len(studentIdToPage)} students\")\n",
    "\n",
    "        marksDf_list = marksDf.to_dict(orient=\"records\")\n",
    "        progress = IntProgress(min=0, max=len(marksDf_list), description=\"Adding marks\")\n",
    "        display(progress)\n",
    "\n",
    "        processed_students = 0\n",
    "        failed_students = []\n",
    "\n",
    "        for student in marksDf_list:\n",
    "            try:\n",
    "                first_page = studentIdToPage.get(str(student[\"ID\"]))\n",
    "                if first_page is None:\n",
    "                    logger.warning(f\"No page mapping for student {student['ID']}\")\n",
    "                    failed_students.append(student[\"ID\"])\n",
    "                    progress.value += 1\n",
    "                    continue\n",
    "\n",
    "                for label, annotation in annotations_dict.items():\n",
    "                    value = student.get(label, \"\")\n",
    "                    if label == \"ID\":\n",
    "                        value = f\"{value} Marks: {student.get('Marks', '')}\"\n",
    "                    if pd.isna(value):\n",
    "                        continue\n",
    "\n",
    "                    x, y = annotation[\"left\"], annotation[\"top\"]\n",
    "                    page = first_page + annotation[\"page\"]\n",
    "                    image_path = os.path.join(base_path_marked_images, f\"{page}.jpg\")\n",
    "\n",
    "                    if not os.path.exists(image_path):\n",
    "                        logger.warning(f\"Image not found: {image_path}\")\n",
    "                        continue\n",
    "\n",
    "                    try:\n",
    "                        img = cv2.imread(image_path)\n",
    "                        if img is None:\n",
    "                            logger.warning(f\"Failed to load image: {image_path}\")\n",
    "                            continue\n",
    "\n",
    "                        text = str(value)\n",
    "                        (text_width, text_height), baseline = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 1, 2)\n",
    "                        cv2.putText(img, text, (x, y + text_height), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "                        cv2.imwrite(image_path, img)\n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"Failed to add text to {image_path}: {e}\")\n",
    "                        continue\n",
    "\n",
    "                processed_students += 1\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to process student {student.get('ID')}: {e}\")\n",
    "                failed_students.append(student.get(\"ID\"))\n",
    "            finally:\n",
    "                progress.value += 1\n",
    "\n",
    "        logger.info(f\"‚úì Added marks to images for {processed_students} students\")\n",
    "        if failed_students:\n",
    "            logger.warning(f\"Failed to process {len(failed_students)} students: {failed_students}\")\n",
    "\n",
    "        return studentIdToPage, processed_students, failed_students\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Scored scripts creation failed: {e}\")\n",
    "        raise\n",
    "\n",
    "studentIdToPage, processed_students, failed_students = create_scored_scripts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust PDF generation with comprehensive validation\n",
    "def generate_pdfs(studentIdToPage, numberOfPage):\n",
    "    \"\"\"Generate individual PDFs with validation and error handling\"\"\"\n",
    "    print(\"üìÑ Generating individual PDFs...\")\n",
    "    \n",
    "    try:\n",
    "        marksDf_list = marksDf.to_dict(orient=\"records\")\n",
    "        \n",
    "        pdf_generation_stats = {\n",
    "            'successful': 0,\n",
    "            'failed': 0,\n",
    "            'errors': []\n",
    "        }\n",
    "        \n",
    "        for student in marksDf_list:\n",
    "            try:\n",
    "                studentId = str(student[\"ID\"])\n",
    "                first_page = studentIdToPage.get(studentId)\n",
    "                \n",
    "                if first_page is None:\n",
    "                    error_msg = f\"No page mapping found for student {studentId}\"\n",
    "                    logger.error(error_msg)\n",
    "                    pdf_generation_stats['errors'].append(error_msg)\n",
    "                    pdf_generation_stats['failed'] += 1\n",
    "                    continue\n",
    "                \n",
    "                last_page = first_page + numberOfPage - 1\n",
    "                \n",
    "                logger.info(f\"Processing PDF for {studentId}: pages {first_page}-{last_page}\")\n",
    "                \n",
    "                pdf_path = os.path.join(base_path_marked_pdfs, f\"{studentId}.pdf\")\n",
    "                \n",
    "                # Validate all required images exist\n",
    "                image_paths = [os.path.join(base_path_marked_images, f\"{i}.jpg\") for i in range(first_page, last_page + 1)]\n",
    "                missing_images = [path for path in image_paths if not os.path.exists(path)]\n",
    "                \n",
    "                if missing_images:\n",
    "                    error_msg = f\"Missing images for {studentId}: {len(missing_images)} files\"\n",
    "                    logger.error(error_msg)\n",
    "                    pdf_generation_stats['errors'].append(error_msg)\n",
    "                    pdf_generation_stats['failed'] += 1\n",
    "                    continue\n",
    "                \n",
    "                # Load and validate images\n",
    "                try:\n",
    "                    images = []\n",
    "                    for path in image_paths:\n",
    "                        img = Image.open(path)\n",
    "                        if img.mode != 'RGB':\n",
    "                            img = img.convert('RGB')\n",
    "                        images.append(img)\n",
    "                    \n",
    "                    # Create PDF with validation\n",
    "                    if images:\n",
    "                        images[0].save(pdf_path, save_all=True, append_images=images[1:] if len(images) > 1 else [])\n",
    "                        \n",
    "                        # Validate PDF creation\n",
    "                        if os.path.exists(pdf_path) and os.path.getsize(pdf_path) > 0:\n",
    "                            pdf_generation_stats['successful'] += 1\n",
    "                            logger.info(f\"‚úì Created PDF for {studentId}: {os.path.getsize(pdf_path)} bytes\")\n",
    "                        else:\n",
    "                            error_msg = f\"PDF creation failed for {studentId}: file not created or empty\"\n",
    "                            logger.error(error_msg)\n",
    "                            pdf_generation_stats['errors'].append(error_msg)\n",
    "                            pdf_generation_stats['failed'] += 1\n",
    "                    else:\n",
    "                        error_msg = f\"No images loaded for {studentId}\"\n",
    "                        logger.error(error_msg)\n",
    "                        pdf_generation_stats['errors'].append(error_msg)\n",
    "                        pdf_generation_stats['failed'] += 1\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    error_msg = f\"Image processing failed for {studentId}: {e}\"\n",
    "                    logger.error(error_msg)\n",
    "                    pdf_generation_stats['errors'].append(error_msg)\n",
    "                    pdf_generation_stats['failed'] += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                error_msg = f\"PDF generation failed for {studentId}: {e}\"\n",
    "                logger.error(error_msg)\n",
    "                pdf_generation_stats['errors'].append(error_msg)\n",
    "                pdf_generation_stats['failed'] += 1\n",
    "        \n",
    "        # Display generation summary\n",
    "        total = pdf_generation_stats['successful'] + pdf_generation_stats['failed']\n",
    "        print(f\"\\nüìä PDF Generation Summary:\")\n",
    "        print(f\"   Successful: {pdf_generation_stats['successful']}\")\n",
    "        print(f\"   Failed: {pdf_generation_stats['failed']}\")\n",
    "        if total > 0:\n",
    "            print(f\"   Success rate: {pdf_generation_stats['successful']/total*100:.1f}%\")\n",
    "        \n",
    "        if pdf_generation_stats['errors']:\n",
    "            print(f\"\\n‚ùå Errors encountered:\")\n",
    "            for error in pdf_generation_stats['errors'][:5]:  # Show first 5 errors\n",
    "                print(f\"   ‚Ä¢ {error}\")\n",
    "            if len(pdf_generation_stats['errors']) > 5:\n",
    "                print(f\"   ... and {len(pdf_generation_stats['errors'])-5} more errors\")\n",
    "        \n",
    "        return pdf_generation_stats\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå PDF generation failed: {e}\")\n",
    "        raise\n",
    "\n",
    "# Generate PDFs\n",
    "# Get numberOfPage from the student ID mapping\n",
    "pageToStudentId, numberOfPage, getStudentId = build_student_id_mapping(\n",
    "    base_path_questions, base_path_annotations\n",
    ")\n",
    "pdf_stats = generate_pdfs(studentIdToPage, numberOfPage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust sample generation with comprehensive validation\n",
    "def generate_samples():\n",
    "    \"\"\"Generate sample PDFs with validation and error handling\"\"\"\n",
    "    print(\"üìö Generating sample collections...\")\n",
    "    \n",
    "    try:\n",
    "        # Create combined PDF of all scripts\n",
    "        writer = PdfWriter()\n",
    "        \n",
    "        pdf_files_added = 0\n",
    "        for path, currentDirectory, files in os.walk(base_path_marked_pdfs):\n",
    "            for file in files:\n",
    "                if file.endswith(\".pdf\"):\n",
    "                    pdf_path = os.path.join(path, file)\n",
    "                    try:\n",
    "                        reader = PdfReader(pdf_path)\n",
    "                        for page in reader.pages:\n",
    "                            writer.add_page(page)\n",
    "                        pdf_files_added += 1\n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"Failed to add {pdf_path} to combined PDF: {e}\")\n",
    "        \n",
    "        combined_path = base_path_marked_scripts + \"all.pdf\"\n",
    "        with open(combined_path, \"wb\") as f:\n",
    "            writer.write(f)\n",
    "        \n",
    "        logger.info(f\"‚úì Created combined PDF with {pdf_files_added} individual PDFs\")\n",
    "        \n",
    "        # Generate stratified samples with validation\n",
    "        sampling = marksDf.sort_values(by=[\"Marks\"], ascending=False)[\"Marks\"]\n",
    "        \n",
    "        from_directory = os.path.join(os.getcwd(), \"..\", \"templates\", \"pdf\")\n",
    "        \n",
    "        # Validate template files exist\n",
    "        template_files = {\n",
    "            'good': os.path.join(from_directory, \"Good.pdf\"),\n",
    "            'average': os.path.join(from_directory, \"Average.pdf\"),\n",
    "            'weak': os.path.join(from_directory, \"Weak.pdf\")\n",
    "        }\n",
    "        \n",
    "        missing_templates = [name for name, path in template_files.items() if not os.path.exists(path)]\n",
    "        if missing_templates:\n",
    "            logger.warning(f\"Missing template files: {missing_templates}\")\n",
    "            logger.info(\"Creating sample without templates...\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            goodPage = PdfReader(template_files['good'])\n",
    "            averagePage = PdfReader(template_files['average'])\n",
    "            weakPage = PdfReader(template_files['weak'])\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to load template files: {e}\")\n",
    "            logger.info(\"Creating sample without templates...\")\n",
    "            return\n",
    "        \n",
    "        def get_scripts_pdf(df):\n",
    "            return list(map(lambda rowNumber: base_path_marked_pdfs + rowNumber + \".pdf\", df.index))\n",
    "        \n",
    "        def take_sample(n, sampling, suffix=\"\"):\n",
    "            \"\"\"Robust sample generation with validation\"\"\"\n",
    "            try:\n",
    "                if len(sampling) < 3 * n:\n",
    "                    n = max(1, int(len(sampling) / 3))\n",
    "                    logger.warning(f\"Adjusted sample size to {n} due to insufficient data\")\n",
    "                \n",
    "                good = sampling.head(n)\n",
    "                weak = sampling.tail(n)\n",
    "                median = int(len(sampling) / 2)\n",
    "                take = max(1, int(n / 2))\n",
    "                average = sampling.iloc[median - take : median + take]\n",
    "                \n",
    "                writer = PdfWriter()\n",
    "                \n",
    "                # Add template pages and student PDFs with validation\n",
    "                for page in goodPage.pages:\n",
    "                    writer.add_page(page)\n",
    "                \n",
    "                for pdf in get_scripts_pdf(good):\n",
    "                    if os.path.exists(pdf):\n",
    "                        try:\n",
    "                            reader = PdfReader(pdf)\n",
    "                            for page in reader.pages:\n",
    "                                writer.add_page(page)\n",
    "                        except Exception as e:\n",
    "                            logger.warning(f\"Failed to add {pdf}: {e}\")\n",
    "                \n",
    "                for page in averagePage.pages:\n",
    "                    writer.add_page(page)\n",
    "                \n",
    "                for pdf in get_scripts_pdf(average):\n",
    "                    if os.path.exists(pdf):\n",
    "                        try:\n",
    "                            reader = PdfReader(pdf)\n",
    "                            for page in reader.pages:\n",
    "                                writer.add_page(page)\n",
    "                        except Exception as e:\n",
    "                            logger.warning(f\"Failed to add {pdf}: {e}\")\n",
    "                \n",
    "                for page in weakPage.pages:\n",
    "                    writer.add_page(page)\n",
    "                \n",
    "                for pdf in get_scripts_pdf(weak):\n",
    "                    if os.path.exists(pdf):\n",
    "                        try:\n",
    "                            reader = PdfReader(pdf)\n",
    "                            for page in reader.pages:\n",
    "                                writer.add_page(page)\n",
    "                        except Exception as e:\n",
    "                            logger.warning(f\"Failed to add {pdf}: {e}\")\n",
    "                \n",
    "                fileName = base_path_marked_scripts + \"sampleOf\" + str(n) + suffix + \".pdf\"\n",
    "                with open(fileName, \"wb\") as f:\n",
    "                    writer.write(f)\n",
    "                \n",
    "                # Validate sample creation\n",
    "                if os.path.exists(fileName) and os.path.getsize(fileName) > 0:\n",
    "                    logger.info(f\"‚úì Created sample: {fileName} ({os.path.getsize(fileName)} bytes)\")\n",
    "                else:\n",
    "                    logger.error(f\"‚ùå Failed to create sample: {fileName}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.error(f\"‚ùå Sample generation failed for n={n}, suffix={suffix}: {e}\")\n",
    "        \n",
    "        # Generate different sample sizes\n",
    "        take_sample(3, sampling)\n",
    "        take_sample(5, sampling)\n",
    "        \n",
    "        # Generate samples for passing students only\n",
    "        passing_sampling = sampling.where(lambda x: x > passingMark).dropna()\n",
    "        if len(passing_sampling) >= 3:\n",
    "            take_sample(3, passing_sampling, \"_only_pass\")\n",
    "        if len(passing_sampling) >= 5:\n",
    "            take_sample(5, passing_sampling, \"_only_pass\")\n",
    "        else:\n",
    "            logger.warning(f\"Insufficient passing students ({len(passing_sampling)}) for passing-only samples\")\n",
    "        \n",
    "        logger.info(\"‚úì Sample generation completed\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Sample generation failed: {e}\")\n",
    "        raise\n",
    "\n",
    "# Generate samples\n",
    "generate_samples()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f896f4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust answer collection and reasoning with metadata exclusion\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def clean_answer_text(val: str) -> str:\n",
    "    \"\"\"Strip leading numbering and drop standalone question labels like Q2.\"\"\"\n",
    "    if not isinstance(val, str):\n",
    "        return val\n",
    "    lines = [ln.strip() for ln in str(val).splitlines()]\n",
    "    cleaned = []\n",
    "    for ln in lines:\n",
    "        ln = re.sub(r\"^\\s*\\d+\\s*[\\.|\\)]\\s*\", \"\", ln)\n",
    "        if re.fullmatch(r\"q\\d+\", ln, flags=re.IGNORECASE):\n",
    "            continue\n",
    "        if ln:\n",
    "            cleaned.append(ln)\n",
    "    return \"\\n\".join(cleaned).strip()\n",
    "\n",
    "def collect_answers_and_reasoning():\n",
    "    \"\"\"Gather per-student answers and model reasoning from each question's CSV and pivot to a mark-style wide format.\"\"\"\n",
    "    print(\"üìù Collecting answers and reasoning from question data...\")\n",
    "    \n",
    "    answer_rows = []\n",
    "    reasoning_rows = []\n",
    "    questions_processed = 0\n",
    "\n",
    "    for path, currentDirectory, files in os.walk(base_path_questions):\n",
    "        if \"data.csv\" not in files:\n",
    "            continue\n",
    "\n",
    "        question = path[len(base_path_questions) + 1 :]\n",
    "        \n",
    "        # Skip metadata questions (NAME, ID, CLASS)\n",
    "        if question in METADATA_QUESTIONS:\n",
    "            logger.info(f\"‚è≠Ô∏è Skipping metadata question: {question}\")\n",
    "            continue\n",
    "            \n",
    "        data_path = os.path.join(path, \"data.csv\")\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(data_path)\n",
    "            if \"page\" not in df.columns:\n",
    "                logger.warning(f\"No 'page' column in {question} data.csv\")\n",
    "                continue\n",
    "\n",
    "            # Map scanned page back to student ID using the existing helper\n",
    "            df[\"StudentID\"] = df[\"page\"].apply(\n",
    "                lambda p: getStudentId(int(str(p).split(\".\")[0])) if pd.notna(p) else None\n",
    "            )\n",
    "\n",
    "            for _, row in df.iterrows():\n",
    "                sid = row.get(\"StudentID\")\n",
    "                if sid is None:\n",
    "                    continue\n",
    "\n",
    "                raw_answer = row.get(\"Answer\", \"\")\n",
    "                answer_val = clean_answer_text(raw_answer)\n",
    "                source_page = row.get(\"page\", \"\")\n",
    "                row_number = row.get(\"RowNumber\", \"\")\n",
    "\n",
    "                answer_rows.append(\n",
    "                    {\n",
    "                        \"ID\": str(sid),\n",
    "                        \"Question\": question,\n",
    "                        \"Answer\": answer_val,\n",
    "                        \"SourcePage\": source_page,\n",
    "                        \"RowNumber\": row_number,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                reasoning_rows.append(\n",
    "                    {\n",
    "                        \"ID\": str(sid),\n",
    "                        \"Question\": question,\n",
    "                        \"Reasoning\": row.get(\"Reasoning\", \"\"),\n",
    "                        \"Similarity\": row.get(\"Similarity\", \"\"),\n",
    "                        \"ModelMark\": row.get(\"Mark\", \"\"),\n",
    "                        \"Answer\": answer_val,\n",
    "                        \"SourcePage\": source_page,\n",
    "                        \"RowNumber\": row_number,\n",
    "                    }\n",
    "                )\n",
    "            \n",
    "            questions_processed += 1\n",
    "            logger.info(f\"‚úì Processed answers for {question}: {len(df)} entries\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Failed to process {question}: {e}\")\n",
    "            continue\n",
    "\n",
    "    answers_df = pd.DataFrame(answer_rows)\n",
    "    reasoning_df = pd.DataFrame(reasoning_rows)\n",
    "\n",
    "    if not answers_df.empty:\n",
    "        answers_df = answers_df.sort_values(\n",
    "            by=[\"ID\", \"Question\", \"SourcePage\", \"RowNumber\"]\n",
    "        ).reset_index(drop=True)\n",
    "    if not reasoning_df.empty:\n",
    "        reasoning_df = reasoning_df.sort_values(\n",
    "            by=[\"ID\", \"Question\", \"SourcePage\", \"RowNumber\"]\n",
    "        ).reset_index(drop=True)\n",
    "\n",
    "    # Preserve ID/NAME/CLASS to match marks layout\n",
    "    meta_cols = [\"ID\", \"NAME\", \"CLASS\"]\n",
    "    student_meta = marksDf[meta_cols].drop_duplicates().set_index(\"ID\")\n",
    "\n",
    "    # Keep question ordering aligned with marks sheet (excluding metadata)\n",
    "    question_cols = [\n",
    "        col\n",
    "        for col in marksDf.columns\n",
    "        if col not in [\"ID\", \"NAME\", \"CLASS\", \"Marks\"] and col not in METADATA_QUESTIONS\n",
    "    ]\n",
    "\n",
    "    # Wide answers: one row per student, one column per question\n",
    "    answers_wide = student_meta.copy()\n",
    "    if not answers_df.empty:\n",
    "        answers_pivot = answers_df.pivot_table(\n",
    "            index=\"ID\", columns=\"Question\", values=\"Answer\", aggfunc=\"first\"\n",
    "        )\n",
    "        answers_pivot = answers_pivot.reindex(columns=question_cols)\n",
    "        answers_wide = answers_wide.join(answers_pivot)\n",
    "        answers_wide = answers_wide.reset_index()\n",
    "\n",
    "    # Wide reasoning: only the reasoning text per question (matches marks layout)\n",
    "    reasoning_wide = student_meta.copy()\n",
    "    if not reasoning_df.empty:\n",
    "        reasoning_pivot = reasoning_df.pivot_table(\n",
    "            index=\"ID\", columns=\"Question\", values=\"Reasoning\", aggfunc=\"first\"\n",
    "        )\n",
    "        reasoning_pivot = reasoning_pivot.reindex(columns=question_cols)\n",
    "        reasoning_wide = reasoning_wide.join(reasoning_pivot)\n",
    "        reasoning_wide = reasoning_wide.reset_index()\n",
    "\n",
    "    logger.info(f\"‚úì Collected answers and reasoning from {questions_processed} questions\")\n",
    "    logger.info(f\"  Answer entries: {len(answers_df)}\")\n",
    "    logger.info(f\"  Reasoning entries: {len(reasoning_df)}\")\n",
    "    \n",
    "    return answers_wide, reasoning_wide, answers_df, reasoning_df\n",
    "\n",
    "# Collect answers and reasoning\n",
    "answers_sheet, reasoning_sheet, answers_raw, reasoning_raw = collect_answers_and_reasoning()\n",
    "\n",
    "# Collect answers and reasoning\n",
    "answers_sheet, reasoning_sheet, answers_raw, reasoning_raw = collect_answers_and_reasoning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b0a7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust Excel report generation with comprehensive analytics\n",
    "def generate_comprehensive_excel_report():\n",
    "    \"\"\"Generate comprehensive Excel report with multiple sheets and analytics\"\"\"\n",
    "    print(\"üìä Generating comprehensive Excel reports...\")\n",
    "    \n",
    "    try:\n",
    "        details_report_path = base_path_marked_scripts + \"details_score_report.xlsx\"\n",
    "        \n",
    "        # Multi-sheet Excel: marks, answers (wide), reasoning (wide) + raw long-form for audit\n",
    "        with pd.ExcelWriter(details_report_path, engine='openpyxl') as writer:\n",
    "            # Main sheets\n",
    "            marksDf.to_excel(writer, sheet_name=\"Marks\", index=False)\n",
    "            \n",
    "            if not answers_sheet.empty:\n",
    "                answers_sheet.to_excel(writer, sheet_name=\"Answers\", index=False)\n",
    "            else:\n",
    "                pd.DataFrame({\"Note\": [\"No answer data available\"]}).to_excel(writer, sheet_name=\"Answers\", index=False)\n",
    "            \n",
    "            if not reasoning_sheet.empty:\n",
    "                reasoning_sheet.to_excel(writer, sheet_name=\"Reasoning\", index=False)\n",
    "            else:\n",
    "                pd.DataFrame({\"Note\": [\"No reasoning data available\"]}).to_excel(writer, sheet_name=\"Reasoning\", index=False)\n",
    "            \n",
    "            # Raw data sheets for audit\n",
    "            if not answers_raw.empty:\n",
    "                answers_raw.to_excel(writer, sheet_name=\"AnswersRaw\", index=False)\n",
    "            if not reasoning_raw.empty:\n",
    "                reasoning_raw.to_excel(writer, sheet_name=\"ReasoningRaw\", index=False)\n",
    "\n",
    "        # Lightweight summary sheet\n",
    "        summary_path = base_path_marked_scripts + \"score_report.xlsx\"\n",
    "        marksDf[[\"ID\", \"NAME\", \"CLASS\", \"Marks\"]].to_excel(summary_path, index=False)\n",
    "\n",
    "        logger.info(f\"‚úì Generated comprehensive Excel report: {details_report_path}\")\n",
    "        logger.info(f\"‚úì Generated summary Excel report: {summary_path}\")\n",
    "        \n",
    "        return details_report_path, summary_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Excel report generation failed: {e}\")\n",
    "        raise\n",
    "\n",
    "# Generate comprehensive Excel reports\n",
    "details_report_path, summary_report_path = generate_comprehensive_excel_report()\n",
    "print(f\"üìÑ Excel reports saved:\")\n",
    "print(f\"   ‚Ä¢ Detailed: {os.path.basename(details_report_path)}\")\n",
    "print(f\"   ‚Ä¢ Summary: {os.path.basename(summary_report_path)}\")\n",
    "\n",
    "# Generate comprehensive Excel reports\n",
    "details_report_path, summary_report_path = generate_comprehensive_excel_report()\n",
    "print(f\"üìÑ Excel reports saved:\")\n",
    "print(f\"   ‚Ä¢ Detailed: {os.path.basename(details_report_path)}\")\n",
    "print(f\"   ‚Ä¢ Summary: {os.path.basename(summary_report_path)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a682ac6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust Gemini Performance Report Generation\n",
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "from grading_utils import (\n",
    "    init_gemini_client,\n",
    "    get_cache_key,\n",
    "    get_from_cache,\n",
    "    save_to_cache,\n",
    "    create_gemini_config,\n",
    ")\n",
    "\n",
    "def generate_gemini_performance_reports():\n",
    "    \"\"\"Generate AI-powered individual student performance reports\"\"\"\n",
    "    print(\"ü§ñ Generating Gemini-powered performance reports...\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize Gemini client\n",
    "        client = init_gemini_client()\n",
    "        \n",
    "        # Load marking scheme for context\n",
    "        marking_scheme_file = paths[\"marking_scheme_file\"]\n",
    "        \n",
    "        if not os.path.exists(marking_scheme_file):\n",
    "            logger.warning(\"Marking scheme file not found, generating reports without detailed context\")\n",
    "            question_meta = {}\n",
    "        else:\n",
    "            try:\n",
    "                marking_scheme_df = pd.read_excel(marking_scheme_file, sheet_name=\"Marking Scheme\")\n",
    "                required_cols = {\"question_number\", \"question_text\", \"marking_scheme\", \"marks\"}\n",
    "                missing_cols = required_cols - set(marking_scheme_df.columns)\n",
    "                \n",
    "                if missing_cols:\n",
    "                    logger.warning(f\"Marking scheme missing columns: {missing_cols}\")\n",
    "                    question_meta = {}\n",
    "                else:\n",
    "                    def normalize_question_label(val):\n",
    "                        label = str(val).strip()\n",
    "                        if label.upper().startswith(\"Q\"):\n",
    "                            return label\n",
    "                        try:\n",
    "                            as_float = float(label)\n",
    "                            if as_float.is_integer():\n",
    "                                return f\"Q{int(as_float)}\"\n",
    "                        except Exception:\n",
    "                            pass\n",
    "                        return f\"Q{label}\"\n",
    "\n",
    "                    marking_scheme_df[\"Question\"] = marking_scheme_df[\"question_number\"].apply(normalize_question_label)\n",
    "                    question_meta = (\n",
    "                        marking_scheme_df\n",
    "                        .set_index(\"Question\")[\n",
    "                            [\"question_text\", \"marking_scheme\", \"marks\"]\n",
    "                        ]\n",
    "                        .to_dict(orient=\"index\")\n",
    "                    )\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to load marking scheme: {e}\")\n",
    "                question_meta = {}\n",
    "\n",
    "        # Question columns (excluding metadata)\n",
    "        question_cols = [col for col in marksDf.columns if col not in [\"ID\", \"NAME\", \"CLASS\", \"Marks\"] and col not in METADATA_QUESTIONS]\n",
    "        \n",
    "        # Build answers lookup\n",
    "        answers_lookup = {}\n",
    "        if not answers_sheet.empty:\n",
    "            for _, row in answers_sheet.iterrows():\n",
    "                answers_lookup[str(row[\"ID\"])] = {\n",
    "                    col: (\"\" if pd.isna(row.get(col, \"\")) else str(row.get(col, \"\")))\n",
    "                    for col in question_cols\n",
    "                }\n",
    "\n",
    "        cache_subdir = os.path.join(CACHE_DIR, \"performance_report\")\n",
    "\n",
    "        # Create Gemini config using utility function\n",
    "        config = create_gemini_config(temperature=0.35, top_p=0.9, max_output_tokens=1536)\n",
    "\n",
    "        def build_question_section(student_row, student_answers):\n",
    "            blocks = []\n",
    "            for q in question_cols:\n",
    "                meta = question_meta.get(q, {})\n",
    "                awarded_mark = student_row.get(q, \"\")\n",
    "                if pd.isna(awarded_mark):\n",
    "                    awarded_mark = \"\"\n",
    "                answer_text = student_answers.get(q, \"\")\n",
    "                blocks.append(\n",
    "                    f\"Question {q} (max {meta.get('marks', 'N/A')}):\\n\"\n",
    "                    f\"Prompt: {meta.get('question_text', 'N/A')}\\n\"\n",
    "                    f\"Marking scheme: {meta.get('marking_scheme', 'N/A')}\\n\"\n",
    "                    f\"Awarded mark: {awarded_mark}\\n\"\n",
    "                    f\"Student answer:\\n{answer_text or '[no answer captured]'}\"\n",
    "                )\n",
    "            return \"\\n\\n---\\n\\n\".join(blocks)\n",
    "\n",
    "        def generate_student_report(student_row):\n",
    "            sid = str(student_row.get(\"ID\", \"\"))\n",
    "            student_answers = answers_lookup.get(sid, {})\n",
    "            question_section = build_question_section(student_row, student_answers)\n",
    "            \n",
    "            prompt = f\"\"\"You are an instructor drafting a concise performance report.\n",
    "\n",
    "Student: {sid} - {student_row.get('NAME', '')} (Class: {student_row.get('CLASS', '')})\n",
    "Total score: {student_row.get('Marks', '')}\n",
    "\n",
    "Use the question details, marking schemes, awarded marks, and answers below:\n",
    "{question_section}\n",
    "\n",
    "Write:\n",
    "- 2-3 sentence overall summary of strengths and weaknesses.\n",
    "- One short bullet per question with actionable feedback tied to the marking scheme.\n",
    "- 2 concrete next-step study suggestions focused on the weakest skills.\n",
    "Keep it under 220 words and avoid restating the input verbatim.\"\"\"\n",
    "            \n",
    "            payload_hash = hashlib.sha256(question_section.encode(\"utf-8\")).hexdigest()\n",
    "            cache_key = get_cache_key(\n",
    "                \"performance_report\",\n",
    "                model=\"gemini-3-flash-preview\",\n",
    "                student_id=sid,\n",
    "                payload_hash=payload_hash,\n",
    "            )\n",
    "            \n",
    "            cached = get_from_cache(cache_key, cache_subdir)\n",
    "            if cached is not None:\n",
    "                return cached.get(\"report\", \"\")\n",
    "            \n",
    "            try:\n",
    "                response = client.models.generate_content(\n",
    "                    model=\"gemini-3-flash-preview\",\n",
    "                    contents=[{\"role\": \"user\", \"parts\": [{\"text\": prompt}]}],\n",
    "                    config=config,\n",
    "                )\n",
    "                report_text = response.text if response.text else \"\"\n",
    "                save_to_cache(cache_key, {\"report\": report_text}, cache_subdir)\n",
    "                return report_text\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to generate report for {sid}: {e}\")\n",
    "                return f\"Report generation failed: {e}\"\n",
    "\n",
    "        # Generate reports with progress tracking\n",
    "        performance_rows = []\n",
    "        progress = IntProgress(min=0, max=len(marksDf), description=\"AI Reports\")\n",
    "        display(progress)\n",
    "        \n",
    "        for _, row in marksDf.iterrows():\n",
    "            report_text = generate_student_report(row)\n",
    "            performance_rows.append(\n",
    "                {\n",
    "                    \"ID\": row.get(\"ID\", \"\"),\n",
    "                    \"NAME\": row.get(\"NAME\", \"\"),\n",
    "                    \"CLASS\": row.get(\"CLASS\", \"\"),\n",
    "                    \"Marks\": row.get(\"Marks\", \"\"),\n",
    "                    \"PerformanceReport\": report_text,\n",
    "                }\n",
    "            )\n",
    "            progress.value += 1\n",
    "\n",
    "        performance_df = pd.DataFrame(performance_rows)\n",
    "\n",
    "        # Append/replace Performance sheet inside the existing details_score_report.xlsx\n",
    "        writer_kwargs = {\"sheet_name\": \"Performance\", \"index\": False}\n",
    "        if os.path.exists(details_report_path):\n",
    "            with pd.ExcelWriter(\n",
    "                details_report_path,\n",
    "                mode=\"a\",\n",
    "                engine=\"openpyxl\",\n",
    "                if_sheet_exists=\"replace\",\n",
    "            ) as writer:\n",
    "                performance_df.to_excel(writer, **writer_kwargs)\n",
    "        else:\n",
    "            performance_df.to_excel(details_report_path, **writer_kwargs)\n",
    "\n",
    "        logger.info(f\"‚úì Generated {len(performance_df)} AI-powered performance reports\")\n",
    "        print(f\"ü§ñ Saved Performance sheet in: {os.path.basename(details_report_path)}\")\n",
    "        \n",
    "        return performance_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Gemini performance report generation failed: {e}\")\n",
    "        # Return empty dataframe to continue processing\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Generate Gemini performance reports\n",
    "performance_df = generate_gemini_performance_reports()\n",
    "if not performance_df.empty:\n",
    "    display(performance_df.head())\n",
    "\n",
    "# Generate Gemini performance reports\n",
    "performance_df = generate_gemini_performance_reports()\n",
    "if not performance_df.empty:\n",
    "    display(performance_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716d6cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust Class-Level Analytics and Overview\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def generate_class_analytics():\n",
    "    \"\"\"Generate comprehensive class-level analytics and visualizations\"\"\"\n",
    "    print(\"üìà Generating class-level analytics and visualizations...\")\n",
    "    \n",
    "    try:\n",
    "        # Ensure required variables are available\n",
    "        if 'marksDf' not in globals():\n",
    "            raise ValueError(\"marksDf not available - run score report generation first\")\n",
    "        \n",
    "        if 'paths' not in globals():\n",
    "            raise ValueError(\"paths not available - run setup cell first\")\n",
    "        \n",
    "        # Ensure cache directory exists\n",
    "        os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "        \n",
    "        # Create subdirectories for different cache types\n",
    "        for subdir in [\"grade_answer\", \"grade_moderator\", \"ocr\", \"performance_report\", \"class_overview_report\"]:\n",
    "            os.makedirs(os.path.join(CACHE_DIR, subdir), exist_ok=True)\n",
    "        \n",
    "        logger.info(f\"‚úì Cache directory structure created: {CACHE_DIR}\")\n",
    "        \n",
    "        # Ensure details_report_path is available\n",
    "        if 'details_report_path' not in globals():\n",
    "            # Create it from base paths\n",
    "            base_path_marked_scripts = paths.get(\"base_path_marked_scripts\", \"marked/scripts/\")\n",
    "            details_report_path = os.path.join(base_path_marked_scripts, \"details_score_report.xlsx\")\n",
    "            globals()['details_report_path'] = details_report_path\n",
    "        else:\n",
    "            details_report_path = globals()['details_report_path']\n",
    "        \n",
    "        # Basic aggregate metrics\n",
    "        n_students = len(marksDf)\n",
    "        mean_score = marksDf[\"Marks\"].mean()\n",
    "        median_score = marksDf[\"Marks\"].median()\n",
    "        min_score = marksDf[\"Marks\"].min()\n",
    "        max_score = marksDf[\"Marks\"].max()\n",
    "        std_score = marksDf[\"Marks\"].std()\n",
    "\n",
    "        # Passing stats\n",
    "        passing_cutoff = globals().get(\"passingMark\", 15)\n",
    "        if passing_cutoff is None or (isinstance(passing_cutoff, float) and math.isnan(passing_cutoff)):\n",
    "            max_possible = marksDf[[c for c in marksDf.columns if c not in [\"ID\", \"NAME\", \"CLASS\", \"Marks\"]]].sum(axis=1).max()\n",
    "            passing_cutoff = 0.5 * max_possible if pd.notna(max_possible) else 0\n",
    "        pass_count = (marksDf[\"Marks\"] >= passing_cutoff).sum()\n",
    "        pass_rate = (pass_count / n_students * 100) if n_students else 0\n",
    "\n",
    "        # Question-level strengths/needs (excluding metadata)\n",
    "        question_cols = [c for c in marksDf.columns if c not in [\"ID\", \"NAME\", \"CLASS\", \"Marks\"] and c not in METADATA_QUESTIONS]\n",
    "        question_means = marksDf[question_cols].mean(numeric_only=True).dropna()\n",
    "        top_strengths = question_means.sort_values(ascending=False).head(3)\n",
    "        top_focus = question_means.sort_values(ascending=True).head(3)\n",
    "\n",
    "        # Generate class overview report\n",
    "        class_overview_report = \"\"\n",
    "        \n",
    "        # Check if performance reports are available for AI generation\n",
    "        if 'performance_df' in globals() and not performance_df.empty and len(performance_df) > 0:\n",
    "            try:\n",
    "                # Initialize Gemini client\n",
    "                client = init_gemini_client()\n",
    "                config = create_gemini_config(temperature=0.35, top_p=0.9, max_output_tokens=1536)\n",
    "                \n",
    "                # Trim individual reports for Gemini context\n",
    "                sample_reports = performance_df[\"PerformanceReport\"].dropna().tolist()\n",
    "                max_reports = 30\n",
    "                sample_reports = sample_reports[:max_reports]\n",
    "                report_blob = \"\\n\\n---\\n\\n\".join(sample_reports)\n",
    "\n",
    "                # Structured context for the model\n",
    "                summary_payload = {\n",
    "                    \"n_students\": n_students,\n",
    "                    \"mean\": round(mean_score, 2) if pd.notna(mean_score) else None,\n",
    "                    \"median\": round(median_score, 2) if pd.notna(median_score) else None,\n",
    "                    \"min\": round(min_score, 2) if pd.notna(min_score) else None,\n",
    "                    \"max\": round(max_score, 2) if pd.notna(max_score) else None,\n",
    "                    \"std\": round(std_score, 2) if pd.notna(std_score) else None,\n",
    "                    \"pass_rate\": round(pass_rate, 2),\n",
    "                    \"pass_cutoff\": passing_cutoff,\n",
    "                    \"top_strength_questions\": top_strengths.round(2).to_dict(),\n",
    "                    \"top_focus_questions\": top_focus.round(2).to_dict(),\n",
    "                }\n",
    "\n",
    "                # Cache-aware Gemini call\n",
    "                payload_hash = hashlib.sha256(\n",
    "                    (json.dumps(summary_payload, sort_keys=True) + report_blob).encode(\"utf-8\")\n",
    "                ).hexdigest()\n",
    "                class_cache_key = get_cache_key(\n",
    "                    \"class_overview_report\",\n",
    "                    model=\"gemini-3-flash-preview\",\n",
    "                    payload_hash=payload_hash,\n",
    "                )\n",
    "                cached_overview = get_from_cache(class_cache_key, os.path.join(CACHE_DIR, \"class_overview_report\"))\n",
    "                if cached_overview is not None:\n",
    "                    class_overview_report = cached_overview.get(\"report\", \"\")\n",
    "                    logger.info(\"‚úì Using cached class overview report\")\n",
    "                else:\n",
    "                    overview_prompt = f\"\"\"You are summarizing overall class performance from individual reports.\n",
    "Key metrics (JSON): {json.dumps(summary_payload)}\n",
    "Number of sampled individual reports: {len(sample_reports)}\n",
    "Individual reports (separated by ---):\n",
    "{report_blob}\n",
    "\n",
    "Write a concise class-level overview (<200 words):\n",
    "- 4-6 bullets on class strengths and weaknesses\n",
    "- 3 targeted next-step actions for instruction\n",
    "- 2 questions/topics to re-teach next\n",
    "Focus on patterns; do not restate student names or IDs.\"\"\"\n",
    "                    \n",
    "                    try:\n",
    "                        overview_response = client.models.generate_content(\n",
    "                            model=\"gemini-3-flash-preview\",\n",
    "                            contents=[{\"role\": \"user\", \"parts\": [{\"text\": overview_prompt}]}],\n",
    "                            config=config,\n",
    "                        )\n",
    "                        class_overview_report = overview_response.text if overview_response.text else \"\"\n",
    "                        save_to_cache(class_cache_key, {\"report\": class_overview_report}, os.path.join(CACHE_DIR, \"class_overview_report\"))\n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"Failed to generate class overview with Gemini: {e}\")\n",
    "                        class_overview_report = \"AI-generated class overview temporarily unavailable due to API issues. Please check the individual performance reports and question metrics for detailed insights.\"\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to generate AI class overview: {e}\")\n",
    "                class_overview_report = \"Class overview generation encountered issues. Statistical data is available in the metrics below.\"\n",
    "        \n",
    "        # If no AI overview was generated, create a statistical overview\n",
    "        if not class_overview_report or class_overview_report == \"\":\n",
    "            logger.info(\"Generating statistical class overview\")\n",
    "            class_overview_report = f\"\"\"**Statistical Class Overview**\n",
    "\n",
    "Based on the performance data from {n_students} students:\n",
    "\n",
    "**Class Performance:**\n",
    "‚Ä¢ Average score: {mean_score:.1f} points (Range: {min_score:.1f} - {max_score:.1f})\n",
    "‚Ä¢ Pass rate: {pass_rate:.1f}% of students met the passing criteria (‚â•{passing_cutoff})\n",
    "‚Ä¢ Standard deviation: {std_score:.2f} points\n",
    "\n",
    "**Question Analysis:**\n",
    "‚Ä¢ Strongest questions: {', '.join([f\"{k} ({v:.1f})\" for k, v in top_strengths.items()])}\n",
    "‚Ä¢ Focus areas: {', '.join([f\"{k} ({v:.1f})\" for k, v in top_focus.items()])}\n",
    "\n",
    "**Recommendations:**\n",
    "‚Ä¢ Provide targeted support for questions with low average scores\n",
    "‚Ä¢ Review curriculum alignment for challenging topics\n",
    "‚Ä¢ Consider differentiated instruction based on performance patterns\n",
    "‚Ä¢ Analyze individual student needs for personalized support\"\"\"\n",
    "\n",
    "        # Create class overview DataFrame\n",
    "        strengths_str = \", \".join([f\"{k}: {v:.2f}\" for k, v in top_strengths.items()]) if not top_strengths.empty else \"\"\n",
    "        focus_str = \", \".join([f\"{k}: {v:.2f}\" for k, v in top_focus.items()]) if not top_focus.empty else \"\"\n",
    "        \n",
    "        class_overview_df = pd.DataFrame(\n",
    "            [\n",
    "                (\"Total Students\", n_students),\n",
    "                (\"Mean\", round(mean_score, 2) if pd.notna(mean_score) else \"N/A\"),\n",
    "                (\"Median\", round(median_score, 2) if pd.notna(median_score) else \"N/A\"),\n",
    "                (\"Min\", round(min_score, 2) if pd.notna(min_score) else \"N/A\"),\n",
    "                (\"Max\", round(max_score, 2) if pd.notna(max_score) else \"N/A\"),\n",
    "                (\"StdDev\", round(std_score, 2) if pd.notna(std_score) else \"N/A\"),\n",
    "                (\"Pass Cutoff\", passing_cutoff),\n",
    "                (\"Pass Rate (%)\", round(pass_rate, 2)),\n",
    "                (\"Top Strength Questions\", strengths_str),\n",
    "                (\"Top Focus Questions\", focus_str),\n",
    "                (\"Gemini Class Overview\", class_overview_report),\n",
    "            ],\n",
    "            columns=[\"Metric\", \"Value\"],\n",
    "        )\n",
    "\n",
    "        # Save class overview to Excel\n",
    "        try:\n",
    "            # Ensure the directory exists\n",
    "            os.makedirs(os.path.dirname(details_report_path), exist_ok=True)\n",
    "            \n",
    "            with pd.ExcelWriter(\n",
    "                details_report_path,\n",
    "                mode=\"a\" if os.path.exists(details_report_path) else \"w\",\n",
    "                engine=\"openpyxl\" if os.path.exists(details_report_path) else None,\n",
    "                if_sheet_exists=\"replace\" if os.path.exists(details_report_path) else None,\n",
    "            ) as writer:\n",
    "                class_overview_df.to_excel(writer, sheet_name=\"ClassOverview\", index=False)\n",
    "            \n",
    "            logger.info(f\"‚úì Saved class overview to Excel: {details_report_path}\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to save class overview to Excel: {e}\")\n",
    "\n",
    "        logger.info(\"‚úì Generated class overview analytics\")\n",
    "        print(f\"üìä Class Overview Generated Successfully:\")\n",
    "        print(f\"   ‚Ä¢ Students: {n_students}\")\n",
    "        print(f\"   ‚Ä¢ Mean Score: {mean_score:.2f}\")\n",
    "        print(f\"   ‚Ä¢ Pass Rate: {pass_rate:.1f}%\")\n",
    "        print(f\"   ‚Ä¢ Overview Type: {'AI-Generated' if class_overview_report and 'Statistical Class Overview' not in class_overview_report else 'Statistical'}\")\n",
    "        print(f\"   ‚Ä¢ Excel Report: {os.path.basename(details_report_path)}\")\n",
    "        \n",
    "        return class_overview_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Class analytics generation failed: {e}\")\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        \n",
    "        # Return a basic DataFrame to prevent downstream failures\n",
    "        try:\n",
    "            n_students = len(marksDf) if 'marksDf' in globals() else 0\n",
    "        except:\n",
    "            n_students = 0\n",
    "            \n",
    "        basic_df = pd.DataFrame([\n",
    "            (\"Total Students\", n_students),\n",
    "            (\"Status\", f\"Analytics generation failed: {str(e)}\"),\n",
    "            (\"Gemini Class Overview\", \"Class overview temporarily unavailable - check error logs\")\n",
    "        ], columns=[\"Metric\", \"Value\"])\n",
    "        return basic_df\n",
    "\n",
    "# Generate class analytics\n",
    "class_overview_df = generate_class_analytics()\n",
    "if not class_overview_df.empty:\n",
    "    print(\"\\nüìã Class Overview Data:\")\n",
    "    display(class_overview_df.head(10))  # Show more rows\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Class overview generation failed - check previous cells for errors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e85910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust Question-Level Metrics and Visualizations\n",
    "def generate_question_metrics():\n",
    "    \"\"\"Generate detailed question-level performance metrics and visualizations\"\"\"\n",
    "    print(\"üìä Generating question-level metrics and visualizations...\")\n",
    "    \n",
    "    try:\n",
    "        # Question columns (excluding metadata)\n",
    "        question_cols = [c for c in marksDf.columns if c not in [\"ID\", \"NAME\", \"CLASS\", \"Marks\"] and c not in METADATA_QUESTIONS]\n",
    "        \n",
    "        if not question_cols:\n",
    "            logger.warning(\"No question columns found for metrics\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Calculate question-level statistics\n",
    "        question_stats = []\n",
    "        passing_cutoff = globals().get(\"passingMark\", 15)\n",
    "        \n",
    "        for q in question_cols:\n",
    "            q_data = marksDf[q].apply(pd.to_numeric, errors='coerce').dropna()\n",
    "            if len(q_data) == 0:\n",
    "                continue\n",
    "                \n",
    "            # Assume 50% of max score as passing for individual questions\n",
    "            q_max = q_data.max()\n",
    "            q_pass_cutoff = q_max * 0.5 if q_max > 0 else 0\n",
    "            q_pass_count = (q_data >= q_pass_cutoff).sum()\n",
    "            q_pass_rate = (q_pass_count / len(q_data) * 100) if len(q_data) > 0 else 0\n",
    "            \n",
    "            question_stats.append({\n",
    "                \"Question\": q,\n",
    "                \"Mean\": round(q_data.mean(), 2),\n",
    "                \"Median\": round(q_data.median(), 2),\n",
    "                \"StdDev\": round(q_data.std(), 6),\n",
    "                \"Min\": round(q_data.min(), 2),\n",
    "                \"Max\": round(q_data.max(), 2),\n",
    "                \"NonNull\": len(q_data),\n",
    "                \"Pass Cutoff\": round(q_pass_cutoff, 2),\n",
    "                \"Pass Rate (%)\": round(q_pass_rate, 1)\n",
    "            })\n",
    "        \n",
    "        question_metrics_df = pd.DataFrame(question_stats)\n",
    "        \n",
    "        # Save question metrics to Excel\n",
    "        with pd.ExcelWriter(\n",
    "            details_report_path,\n",
    "            mode=\"a\" if os.path.exists(details_report_path) else \"w\",\n",
    "            engine=\"openpyxl\" if os.path.exists(details_report_path) else None,\n",
    "            if_sheet_exists=\"replace\" if os.path.exists(details_report_path) else None,\n",
    "        ) as writer:\n",
    "            question_metrics_df.to_excel(writer, sheet_name=\"QuestionMetrics\", index=False)\n",
    "\n",
    "        # Generate visualizations\n",
    "        if len(question_cols) > 0:\n",
    "            try:\n",
    "                # Set up the plotting style\n",
    "                plt.style.use('default')\n",
    "                sns.set_palette(\"husl\")\n",
    "                \n",
    "                # Create subplots for different visualizations\n",
    "                fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "                fig.suptitle('Question Performance Analysis', fontsize=16, fontweight='bold')\n",
    "                \n",
    "                # 1. Question means bar chart\n",
    "                ax1 = axes[0, 0]\n",
    "                question_means = marksDf[question_cols].mean(numeric_only=True)\n",
    "                bars1 = ax1.bar(question_means.index, question_means.values, color='skyblue', alpha=0.7)\n",
    "                ax1.set_title('Average Score by Question', fontweight='bold')\n",
    "                ax1.set_xlabel('Question')\n",
    "                ax1.set_ylabel('Average Score')\n",
    "                ax1.tick_params(axis='x', rotation=45)\n",
    "                \n",
    "                # Add value labels on bars\n",
    "                for bar in bars1:\n",
    "                    height = bar.get_height()\n",
    "                    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "                            f'{height:.1f}', ha='center', va='bottom')\n",
    "                \n",
    "                # 2. Score distribution histogram\n",
    "                ax2 = axes[0, 1]\n",
    "                ax2.hist(marksDf['Marks'], bins=min(10, len(marksDf)), color='lightgreen', alpha=0.7, edgecolor='black')\n",
    "                ax2.axvline(marksDf['Marks'].mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {marksDf[\"Marks\"].mean():.1f}')\n",
    "                ax2.axvline(passing_cutoff, color='orange', linestyle='--', linewidth=2, label=f'Pass: {passing_cutoff}')\n",
    "                ax2.set_title('Total Score Distribution', fontweight='bold')\n",
    "                ax2.set_xlabel('Total Score')\n",
    "                ax2.set_ylabel('Number of Students')\n",
    "                ax2.legend()\n",
    "                \n",
    "                # 3. Question difficulty (pass rates)\n",
    "                ax3 = axes[1, 0]\n",
    "                if not question_metrics_df.empty:\n",
    "                    bars3 = ax3.bar(question_metrics_df['Question'], question_metrics_df['Pass Rate (%)'], \n",
    "                                   color='coral', alpha=0.7)\n",
    "                    ax3.set_title('Question Pass Rates', fontweight='bold')\n",
    "                    ax3.set_xlabel('Question')\n",
    "                    ax3.set_ylabel('Pass Rate (%)')\n",
    "                    ax3.tick_params(axis='x', rotation=45)\n",
    "                    ax3.set_ylim(0, 100)\n",
    "                    \n",
    "                    # Add value labels on bars\n",
    "                    for bar in bars3:\n",
    "                        height = bar.get_height()\n",
    "                        ax3.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                                f'{height:.1f}%', ha='center', va='bottom')\n",
    "                \n",
    "                # 4. Question score ranges (box plot)\n",
    "                ax4 = axes[1, 1]\n",
    "                question_data = [marksDf[col].apply(pd.to_numeric, errors='coerce').dropna() for col in question_cols]\n",
    "                bp = ax4.boxplot(question_data, tick_labels=question_cols, patch_artist=True)\n",
    "                ax4.set_title('Question Score Distributions', fontweight='bold')\n",
    "                ax4.set_xlabel('Question')\n",
    "                ax4.set_ylabel('Score')\n",
    "                ax4.tick_params(axis='x', rotation=45)\n",
    "                \n",
    "                # Color the box plots\n",
    "                colors = plt.cm.Set3(range(len(bp['boxes'])))\n",
    "                for patch, color in zip(bp['boxes'], colors):\n",
    "                    patch.set_facecolor(color)\n",
    "                    patch.set_alpha(0.7)\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                \n",
    "                # Save the plot\n",
    "                chart_path = base_path_marked_scripts + \"question_analysis_charts.png\"\n",
    "                plt.savefig(chart_path, dpi=300, bbox_inches='tight')\n",
    "                plt.show()\n",
    "                \n",
    "                logger.info(f\"‚úì Generated question analysis charts: {os.path.basename(chart_path)}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to generate visualizations: {e}\")\n",
    "        \n",
    "        logger.info(f\"‚úì Generated question metrics for {len(question_cols)} questions\")\n",
    "        print(f\"üìä Saved QuestionMetrics sheet in: {os.path.basename(details_report_path)}\")\n",
    "        \n",
    "        return question_metrics_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Question metrics generation failed: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Generate question metrics and visualizations\n",
    "question_metrics_df = generate_question_metrics()\n",
    "if not question_metrics_df.empty:\n",
    "    display(question_metrics_df)\n",
    "\n",
    "# Generate question metrics and visualizations\n",
    "question_metrics_df = generate_question_metrics()\n",
    "if not question_metrics_df.empty:\n",
    "    display(question_metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6369ce59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust Word Document Report Generation\n",
    "def generate_word_report():\n",
    "    \"\"\"Generate comprehensive Word document report with charts and analytics\"\"\"\n",
    "    print(\"üìÑ Generating comprehensive Word document report...\")\n",
    "    \n",
    "    try:\n",
    "        from docx import Document\n",
    "        from docx.enum.text import WD_ALIGN_PARAGRAPH\n",
    "        from docx.shared import Pt, Inches\n",
    "        import re\n",
    "        import os\n",
    "        \n",
    "        # Ensure required data is available\n",
    "        if 'class_overview_df' not in globals() or class_overview_df.empty:\n",
    "            logger.warning(\"Class overview data not available, generating basic report\")\n",
    "            return None\n",
    "        \n",
    "        # Create charts directory\n",
    "        charts_dir = os.path.join(base_path_marked_scripts, \"charts\")\n",
    "        os.makedirs(charts_dir, exist_ok=True)\n",
    "        \n",
    "        # Initialize Word document\n",
    "        word_path = os.path.join(base_path_marked_scripts, \"class_overview_report.docx\")\n",
    "        doc = Document()\n",
    "        \n",
    "        # Base styles\n",
    "        doc.styles[\"Normal\"].font.size = Pt(11)\n",
    "        \n",
    "        # Document title\n",
    "        title_text = \"Class Performance Overview\"\n",
    "        if \"prefix\" in globals() and prefix:\n",
    "            title_text = f\"{prefix} - {title_text}\"\n",
    "        doc.add_heading(title_text, level=1)\n",
    "        \n",
    "        # Key stats summary\n",
    "        total_students = len(marksDf)\n",
    "        avg_score = marksDf['Marks'].mean()\n",
    "        median_score = marksDf['Marks'].median()\n",
    "        pass_count = len(marksDf[marksDf['Marks'] > passingMark])\n",
    "        pass_rate = (pass_count / total_students * 100) if total_students > 0 else 0\n",
    "        \n",
    "        summary_line = (\n",
    "            f\"Students: {total_students} | Mean: {avg_score:.2f} | Median: {median_score:.2f} | \"\n",
    "            f\"Pass rate: {pass_rate:.1f}% (cutoff {passingMark})\"\n",
    "        )\n",
    "        doc.add_paragraph(summary_line)\n",
    "        \n",
    "        def add_markdown_line(md_line, bullet=False, heading_level=None):\n",
    "            \"\"\"Render a single markdown line into the docx with bold/italic support.\"\"\"\n",
    "            if heading_level:\n",
    "                p = doc.add_heading(level=heading_level)\n",
    "            elif bullet:\n",
    "                p = doc.add_paragraph(style=\"List Bullet\")\n",
    "            else:\n",
    "                p = doc.add_paragraph()\n",
    "\n",
    "            tokens = re.split(r\"(\\*\\*[^*]+\\*\\*|__[^_]+__|\\*[^*]+\\*|_[^_]+_)\", md_line)\n",
    "            for tok in tokens:\n",
    "                if not tok:\n",
    "                    continue\n",
    "                bold = tok.startswith(\"**\") or tok.startswith(\"__\")\n",
    "                italic = tok.startswith(\"*\") or tok.startswith(\"_\")\n",
    "                clean = tok.strip(\"*_ \")\n",
    "                run = p.add_run(clean)\n",
    "                if bold:\n",
    "                    run.bold = True\n",
    "                if italic:\n",
    "                    run.italic = True\n",
    "            return p\n",
    "        \n",
    "        # Add Gemini narrative if available\n",
    "        if 'class_overview_df' in globals():\n",
    "            gemini_overview = class_overview_df[class_overview_df['Metric'] == 'Gemini Class Overview']\n",
    "            if not gemini_overview.empty:\n",
    "                class_overview_report = gemini_overview.iloc[0]['Value']\n",
    "                if class_overview_report and str(class_overview_report) != 'nan':\n",
    "                    doc.add_heading(\"AI-Generated Class Analysis\", level=2)\n",
    "                    for raw_line in str(class_overview_report).splitlines():\n",
    "                        line = raw_line.rstrip()\n",
    "                        if not line:\n",
    "                            continue\n",
    "                        heading_match = re.match(r\"^(#{1,3})\\s+(.*)$\", line)\n",
    "                        if heading_match:\n",
    "                            level = len(heading_match.group(1))\n",
    "                            add_markdown_line(heading_match.group(2), heading_level=level)\n",
    "                            continue\n",
    "                        if re.match(r\"^[-*+]\\s+\", line):\n",
    "                            add_markdown_line(line[2:].strip(), bullet=True)\n",
    "                            continue\n",
    "                        num_match = re.match(r\"^\\d+\\.\\s+(.*)$\", line)\n",
    "                        if num_match:\n",
    "                            p = add_markdown_line(num_match.group(1))\n",
    "                            p.style = \"List Number\"\n",
    "                            continue\n",
    "                        p = add_markdown_line(line)\n",
    "                        p.alignment = WD_ALIGN_PARAGRAPH.LEFT\n",
    "        \n",
    "        # Generate and add class-level charts\n",
    "        try:\n",
    "            overall_hist = os.path.join(charts_dir, \"overall_hist.png\")\n",
    "            pass_bar = os.path.join(charts_dir, \"pass_bar.png\")\n",
    "            \n",
    "            # Score distribution histogram\n",
    "            plt.figure(figsize=(8, 4))\n",
    "            plt.hist(pd.to_numeric(marksDf[\"Marks\"], errors=\"coerce\").dropna(), bins=20, color=\"#4a90e2\", alpha=0.7, edgecolor='black')\n",
    "            plt.xlabel(\"Total Score\")\n",
    "            plt.ylabel(\"Number of Students\")\n",
    "            plt.title(\"Score Distribution (Class)\")\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(overall_hist, dpi=180, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            # Pass/Fail bar chart\n",
    "            plt.figure(figsize=(6, 4))\n",
    "            fail_count = total_students - pass_count\n",
    "            plt.bar([\"Pass\", \"Fail\"], [pass_count, fail_count], color=[\"#4caf50\", \"#e74c3c\"], alpha=0.7)\n",
    "            plt.ylabel(\"Number of Students\")\n",
    "            plt.title(\"Pass / Fail Distribution\")\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(pass_bar, dpi=180, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            # Add charts to document\n",
    "            doc.add_page_break()\n",
    "            doc.add_heading(\"Class Performance Charts\", level=2)\n",
    "            doc.add_paragraph(\"Score histogram shows the distribution of total scores across all students.\")\n",
    "            doc.add_picture(overall_hist, width=Inches(6))\n",
    "            doc.add_paragraph(\"Pass/Fail distribution shows how many students met the passing criteria.\")\n",
    "            doc.add_picture(pass_bar, width=Inches(4))\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to generate class charts: {e}\")\n",
    "        \n",
    "        # Add metrics table\n",
    "        if 'class_overview_df' in globals() and not class_overview_df.empty:\n",
    "            table_df = class_overview_df.copy()\n",
    "            # Exclude the Gemini overview from the metrics table\n",
    "            table_df = table_df[table_df[\"Metric\"] != \"Gemini Class Overview\"]\n",
    "            \n",
    "            if not table_df.empty:\n",
    "                doc.add_page_break()\n",
    "                doc.add_heading(\"Performance Metrics\", level=2)\n",
    "                table = doc.add_table(rows=1, cols=2, style=\"Light Grid\")\n",
    "                hdr_cells = table.rows[0].cells\n",
    "                hdr_cells[0].text = \"Metric\"\n",
    "                hdr_cells[1].text = \"Value\"\n",
    "                \n",
    "                for _, row in table_df.iterrows():\n",
    "                    cells = table.add_row().cells\n",
    "                    cells[0].text = str(row[\"Metric\"])\n",
    "                    cells[1].text = str(row[\"Value\"])\n",
    "        \n",
    "        # Add per-question metrics if available\n",
    "        if 'question_metrics_df' in globals() and not question_metrics_df.empty:\n",
    "            doc.add_page_break()\n",
    "            doc.add_heading(\"Per-Question Analysis\", level=2)\n",
    "            \n",
    "            # Create question metrics table\n",
    "            pq_table = doc.add_table(rows=1, cols=6, style=\"Light Grid\")\n",
    "            hdr = pq_table.rows[0].cells\n",
    "            hdr[0].text = \"Question\"\n",
    "            hdr[1].text = \"Mean\"\n",
    "            hdr[2].text = \"Median\"\n",
    "            hdr[3].text = \"StdDev\"\n",
    "            hdr[4].text = \"Min / Max\"\n",
    "            hdr[5].text = \"Pass Rate (%)\"\n",
    "            \n",
    "            for _, r in question_metrics_df.iterrows():\n",
    "                cells = pq_table.add_row().cells\n",
    "                cells[0].text = str(r.get(\"Question\", \"\"))\n",
    "                cells[1].text = f\"{r['Mean']:.2f}\" if pd.notna(r.get(\"Mean\")) else \"\"\n",
    "                cells[2].text = f\"{r['Median']:.2f}\" if pd.notna(r.get(\"Median\")) else \"\"\n",
    "                cells[3].text = f\"{r['StdDev']:.2f}\" if pd.notna(r.get(\"StdDev\")) else \"\"\n",
    "                min_val = f\"{r['Min']:.2f}\" if pd.notna(r.get(\"Min\")) else \"\"\n",
    "                max_val = f\"{r['Max']:.2f}\" if pd.notna(r.get(\"Max\")) else \"\"\n",
    "                cells[4].text = f\"{min_val} / {max_val}\" if (min_val or max_val) else \"\"\n",
    "                cells[5].text = f\"{r['Pass Rate (%)']:.1f}\" if pd.notna(r.get(\"Pass Rate (%)\")) else \"\"\n",
    "            \n",
    "            # Generate and add question-level charts\n",
    "            try:\n",
    "                mean_chart = os.path.join(charts_dir, \"question_mean.png\")\n",
    "                box_chart = os.path.join(charts_dir, \"question_box.png\")\n",
    "                pass_rate_chart = os.path.join(charts_dir, \"question_pass_rate.png\")\n",
    "                \n",
    "                # Question means with error bars\n",
    "                plt.figure(figsize=(10, 5))\n",
    "                plt.bar(question_metrics_df[\"Question\"], question_metrics_df[\"Mean\"], \n",
    "                       yerr=question_metrics_df[\"StdDev\"], capsize=4, alpha=0.7, color='skyblue')\n",
    "                plt.ylabel(\"Mean Score\")\n",
    "                plt.title(\"Mean Score per Question\")\n",
    "                plt.xticks(rotation=45)\n",
    "                plt.grid(True, alpha=0.3)\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(mean_chart, dpi=180, bbox_inches='tight')\n",
    "                plt.close()\n",
    "                \n",
    "                # Box plots for score distributions\n",
    "                question_cols = [c for c in marksDf.columns if c not in [\"ID\", \"NAME\", \"CLASS\", \"Marks\"] and c not in METADATA_QUESTIONS]\n",
    "                if question_cols:\n",
    "                    plt.figure(figsize=(10, 5))\n",
    "                    box_data = [pd.to_numeric(marksDf[q], errors=\"coerce\").dropna() for q in question_cols]\n",
    "                    plt.boxplot(box_data, tick_labels=question_cols)\n",
    "                    plt.ylabel(\"Score\")\n",
    "                    plt.title(\"Score Distribution per Question\")\n",
    "                    plt.xticks(rotation=45)\n",
    "                    plt.grid(True, alpha=0.3)\n",
    "                    plt.tight_layout()\n",
    "                    plt.savefig(box_chart, dpi=180, bbox_inches='tight')\n",
    "                    plt.close()\n",
    "                \n",
    "                # Pass rate chart\n",
    "                plt.figure(figsize=(10, 5))\n",
    "                plt.bar(question_metrics_df[\"Question\"], question_metrics_df[\"Pass Rate (%)\"], \n",
    "                       color=\"#7b7ce6\", alpha=0.7)\n",
    "                plt.ylabel(\"Pass Rate (%)\")\n",
    "                plt.title(\"Pass Rate per Question\")\n",
    "                plt.xticks(rotation=45)\n",
    "                plt.ylim(0, 100)\n",
    "                plt.grid(True, alpha=0.3)\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(pass_rate_chart, dpi=180, bbox_inches='tight')\n",
    "                plt.close()\n",
    "                \n",
    "                # Add charts to document\n",
    "                doc.add_paragraph(\"Mean score bars show average performance per question, with error bars indicating variability.\")\n",
    "                doc.add_picture(mean_chart, width=Inches(6))\n",
    "                \n",
    "                if question_cols:\n",
    "                    doc.add_paragraph(\"Box plots show the distribution of scores for each question (median, quartiles, and outliers).\")\n",
    "                    doc.add_picture(box_chart, width=Inches(6))\n",
    "                \n",
    "                doc.add_paragraph(\"Pass rate bars show the percentage of students who achieved at least half marks for each question.\")\n",
    "                doc.add_picture(pass_rate_chart, width=Inches(6))\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to generate question charts: {e}\")\n",
    "        \n",
    "        # Add strengths and focus areas\n",
    "        if 'class_overview_df' in globals():\n",
    "            strengths_row = class_overview_df[class_overview_df['Metric'] == 'Top Strength Questions']\n",
    "            focus_row = class_overview_df[class_overview_df['Metric'] == 'Top Focus Questions']\n",
    "            \n",
    "            if not strengths_row.empty and str(strengths_row.iloc[0]['Value']) != '':\n",
    "                doc.add_heading(\"Top Strength Questions\", level=2)\n",
    "                strengths_text = str(strengths_row.iloc[0]['Value'])\n",
    "                for item in strengths_text.split(', '):\n",
    "                    if item.strip():\n",
    "                        doc.add_paragraph(item.strip(), style=\"List Bullet\")\n",
    "            \n",
    "            if not focus_row.empty and str(focus_row.iloc[0]['Value']) != '':\n",
    "                doc.add_heading(\"Areas Needing Focus\", level=2)\n",
    "                focus_text = str(focus_row.iloc[0]['Value'])\n",
    "                for item in focus_text.split(', '):\n",
    "                    if item.strip():\n",
    "                        doc.add_paragraph(item.strip(), style=\"List Bullet\")\n",
    "        \n",
    "        # Add footer with generation info\n",
    "        footer = doc.add_paragraph()\n",
    "        footer_run = footer.add_run(f\"Report generated on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} | Robust Step 6 Processing\")\n",
    "        footer_run.font.size = Pt(9)\n",
    "        footer.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "        \n",
    "        # Save the document\n",
    "        doc.save(word_path)\n",
    "        logger.info(f\"‚úì Generated Word report: {os.path.basename(word_path)}\")\n",
    "        print(f\"üìÑ Saved Word report: {word_path}\")\n",
    "        \n",
    "        return word_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Word report generation failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Generate Word document report\n",
    "word_report_path = generate_word_report()\n",
    "if word_report_path:\n",
    "    print(f\"‚úÖ Word document report generated successfully!\")\n",
    "    print(f\"üìÅ Location: {word_report_path}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Word document generation was skipped due to missing data or errors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust backup and cleanup with validation\n",
    "def backup_and_cleanup():\n",
    "    \"\"\"Robust backup with comprehensive validation and error handling\"\"\"\n",
    "    print(\"üßπ Performing backup and cleanup...\")\n",
    "    \n",
    "    try:\n",
    "        # Remove version history files with progress tracking\n",
    "        version_files_removed = 0\n",
    "        for path, _, files in os.walk(base_path_questions):\n",
    "            for file in files:\n",
    "                if file.startswith(\"control-\") or file.startswith(\"mark-\"):\n",
    "                    try:\n",
    "                        os.remove(os.path.join(path, file))\n",
    "                        version_files_removed += 1\n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"Failed to remove {file}: {e}\")\n",
    "        logger.info(f\"‚úì Removed {version_files_removed} version history files\")\n",
    "        \n",
    "        # Create backup archive with validation\n",
    "        backup_path = shutil.make_archive(base_path, \"zip\", base_path)\n",
    "        if os.path.exists(backup_path):\n",
    "            backup_size = os.path.getsize(backup_path)\n",
    "            logger.info(f\"‚úì Created backup archive: {backup_path}\")\n",
    "            logger.info(f\"  Archive size: {backup_size:,} bytes ({backup_size/1024/1024:.1f} MB)\")\n",
    "            return backup_path\n",
    "        else:\n",
    "            raise Exception(\"Failed to create backup archive\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Backup and cleanup failed: {e}\")\n",
    "        raise\n",
    "\n",
    "# Perform backup and cleanup\n",
    "backup_path = backup_and_cleanup()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust final summary and next steps\n",
    "def generate_final_summary():\n",
    "    \"\"\"Generate comprehensive final summary with actionable next steps\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üéâ ENHANCED STEP 6: POST-SCORING PACKAGING COMPLETE\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Overall statistics\n",
    "    total_students = len(marksDf)\n",
    "    avg_score = marksDf['Marks'].mean()\n",
    "    passing_students = len(marksDf[marksDf['Marks'] > passingMark])\n",
    "    pass_rate = (passing_students / total_students * 100) if total_students > 0 else 0\n",
    "    \n",
    "    print(f\"\\nüìä Processing Results:\")\n",
    "    print(f\"   Total students processed: {total_students}\")\n",
    "    print(f\"   Average score: {avg_score:.2f}\")\n",
    "    print(f\"   Passing students: {passing_students} ({pass_rate:.1f}%)\")\n",
    "    print(f\"   Score range: {marksDf['Marks'].min():.1f} - {marksDf['Marks'].max():.1f}\")\n",
    "    \n",
    "    print(f\"\\nüìÅ Generated Files:\")\n",
    "    print(f\"   ‚úÖ Backup archive: {os.path.basename(backup_path)}\")\n",
    "    print(f\"   ‚úÖ Individual PDFs: {pdf_stats['successful']} created\")\n",
    "    print(f\"   ‚úÖ Combined PDF: all.pdf\")\n",
    "    print(f\"   ‚úÖ Sample collections: Multiple stratified samples\")\n",
    "    print(f\"   ‚úÖ Comprehensive Excel reports with multiple sheets:\")\n",
    "    print(f\"      ‚Ä¢ Marks, Answers, Reasoning (wide format)\")\n",
    "    print(f\"      ‚Ä¢ Raw data for audit trail\")\n",
    "    print(f\"      ‚Ä¢ AI-powered Performance reports\")\n",
    "    print(f\"      ‚Ä¢ Class-level analytics and overview\")\n",
    "    print(f\"      ‚Ä¢ Question-level metrics and statistics\")\n",
    "    print(f\"   ‚úÖ Visual analytics: Question performance charts\")\n",
    "    \n",
    "    if pdf_stats['failed'] > 0:\n",
    "        print(f\"   ‚ö†Ô∏è PDF generation issues: {pdf_stats['failed']} failed\")\n",
    "    \n",
    "    if failed_students:\n",
    "        print(f\"   ‚ö†Ô∏è Student processing issues: {len(failed_students)} students\")\n",
    "    \n",
    "    print(f\"\\nü§ñ AI-Robust Features:\")\n",
    "    if not performance_df.empty:\n",
    "        print(f\"   ‚úÖ Individual performance reports: {len(performance_df)} generated\")\n",
    "    if not class_overview_df.empty:\n",
    "        print(f\"   ‚úÖ Class-level analytics with AI insights\")\n",
    "    print(f\"   ‚úÖ Metadata questions properly excluded from analysis\")\n",
    "    print(f\"   ‚úÖ Comprehensive caching for efficient re-runs\")\n",
    "    \n",
    "    print(f\"\\nüéØ Next Steps:\")\n",
    "    print(f\"   1. üìß Proceed to Step 7: Email Score Distribution\")\n",
    "    print(f\"   2. üìä Review detailed analytics in Excel reports\")\n",
    "    print(f\"   3. üìÑ Use sample PDFs for moderation and review\")\n",
    "    print(f\"   4. ü§ñ Review AI-generated performance insights\")\n",
    "    print(f\"   5. üìà Analyze question-level metrics for curriculum improvement\")\n",
    "    print(f\"   6. üíæ Archive backup file for long-term storage\")\n",
    "    \n",
    "    print(f\"\\nüí° Robust Quality Assurance:\")\n",
    "    print(f\"   ‚Ä¢ All processing includes comprehensive validation\")\n",
    "    print(f\"   ‚Ä¢ Error handling ensures partial failures don't stop processing\")\n",
    "    print(f\"   ‚Ä¢ Detailed logging provides full audit trail\")\n",
    "    print(f\"   ‚Ä¢ Multiple output formats support different use cases\")\n",
    "    print(f\"   ‚Ä¢ AI-powered insights provide actionable feedback\")\n",
    "    print(f\"   ‚Ä¢ Metadata questions properly handled and excluded\")\n",
    "    print(f\"   ‚Ä¢ Visual analytics support data-driven decisions\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"‚úÖ Robust Step 6 completed successfully at {datetime.now().strftime('%H:%M:%S')}\")\n",
    "    print(\"üöÄ Ready for final distribution, analysis, and archival!\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "# Generate final comprehensive summary\n",
    "generate_final_summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
