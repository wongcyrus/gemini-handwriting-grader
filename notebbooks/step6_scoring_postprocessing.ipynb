{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Post-Scoring Packaging\n",
    "Backup results, generate reports, produce scored PDFs, and collect samples for sharing. Run after completing scoring and checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grading_utils import setup_paths, create_directories\n",
    "\n",
    "prefix = \"VTC Test\"\n",
    "paths = setup_paths(prefix, \"sample\")\n",
    "\n",
    "# Extract commonly used paths\n",
    "pdf_file = paths[\"pdf_file\"]\n",
    "name_list_file = paths[\"name_list_file\"]\n",
    "base_path = paths[\"base_path\"]\n",
    "base_path_images = paths[\"base_path_images\"]\n",
    "base_path_annotations = paths[\"base_path_annotations\"]\n",
    "base_path_questions = paths[\"base_path_questions\"]\n",
    "base_path_marked_images = paths[\"base_path_marked_images\"]\n",
    "base_path_marked_pdfs = paths[\"base_path_marked_pdfs\"]\n",
    "base_path_marked_scripts = paths[\"base_path_marked_scripts\"]\n",
    "\n",
    "# Create all necessary directories\n",
    "create_directories(paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backup grading result\n",
    "Remove version history, before you backup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for path, currentDirectory, files in os.walk(base_path_questions):\n",
    "    for file in files:\n",
    "        if file.startswith(\"control-\") or file.startswith(\"mark-\"):\n",
    "            os.remove(os.path.join(path, file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zip the website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/user/AI-Handwrite-Grader/marking_form/VTC Test.zip'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "shutil.make_archive(base_path,\"zip\",base_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Score Report\n",
    "\n",
    "Check the ID and Name pages to verify the values before generate the marksheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  ID   NAME CLASS   Q1    Q2   Q3   Q4   Q5  Marks\n",
      "234567890  234567890   John     C  1.0   1.0  4.0  0.0  4.0   10.0\n",
      "123456789  123456789  Peter     A  2.0   9.0  0.0  8.0  3.0   22.0\n",
      "987654321  987654321   Mary     B  1.0  10.0  0.0  0.0  0.0   11.0\n",
      "345678912  345678912  Susan     D  2.0  10.0  0.0  0.0  0.0   12.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from grading_utils import build_student_id_mapping\n",
    "\n",
    "# Load name list as authoritative source for student names\n",
    "name_list_df = pd.read_excel(name_list_file, sheet_name=\"Name List\")\n",
    "id_col = next((col for col in name_list_df.columns if col.lower() == \"id\"), None)\n",
    "name_col = next((col for col in name_list_df.columns if col.lower() in [\"name\", \"student name\", \"student_name\"]), None)\n",
    "if id_col is None or name_col is None:\n",
    "    raise ValueError(\"Name list must contain ID and NAME columns.\")\n",
    "name_map = (\n",
    "    name_list_df.assign(**{id_col: name_list_df[id_col].astype(str)})\n",
    "    .set_index(id_col)[name_col]\n",
    "    .astype(str)\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "# Build student ID mapping using utility function\n",
    "pageToStudentId, numberOfPage, getStudentId = build_student_id_mapping(\n",
    "    base_path_questions, base_path_annotations\n",
    ")\n",
    "\n",
    "questionAndMarks = {}\n",
    "for path, currentDirectory, files in os.walk(base_path_questions):\n",
    "    for file in files:\n",
    "        if file == \"mark.json\":\n",
    "            question = path[len(base_path_questions) + 1 :]\n",
    "            f = open(os.path.join(path, file))\n",
    "            data = json.load(f)\n",
    "            marks = {}\n",
    "            for i in data:\n",
    "                studentId = getStudentId(int(i[\"id\"]))\n",
    "                marks[studentId] = (\n",
    "                    i[\"overridedMark\"] if i[\"overridedMark\"] != \"\" else i[\"mark\"]\n",
    "                )\n",
    "            questionAndMarks[question] = marks\n",
    "            f.close()\n",
    "marksDf = pd.DataFrame(questionAndMarks)\n",
    "marksDf = marksDf[\n",
    "    [\"ID\", \"NAME\", \"CLASS\"]\n",
    "    + [\n",
    "        col\n",
    "        for col in sorted(marksDf.columns)\n",
    "        if col != \"ID\" and col != \"NAME\" and col != \"CLASS\"\n",
    "    ]\n",
    "]\n",
    "\n",
    "# Prefer names from the uploaded name list, fallback to marked value when missing\n",
    "marksDf[\"ID\"] = marksDf[\"ID\"].astype(str)\n",
    "marksDf[\"NAME\"] = marksDf[\"ID\"].map(name_map).fillna(marksDf[\"NAME\"])\n",
    "\n",
    "marksDf[\"Marks\"] = (\n",
    "    marksDf.loc[:, ~marksDf.columns.isin([\"ID\", \"NAME\", \"CLASS\"])]\n",
    "    .apply(pd.to_numeric)\n",
    "    .sum(axis=1)\n",
    ")\n",
    "print(marksDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Scored Scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy raw images to marked folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../marking_form/VTC Test/marked/images/'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "if os.path.exists(base_path_marked_images):\n",
    "    shutil.rmtree(base_path_marked_images)\n",
    "shutil.copytree(base_path_images, base_path_marked_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Q1': {'width': 1189,\n",
       "  'height': 257,\n",
       "  'label': 'Q1',\n",
       "  'page': 0,\n",
       "  'left': 222,\n",
       "  'top': 608},\n",
       " 'Q2': {'width': 1189,\n",
       "  'height': 292,\n",
       "  'label': 'Q2',\n",
       "  'page': 0,\n",
       "  'left': 222,\n",
       "  'top': 865},\n",
       " 'Q3': {'width': 1189,\n",
       "  'height': 253,\n",
       "  'label': 'Q3',\n",
       "  'page': 0,\n",
       "  'left': 222,\n",
       "  'top': 1157},\n",
       " 'NAME': {'width': 171,\n",
       "  'height': 52,\n",
       "  'label': 'NAME',\n",
       "  'page': 0,\n",
       "  'left': 341,\n",
       "  'top': 473},\n",
       " 'ID': {'width': 216,\n",
       "  'height': 46,\n",
       "  'label': 'ID',\n",
       "  'page': 0,\n",
       "  'left': 1167,\n",
       "  'top': 472},\n",
       " 'CLASS': {'width': 83,\n",
       "  'height': 48,\n",
       "  'label': 'CLASS',\n",
       "  'page': 0,\n",
       "  'left': 327,\n",
       "  'top': 535},\n",
       " 'Q4': {'width': 1189,\n",
       "  'height': 260,\n",
       "  'label': 'Q4',\n",
       "  'page': 1,\n",
       "  'left': 223,\n",
       "  'top': 547},\n",
       " 'Q5': {'width': 1189,\n",
       "  'height': 288,\n",
       "  'label': 'Q5',\n",
       "  'page': 1,\n",
       "  'left': 223,\n",
       "  'top': 807}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "annotations_path = base_path_annotations + \"annotations.json\"\n",
    "with open(annotations_path, \"r\") as f: \n",
    "    annotations = json.load(f)          \n",
    "\n",
    "#flatten annotations to list \n",
    "annotations_list = []\n",
    "for page in annotations:\n",
    "    for annotation in annotations[page]:\n",
    "        annotation[\"page\"] = int(page)\n",
    "        # x to left, y to top\n",
    "        annotation[\"left\"] = annotation[\"x\"]\n",
    "        annotation[\"top\"] = annotation[\"y\"]\n",
    "        annotation.pop(\"x\")\n",
    "        annotation.pop(\"y\")\n",
    "        annotations_list.append(annotation) \n",
    "annotations_list\n",
    "\n",
    "# convert annotations_list to dict with key with label\n",
    "annotations_dict = {}\n",
    "for annotation in annotations_list:\n",
    "    annotations_dict[annotation[\"label\"]] = annotation\n",
    "annotations_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'123456789': 0, '987654321': 2, '234567890': 4, '345678912': 6}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "studentIdToPage={}\n",
    "with open(os.path.join(base_path_questions, \"ID\", \"mark.json\")) as f:\n",
    "    data = json.load(f)\n",
    "    for i in data:\n",
    "        studentId = i[\"overridedMark\"] if i[\"overridedMark\"] != \"\" else i[\"mark\"]\n",
    "        studentIdToPage[studentId] = int(i[\"id\"])\n",
    "studentIdToPage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dc2b615614945f1b81d8440bd2b17d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=4)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2\n",
    "from IPython.display import display\n",
    "from ipywidgets import IntProgress\n",
    "\n",
    "\n",
    "# Covert marksDf to dict\n",
    "marksDf_list = marksDf.to_dict(orient=\"records\")\n",
    "\n",
    "f = IntProgress(min=0, max=len(marksDf_list))  # instantiate the bar\n",
    "display(f)  # display the bar\n",
    "\n",
    "for student in marksDf_list:\n",
    "    first_page = studentIdToPage[student[\"ID\"]]\n",
    "    for annotation in annotations_dict:\n",
    "        value = student[annotation]\n",
    "        if annotation == \"ID\":\n",
    "            value = value + \" Marks: \" + str(student[\"Marks\"])\n",
    "        x = annotations_dict[annotation][\"left\"]\n",
    "        y = annotations_dict[annotation][\"top\"]\n",
    "        page = first_page + annotations_dict[annotation][\"page\"]\n",
    "      \n",
    "        image_path = base_path_marked_images + str(page) + \".jpg\"\n",
    "        # print(value, x, y, imagePath)\n",
    "        img = cv2.imread(image_path)\n",
    "        textSize = cv2.getTextSize(text=str(value), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=1, thickness=2)\n",
    "        height = textSize[0][1]\n",
    "        cv2.putText(img, str(value), (x, y + height), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "        cv2.imwrite(image_path, img)\n",
    "    f.value += 1      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "234567890 4 5\n",
      "123456789 0 1\n",
      "987654321 2 3\n",
      "345678912 6 7\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "for student in marksDf_list:\n",
    "    studentId = student[\"ID\"]\n",
    "    first_page = studentIdToPage[student[\"ID\"]]\n",
    "    last_page = first_page + numberOfPage - 1\n",
    "    print(studentId, first_page, last_page)\n",
    "    pdf_path = base_path_marked_pdfs + studentId + \".pdf\"\n",
    "\n",
    "    images = list(map(Image.open, [base_path_marked_images + str(i) + \".jpg\" for i in range(first_page, last_page + 1)]))\n",
    "    images[0].save(pdf_path, save_all=True, append_images=images[1:]) \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Script Sample\n",
    "\n",
    "5 set Samples:\n",
    "1. Combined scripts\n",
    "2. 3 Good, 3 Average, and 3 Weak.\n",
    "3. 5 Good, 5 Average, and 5 Weak.\n",
    "4. 3 Good, 3 Average, and 3 Weak above the passing mark.\n",
    "5. 5 Good, 5 Average, and 5 Weak above the passing mark. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "passingMark = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF4 import PdfFileMerger\n",
    "\n",
    "writer = PdfFileMerger(strict=True)\n",
    "\n",
    "# merge all pdfs in base_path_marked_pdfs\n",
    "for path, currentDirectory, files in os.walk(base_path_marked_pdfs):\n",
    "    for file in files:\n",
    "        if file.endswith(\".pdf\"):\n",
    "            pdf_path = os.path.join(path, file)\n",
    "            writer.append(pdf_path)\n",
    "writer.write(base_path_marked_scripts + \"all.pdf\")           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output successfully written to../marking_form/VTC Test/marked/scripts/sampleOf1.pdf\n",
      "Output successfully written to../marking_form/VTC Test/marked/scripts/sampleOf1.pdf\n",
      "Output successfully written to../marking_form/VTC Test/marked/scripts/sampleOf1_only_pass.pdf\n",
      "Output successfully written to../marking_form/VTC Test/marked/scripts/sampleOf1_only_pass.pdf\n"
     ]
    }
   ],
   "source": [
    "from PyPDF4 import PdfFileMerger, PdfFileReader\n",
    "\n",
    "sampling = marksDf.sort_values(by=[\"Marks\"], ascending=False)[\"Marks\"]\n",
    "\n",
    "from_directory = os.path.join(os.getcwd(), \"..\", \"templates\", \"pdf\")\n",
    "\n",
    "goodPage = PdfFileReader(from_directory + \"/Good.pdf\")\n",
    "averagePage = PdfFileReader(from_directory + \"/Average.pdf\")\n",
    "weakPage = PdfFileReader(from_directory + \"/Weak.pdf\")\n",
    "\n",
    "\n",
    "def get_scripts_psf(df):\n",
    "    return list(map(lambda rowNumber: base_path_marked_pdfs + rowNumber + \".pdf\", df.index))\n",
    "\n",
    "\n",
    "def take_sample(n, sampling, suffix=\"\"):\n",
    "    if len(sampling) < 3 * n:\n",
    "        n = int(len(sampling) / 3)\n",
    "    good = sampling.head(n)\n",
    "    weak = sampling.tail(n)\n",
    "    median = int(len(sampling) / 2)\n",
    "    take = int(n / 2)\n",
    "    average = sampling.iloc[median - take : median + take]\n",
    "\n",
    "    merger = PdfFileMerger()\n",
    "    merger.append(goodPage)\n",
    "    for pdf in get_scripts_psf(good):\n",
    "        merger.append(PdfFileReader(pdf))\n",
    "    merger.append(averagePage)\n",
    "    for pdf in get_scripts_psf(average):\n",
    "        merger.append(PdfFileReader(pdf))\n",
    "    merger.append(weakPage)\n",
    "    for pdf in get_scripts_psf(weak):\n",
    "        merger.append(PdfFileReader(pdf))\n",
    "    fileName = base_path_marked_scripts + \"sampleOf\" + str(n) + suffix + \".pdf\"\n",
    "    merger.write(open(fileName, \"wb\"))\n",
    "    print(\"Output successfully written to\" + fileName)\n",
    "    merger.close()\n",
    "\n",
    "\n",
    "take_sample(3, sampling)\n",
    "take_sample(5, sampling)\n",
    "\n",
    "sampling = sampling.where(lambda x: x > passingMark)\n",
    "take_sample(3, sampling, \"_only_pass\")\n",
    "take_sample(5, sampling, \"_only_pass\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "def clean_answer_text(val: str) -> str:\n",
    "    \"\"\"Strip leading numbering and drop standalone question labels like Q2.\"\"\"\n",
    "    if not isinstance(val, str):\n",
    "        return val\n",
    "    lines = [ln.strip() for ln in str(val).splitlines()]\n",
    "    cleaned = []\n",
    "    for ln in lines:\n",
    "        ln = re.sub(r\"^\\s*\\d+\\s*[\\.|\\)]\\s*\", \"\", ln)\n",
    "        if re.fullmatch(r\"q\\d+\", ln, flags=re.IGNORECASE):\n",
    "            continue\n",
    "        if ln:\n",
    "            cleaned.append(ln)\n",
    "    return \"\\n\".join(cleaned).strip()\n",
    "\n",
    "\n",
    "def collect_answers_and_reasoning():\n",
    "    \"\"\"Gather per-student answers and model reasoning from each question's CSV and pivot to a mark-style wide format.\"\"\"\n",
    "    answer_rows = []\n",
    "    reasoning_rows = []\n",
    "\n",
    "    for path, currentDirectory, files in os.walk(base_path_questions):\n",
    "        if \"data.csv\" not in files:\n",
    "            continue\n",
    "\n",
    "        question = path[len(base_path_questions) + 1 :]\n",
    "        data_path = os.path.join(path, \"data.csv\")\n",
    "        df = pd.read_csv(data_path)\n",
    "        if \"page\" not in df.columns:\n",
    "            continue\n",
    "\n",
    "        # Map scanned page back to student ID using the existing helper\n",
    "        df[\"StudentID\"] = df[\"page\"].apply(\n",
    "            lambda p: getStudentId(int(str(p).split(\".\")[0])) if pd.notna(p) else None\n",
    "        )\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            sid = row.get(\"StudentID\")\n",
    "            if sid is None:\n",
    "                continue\n",
    "\n",
    "            raw_answer = row.get(\"Answer\", \"\")\n",
    "            answer_val = clean_answer_text(raw_answer)\n",
    "            source_page = row.get(\"page\", \"\")\n",
    "            row_number = row.get(\"RowNumber\", \"\")\n",
    "\n",
    "            answer_rows.append(\n",
    "                {\n",
    "                    \"ID\": str(sid),\n",
    "                    \"Question\": question,\n",
    "                    \"Answer\": answer_val,\n",
    "                    \"SourcePage\": source_page,\n",
    "                    \"RowNumber\": row_number,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            reasoning_rows.append(\n",
    "                {\n",
    "                    \"ID\": str(sid),\n",
    "                    \"Question\": question,\n",
    "                    \"Reasoning\": row.get(\"Reasoning\", \"\"),\n",
    "                    \"Similarity\": row.get(\"Similarity\", \"\"),\n",
    "                    \"ModelMark\": row.get(\"Mark\", \"\"),\n",
    "                    \"Answer\": answer_val,\n",
    "                    \"SourcePage\": source_page,\n",
    "                    \"RowNumber\": row_number,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    answers_df = pd.DataFrame(answer_rows)\n",
    "    reasoning_df = pd.DataFrame(reasoning_rows)\n",
    "\n",
    "    if not answers_df.empty:\n",
    "        answers_df = answers_df.sort_values(\n",
    "            by=[\"ID\", \"Question\", \"SourcePage\", \"RowNumber\"]\n",
    "        ).reset_index(drop=True)\n",
    "    if not reasoning_df.empty:\n",
    "        reasoning_df = reasoning_df.sort_values(\n",
    "            by=[\"ID\", \"Question\", \"SourcePage\", \"RowNumber\"]\n",
    "        ).reset_index(drop=True)\n",
    "\n",
    "    # Preserve ID/NAME/CLASS to match marks layout\n",
    "    meta_cols = [\"ID\", \"NAME\", \"CLASS\"]\n",
    "    student_meta = marksDf[meta_cols].drop_duplicates().set_index(\"ID\")\n",
    "\n",
    "    # Keep question ordering aligned with marks sheet\n",
    "    question_cols = [\n",
    "        col\n",
    "        for col in marksDf.columns\n",
    "        if col not in [\"ID\", \"NAME\", \"CLASS\", \"Marks\"]\n",
    "    ]\n",
    "\n",
    "    # Wide answers: one row per student, one column per question\n",
    "    answers_wide = student_meta.copy()\n",
    "    if not answers_df.empty:\n",
    "        answers_pivot = answers_df.pivot_table(\n",
    "            index=\"ID\", columns=\"Question\", values=\"Answer\", aggfunc=\"first\"\n",
    "        )\n",
    "        answers_pivot = answers_pivot.reindex(columns=question_cols)\n",
    "        answers_wide = answers_wide.join(answers_pivot)\n",
    "        answers_wide = answers_wide.reset_index()\n",
    "\n",
    "    # Wide reasoning: only the reasoning text per question (matches marks layout)\n",
    "    reasoning_wide = student_meta.copy()\n",
    "    if not reasoning_df.empty:\n",
    "        reasoning_pivot = reasoning_df.pivot_table(\n",
    "            index=\"ID\", columns=\"Question\", values=\"Reasoning\", aggfunc=\"first\"\n",
    "        )\n",
    "        reasoning_pivot = reasoning_pivot.reindex(columns=question_cols)\n",
    "        reasoning_wide = reasoning_wide.join(reasoning_pivot)\n",
    "        reasoning_wide = reasoning_wide.reset_index()\n",
    "\n",
    "    return answers_wide, reasoning_wide, answers_df, reasoning_df\n",
    "\n",
    "\n",
    "answers_sheet, reasoning_sheet, answers_raw, reasoning_raw = collect_answers_and_reasoning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../marking_form/VTC Test/marked/scripts/details_score_report.xlsx\n"
     ]
    }
   ],
   "source": [
    "details_report_path = base_path_marked_scripts + \"details_score_report.xlsx\"\n",
    "\n",
    "# Multi-sheet Excel: marks, answers (wide), reasoning (wide) + raw long-form for audit\n",
    "with pd.ExcelWriter(details_report_path) as writer:\n",
    "    marksDf.to_excel(writer, sheet_name=\"Marks\", index=False)\n",
    "    answers_sheet.to_excel(writer, sheet_name=\"Answers\", index=False)\n",
    "    reasoning_sheet.to_excel(writer, sheet_name=\"Reasoning\", index=False)\n",
    "    answers_raw.to_excel(writer, sheet_name=\"AnswersRaw\", index=False)\n",
    "    reasoning_raw.to_excel(writer, sheet_name=\"ReasoningRaw\", index=False)\n",
    "\n",
    "# Lightweight summary sheet remains unchanged\n",
    "marksDf[[\"ID\", \"NAME\", \"CLASS\", \"Marks\"]].to_excel(\n",
    "    base_path_marked_scripts + \"score_report.xlsx\", index=False\n",
    ")\n",
    "\n",
    "print(\"Saved:\", details_report_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gemini Performance Report\n",
    "Use Gemini to generate a concise per-student performance report (question text, marking scheme, awarded marks, captured answer) and store it as a new `Performance` sheet inside `details_score_report.xlsx` (alongside Marks/Answers/Reasoning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Vertex AI Express Mode initialized\n",
      "Saved Performance sheet in: ../marking_form/VTC Test/marked/scripts/details_score_report.xlsx\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>NAME</th>\n",
       "      <th>CLASS</th>\n",
       "      <th>Marks</th>\n",
       "      <th>PerformanceReport</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>234567890</td>\n",
       "      <td>John</td>\n",
       "      <td>C</td>\n",
       "      <td>10.0</td>\n",
       "      <td>**Performance Report: John (Class C)**\\n\\n**Ov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>123456789</td>\n",
       "      <td>Peter</td>\n",
       "      <td>A</td>\n",
       "      <td>22.0</td>\n",
       "      <td>**Performance Report**\\n\\n**Student:** Peter (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>987654321</td>\n",
       "      <td>Mary</td>\n",
       "      <td>B</td>\n",
       "      <td>11.0</td>\n",
       "      <td>**Performance Report: Mary (987654321)**\\n\\n**...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>345678912</td>\n",
       "      <td>Susan</td>\n",
       "      <td>D</td>\n",
       "      <td>12.0</td>\n",
       "      <td>**Performance Report: Susan (345678912)**\\n\\nS...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID   NAME CLASS  Marks  \\\n",
       "0  234567890   John     C   10.0   \n",
       "1  123456789  Peter     A   22.0   \n",
       "2  987654321   Mary     B   11.0   \n",
       "3  345678912  Susan     D   12.0   \n",
       "\n",
       "                                   PerformanceReport  \n",
       "0  **Performance Report: John (Class C)**\\n\\n**Ov...  \n",
       "1  **Performance Report**\\n\\n**Student:** Peter (...  \n",
       "2  **Performance Report: Mary (987654321)**\\n\\n**...  \n",
       "3  **Performance Report: Susan (345678912)**\\n\\nS...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "from grading_utils import (\n",
    "    init_gemini_client,\n",
    "    get_cache_key,\n",
    "    get_from_cache,\n",
    "    save_to_cache,\n",
    "    create_gemini_config,\n",
    ")\n",
    "\n",
    "# Initialize Gemini client\n",
    "client = init_gemini_client()\n",
    "\n",
    "marking_scheme_file = paths[\"marking_scheme_file\"]\n",
    "marking_scheme_df = pd.read_excel(marking_scheme_file, sheet_name=\"Marking Scheme\")\n",
    "required_cols = {\"question_number\", \"question_text\", \"marking_scheme\", \"marks\"}\n",
    "missing_cols = required_cols - set(marking_scheme_df.columns)\n",
    "if missing_cols:\n",
    "    raise ValueError(f\"Marking scheme is missing columns: {missing_cols}\")\n",
    "\n",
    "def normalize_question_label(val):\n",
    "    label = str(val).strip()\n",
    "    if label.upper().startswith(\"Q\"):\n",
    "        return label\n",
    "    try:\n",
    "        as_float = float(label)\n",
    "        if as_float.is_integer():\n",
    "            return f\"Q{int(as_float)}\"\n",
    "    except Exception:\n",
    "        pass\n",
    "    return f\"Q{label}\"\n",
    "\n",
    "marking_scheme_df[\"Question\"] = marking_scheme_df[\"question_number\"].apply(normalize_question_label)\n",
    "question_meta = (\n",
    "    marking_scheme_df\n",
    "    .set_index(\"Question\")[\n",
    "        [\"question_text\", \"marking_scheme\", \"marks\"]\n",
    "    ]\n",
    "    .to_dict(orient=\"index\")\n",
    ")\n",
    "\n",
    "question_cols = [col for col in marksDf.columns if col not in [\"ID\", \"NAME\", \"CLASS\", \"Marks\"]]\n",
    "answers_lookup = {}\n",
    "if not answers_sheet.empty:\n",
    "    for _, row in answers_sheet.iterrows():\n",
    "        answers_lookup[str(row[\"ID\"])] = {\n",
    "            col: (\"\" if pd.isna(row.get(col, \"\")) else str(row.get(col, \"\")))\n",
    "            for col in question_cols\n",
    "        }\n",
    "\n",
    "cache_dir = paths[\"cache_dir\"]\n",
    "\n",
    "# Create Gemini config using utility function\n",
    "config = create_gemini_config(temperature=0.35, top_p=0.9, max_output_tokens=1536)\n",
    "\n",
    "def build_question_section(student_row, student_answers):\n",
    "    blocks = []\n",
    "    for q, meta in sorted(question_meta.items()):\n",
    "        awarded_mark = student_row.get(q, \"\")\n",
    "        if pd.isna(awarded_mark):\n",
    "            awarded_mark = \"\"\n",
    "        answer_text = student_answers.get(q, \"\")\n",
    "        blocks.append(\n",
    "            f\"Question {q} (max {meta.get('marks', '')}):\\n\"\n",
    "            f\"Prompt: {meta.get('question_text', '')}\\n\"\n",
    "            f\"Marking scheme: {meta.get('marking_scheme', '')}\\n\"\n",
    "            f\"Awarded mark: {awarded_mark}\\n\"\n",
    "            f\"Student answer:\\n{answer_text or '[no answer captured]'}\"\n",
    "        )\n",
    "    return \"\\n\\n---\\n\\n\".join(blocks)\n",
    "\n",
    "def generate_student_report(student_row):\n",
    "    sid = str(student_row.get(\"ID\", \"\"))\n",
    "    student_answers = answers_lookup.get(sid, {})\n",
    "    question_section = build_question_section(student_row, student_answers)\n",
    "    prompt = f\"\"\"You are an instructor drafting a concise performance report.\n",
    "\n",
    "Student: {sid} - {student_row.get('NAME', '')} (Class: {student_row.get('CLASS', '')})\n",
    "Total score: {student_row.get('Marks', '')}\n",
    "\n",
    "Use the question details, marking schemes, awarded marks, and answers below:\n",
    "{question_section}\n",
    "\n",
    "Write:\n",
    "- 2-3 sentence overall summary of strengths and weaknesses.\n",
    "- One short bullet per question with actionable feedback tied to the marking scheme.\n",
    "- 2 concrete next-step study suggestions focused on the weakest skills.\n",
    "Keep it under 220 words and avoid restating the input verbatim.\"\"\"\n",
    "    payload_hash = hashlib.sha256(question_section.encode(\"utf-8\")).hexdigest()\n",
    "    cache_key = get_cache_key(\n",
    "        \"performance_report\",\n",
    "        model=\"gemini-3-flash-preview\",\n",
    "        student_id=sid,\n",
    "        payload_hash=payload_hash,\n",
    "    )\n",
    "    cached = get_from_cache(cache_key, cache_dir)\n",
    "    if cached is not None:\n",
    "        return cached.get(\"report\", \"\")\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-3-flash-preview\",\n",
    "        contents=[{\"role\": \"user\", \"parts\": [{\"text\": prompt}]}],\n",
    "        config=config,\n",
    "    )\n",
    "    report_text = response.text if response.text else \"\"\n",
    "    save_to_cache(cache_key, {\"report\": report_text}, cache_dir)\n",
    "    return report_text\n",
    "\n",
    "performance_rows = []\n",
    "for _, row in marksDf.iterrows():\n",
    "    report_text = generate_student_report(row)\n",
    "    performance_rows.append(\n",
    "        {\n",
    "            \"ID\": row.get(\"ID\", \"\"),\n",
    "            \"NAME\": row.get(\"NAME\", \"\"),\n",
    "            \"CLASS\": row.get(\"CLASS\", \"\"),\n",
    "            \"Marks\": row.get(\"Marks\", \"\"),\n",
    "            \"PerformanceReport\": report_text,\n",
    "        }\n",
    "    )\n",
    "\n",
    "performance_df = pd.DataFrame(performance_rows)\n",
    "\n",
    "# Append/replace Performance sheet inside the existing details_score_report.xlsx\n",
    "writer_kwargs = {\"sheet_name\": \"Performance\", \"index\": False}\n",
    "if os.path.exists(details_report_path):\n",
    "    with pd.ExcelWriter(\n",
    "        details_report_path,\n",
    "        mode=\"a\",\n",
    "        engine=\"openpyxl\",\n",
    "        if_sheet_exists=\"replace\",\n",
    "    ) as writer:\n",
    "        performance_df.to_excel(writer, **writer_kwargs)\n",
    "else:\n",
    "    performance_df.to_excel(details_report_path, **writer_kwargs)\n",
    "\n",
    "print(\"Saved Performance sheet in:\", details_report_path)\n",
    "performance_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='../marking_form/VTC Test/marked/scripts/../scripts.zip' target='_blank'>../marking_form/VTC Test/marked/scripts/../scripts.zip</a><br>"
      ],
      "text/plain": [
       "/home/user/AI-Handwrite-Grader/marking_form/VTC Test/marked/scripts.zip"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import FileLink \n",
    "\n",
    "# zip base_path_marked_scripts folder\n",
    "script_zip = base_path_marked_scripts + \"../scripts\"\n",
    "shutil.make_archive(script_zip, \"zip\", base_path_marked_scripts)\n",
    "FileLink(script_zip + \".zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
